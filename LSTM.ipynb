{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMidiSlices(fileName):\n",
    "    midiArray=[]\n",
    "    HopSize=hopLength/sr\n",
    "    timeTracker=0.0\n",
    "    totalTimeTracker=0.0\n",
    "    MemoryArray=np.zeros(128)\n",
    "    lasttime=0;\n",
    "    bpm=60\n",
    "    for msg in mido.MidiFile(fileName+'.mid'):\n",
    "        if not msg.is_meta:\n",
    "            totalTimeTracker+=msg.time\n",
    "            nextEventTime=lasttime+msg.time\n",
    "            while nextEventTime> timeTracker + HopSize:\n",
    "                MemoryArray[127]=((timeTracker%4) * (bpm/60.0))/4.0\n",
    "                midiArray.append(MemoryArray.copy())\n",
    "                timeTracker += HopSize;\n",
    "            if msg.type=='note_on':\n",
    "                MemoryArray[msg.note]+=1.0;\n",
    "            elif msg.type=='note_off':\n",
    "                MemoryArray[msg.note]-=1.0;\n",
    "            if(timeTracker+msg.time >= timeTracker + HopSize):\n",
    "                MemoryArray[127]=((timeTracker%4) * (bpm/60.0))/4.0\n",
    "                midiArray.append(MemoryArray.copy())\n",
    "                timeTracker += HopSize\n",
    "            lasttime += msg.time\n",
    "        else:\n",
    "            if(msg.type == 'set_tempo'):\n",
    "                bpm=60000000/msg.tempo\n",
    "    return np.array(midiArray)\n",
    "\n",
    "def CreateSeqeunce(dataArray,SeqeunceLen):\n",
    "    OutArray=[]\n",
    "    for i in range(np.int(np.floor(dataArray.shape[0]/SeqeunceLen))):\n",
    "        tempArray=[]\n",
    "        for j in range(SeqeunceLen):\n",
    "            tempArray.append(dataArray[(i*SeqeunceLen)+j])\n",
    "        OutArray.append(tempArray)\n",
    "    return np.array(OutArray)\n",
    "\n",
    "def generateAudio(stringInput,fileName):\n",
    "    print(\"generating audio and Spectrum image\")\n",
    "    MainMidiTest=CreateSeqeunce(GetMidiSlices(stringInput),SequenceLength)\n",
    "    tensor_x_test = torch.stack([torch.Tensor(i) for i in MainMidiTest])\n",
    "    #tensor_x_test = tensor_x_test.view(tensor_x_test.shape[0],-1)\n",
    "    testDataSet = utils.TensorDataset(tensor_x_test,torch.zeros(tensor_x_test.shape)) # create your datset\n",
    "    TestLoader = utils.DataLoader(testDataSet,batch_size=BatchSize) # create your dataloader\n",
    "    model.eval()\n",
    "    outputArray=[]\n",
    "    for data, labels in TestLoader:\n",
    "        print(data.shape)\n",
    "        if data.shape[0] < BatchSize:\n",
    "            zeros = torch.zeros((BatchSize-data.shape[0],SequenceLength,128))\n",
    "            data=torch.cat((data,zeros))\n",
    "        print(data.shape)\n",
    "        with torch.no_grad():\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            output,_,_ = model.forward(data,data.shape[0])\n",
    "            for i in range(data.shape[0]):\n",
    "                for j in range(SequenceLength):\n",
    "                    outputArray.append(output[i,j,:].cpu().numpy())\n",
    "            #outputArray.append(output[0].cpu().numpy())\n",
    "    outputArray=np.array(outputArray)\n",
    "    print(outputArray.shape)\n",
    "    transformedArray=[]\n",
    "    for elem in outputArray:\n",
    "        a=np.array(np.int((n_fft/2+1)) *[1+1j])\n",
    "        a.real=elem[:np.int(n_fft/2+1)]\n",
    "        a.imag=elem[np.int(n_fft/2+1):]\n",
    "        transformedArray.append(a)\n",
    "    transformedArray=np.array(transformedArray).T\n",
    "    Y_infered2 = librosa.istft(transformedArray,hop_length=hopLength) \n",
    "    plt.figure(figsize=(20,10))\n",
    "    \n",
    "    librosa.display.specshow(librosa.amplitude_to_db(transformedArray.real,\n",
    "                                                      ref=np.max),\n",
    "                              y_axis='log', x_axis='time')\n",
    "    plt.title('Power spectrogram  ' + str(Counter) )\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    #plt.show();\n",
    "    plt.savefig(fileName+'.png')\n",
    "    librosa.output.write_wav(fileName+'.wav', Y_infered2, sr)\n",
    "    plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import mido\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import torch.utils.data as utils\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fielName='4Beats2MelLong5'\n",
    "windowSize=50\n",
    "n_fft=2048\n",
    "hopLength=1024\n",
    "SequenceLength = 25 \n",
    "BatchSize=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y, sr = librosa.load(fielName+'.wav')\n",
    "D = np.array(librosa.stft(y,n_fft=n_fft,hop_length=hopLength))\n",
    "del y\n",
    "MidiArray = GetMidiSlices(fielName)\n",
    "minLength = min(MidiArray.shape[0],D.T.shape[0])\n",
    "MidiArray=MidiArray[:minLength];\n",
    "D_data=np.array([np.append(np.float32(elem.real), np.float32(elem.imag)) for elem in D.T[:minLength]])\n",
    "\n",
    "\n",
    "MidiArray=CreateSeqeunce(MidiArray,SequenceLength)\n",
    "D_data=CreateSeqeunce(D_data,SequenceLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_SeqeuenceLength = librosa.istft(D[:SequenceLength],hop_length=hopLength)\n",
    "ipd.Audio(Y_SeqeuenceLength,rate=sr);\n",
    "del D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2132, 25, 128) (2132, 25, 2050)\n",
      "float64 float32\n"
     ]
    }
   ],
   "source": [
    "print(MidiArray.shape,D_data.shape)\n",
    "print(MidiArray.dtype,D_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe280e7ab00>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAAwCAYAAAD5PXpqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAABnlJREFUeJzt3VuoXFcdx/Hv7yTtOWq9nNhSa63RStEGhNQGFS0Sb7H60FosYp+iIOlDgxd8MFS8IAhR8PKiQlJj+6CN4qVNQdDYWhQE6WkJTS+kjaWlPaQJTVNtLU1y0p8Psw8zncxkzuw9zD5z9u8DYfbes9asf/6s889hZ83ask1ERDTLVN0BRETE+KX4R0Q0UIp/REQDpfhHRDRQin9ERAOl+EdENFCl4i9pjaS9kh4tXmf7tDslaV/xZ0+VMSMiojpVWecv6QfAs7a3S9oGzNr+eo92L9g+p0KcERExQlWL/wFgo+1Dki4A7rb9zh7tUvwjIpaRqsX/OdtvKI4FHFs872q3AOwDFoDttm/r83lbgC0Aq161+vLXrT3to85oinJ/F2n4fio1UrkYy8Q3zlyUHqtEv3Hlr+xYyz3vZXLeGqtMn7K5GP6nq0x8KvlTPFWi3yP3v7rUWGVCfN7HnrF93qB2qweOLf0VeFOPt77ReWLb6j+D19qel3QxcJek/bb/3d3I9g5gB8CaS8/zpl3XDArvFaZXLQzVftHZU8P3my7Rp2y/Mn1mpk4O3ac11vD9ZlQ2F2XGOjF8n5K5mFGZ+EqOVSoXZfqcGrpP2X4zJf/RnSlR/Kc1fPmf0cDy12ess4bu84k3ry81llYPH+Pek7ufWEq7gZ9s+2P93pN0WNIFHbd9jvT5jPni9TFJdwOXAacV/4iIGI+qSz33AJuL483A7d0NJM1Kmi6OzwU+CDxUcdyIiKigavHfDnxc0jzwTeBaSdskbZB0U9HmUuBeSc8B88Aq4MWK40ZERAWVir/to8Am4CXg3cC7gOuAF21/sWjzT+BnwG7b08CNwPerjBsREdWM4hu+7wUO2n7M9glgN3B1V5urgVuK498BH1WZ/9KPiIiRGEXxvxB4suP8qeJazza2F4D/AG/s/iBJWyTNSZo7fuylEYQWERG9LKu9fWzvsL3B9obp2Zm6w4mIWLFGUfzngYs6zt9SXOvZRtJq4PXA0RGMHRERJYyi+N8DXCLp7ZLOBj5Hawlop84lodcCdzkPD46IqE3l4l/cw/8lcAD4H/C07QclfVfSVUWzE8AXJB0HfgHcV3XciIgor9z3mztIWgV8ntYyz6eAeySts/2tjmYngZ22t1YdLyIiqhvXUs+IiFhGKv/mT++lnu/r0e4zkj4EPAJ81faT3Q06d/UEXvjNB3Ye6DPmucAz5UNeUZKLtuSiLbloWwa5OFiuW7l9AtcupdEoiv9S3AHcavu4pOtpfeHrI92NOnf1PBNJc7Y3jD7MyZNctCUXbclFW3LR21iWeto+avt4cXoTcPkIxo2IiJLGstSz2O550VXAwyMYNyIiSqp828f2gqStwJ9p7di5a3GpJzBnew/wpWLZ5wLwLK3VQVUMvDXUIMlFW3LRlly0JRc9VHqMY0RETKZltbdPRESMR4p/REQDTVzxl3SlpAOSDkraVnc8dZL0uKT9kvZJmqs7nnGStEvSEUkPdFxbI2mvpEeL19k6YxyXPrn4jqT5Ym7sk/SpOmMcB0kXSfqbpIckPSjpy8X1Rs6LQSaq+BdbSfwU+CSwDrhO0rp6o6rdh22vb+A65puBK7uubQPutH0JcGdx3gQ3c3ouAH5czI31tv805pjqsAB8zfY64P3ADUV9aOq8OKOJKv5kK4ko2P47rZVjnTqfGHcL8OmxBlWTPrloHNuHbN9XHD9Pa0n5hTR0XgwyacV/KU8NaxIDf5F0b7E1RtOdb/tQcfw0cH6dwSwDWyXdX9wWatStDklvAy4D/kXmRU+TVvzjla6w/R5at8FuKPZOCqB4XkST1zH/HHgHsB44BPyw3nDGR9I5wO+Br9j+b+d7mRdtk1b8l/LUsMawPV+8HgH+SOu2WJMdXvw2efF6pOZ4amP7sO1Ttl8GdtKQuSHpLFqF/1e2/1BczrzoYdKK/1KeGtYIkl4j6bWLx8Am4IEz91rxOp8Ytxm4vcZYatW1pco1NGBuSBKth0U9bPtHHW9lXvQwcd/wLZas/YT2VhLfqzmkWki6mNZv+9DapuPXTcqFpFuBjbS26z0MfBu4Dfgt8FbgCeCztlf8f4T2ycVGWrd8DDwOXN9x33tFknQF8A9gP/BycflGWvf9GzcvBpm44h8REdVN2m2fiIgYgRT/iIgGSvGPiGigFP+IiAZK8Y+IaKAU/4iIBkrxj4hooP8DBB4/DlQQtUEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(MidiArray[6,:,127:].T) # visulizing the bea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2132, 25, 128]) torch.Size([2132, 25, 2050])\n",
      "14 4\n"
     ]
    }
   ],
   "source": [
    "tensor_x = torch.stack([torch.Tensor(i) for i in MidiArray]) # transform to torch tensors\n",
    "tensor_y = torch.stack([torch.Tensor(i) for i in np.abs(D_data)])\n",
    "print(tensor_x.shape,tensor_y.shape)\n",
    "\n",
    "my_dataset = utils.TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "\n",
    "train_size = int(0.8 * len(my_dataset))\n",
    "test_size = len(my_dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(my_dataset, [train_size, test_size])\n",
    "\n",
    "my_dataloader = utils.DataLoader(train_dataset,batch_size=BatchSize,shuffle=True) # create your dataloader\n",
    "my_validationloader = utils.DataLoader(test_dataset,batch_size=BatchSize,shuffle=True) # create your dataloader\n",
    "print(len(my_dataloader),len(my_validationloader))\n",
    "my_testloader = utils.DataLoader(my_dataset) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(tensor_x.numpy()[50:100,:,127])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Solution\n",
    "\n",
    "class LSTMNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMNetwork,self).__init__()\n",
    "        # Defining the layers, 128, 64, 10 units each\n",
    "        \n",
    "        self.num_layers=1\n",
    "        self.hiddenLayerSize=512\n",
    "        self.word_lstm_init_h = nn.Parameter(torch.zeros(self.num_layers, BatchSize, self.hiddenLayerSize).type(torch.FloatTensor), requires_grad=True)\n",
    "        self.word_lstm_init_c = nn.Parameter(torch.zeros(self.num_layers, BatchSize, self.hiddenLayerSize).type(torch.FloatTensor), requires_grad=True)\n",
    "        self.word_lstm_init_h.cuda()\n",
    "        self.word_lstm_init_c.cuda()\n",
    "        self.lstm = nn.LSTM(128, self.hiddenLayerSize,num_layers=self.num_layers,batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.hiddenLayerSize, self.hiddenLayerSize*4)\n",
    "        self.fc2 = nn.Linear(self.hiddenLayerSize*2, n_fft+2)\n",
    "        #self.h0 = torch.zeros(self.num_layers, 1, (n_fft+2*2)).cuda() # 2 for bidirection \n",
    "        #self.c0 = torch.zeros(self.num_layers, 1, (n_fft+2*2)).cuda()\n",
    "        \n",
    "\n",
    "    def forward(self, x, ActualBatchSize):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        #self.h0=h0\n",
    "        #self.c0=c0 \n",
    "        #x=x.view(SequenceLength,ActualBatchSize,128)\n",
    "        x,hidden = self.lstm(x,(self.word_lstm_init_h,self.word_lstm_init_c)) #(self.h0,self.c0)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.avg_pool1d(x,2)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return x,hidden,self.word_lstm_init_h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reset\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMNetwork()\n",
    "model.train()\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.5,momentum=0.9)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "Counter=0;\n",
    "LossOverEpoch=[]\n",
    "EvalLoss=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.6088020971843175   0  1.2199330627918243\n",
      "Training loss: 1.6138989329338074   1  1.2278996706008911\n",
      "Training loss: 1.6083062291145325   2  1.1857405602931976\n",
      "Training loss: 1.6000607354300362   3  1.164585828781128\n",
      "Training loss: 1.5894403202193124   4  1.1631829738616943\n",
      "Training loss: 1.5840673106057304   5  1.1699718534946442\n",
      "Training loss: 1.5714533499308996   6  1.1629396080970764\n",
      "Training loss: 1.560160253729139   7  1.1588674187660217\n",
      "Training loss: 1.5422339269093104   8  1.1433713138103485\n",
      "Training loss: 1.532463686806815   9  1.1624141037464142\n",
      "Training loss: 1.5179714134761266   10  1.1526192724704742\n",
      "Training loss: 1.5145995106015886   11  1.1263884007930756\n",
      "Training loss: 1.5008524315697807   12  1.1250033676624298\n",
      "Training loss: 1.4902187160083227   13  1.1106307804584503\n",
      "Training loss: 1.4793295349393571   14  1.131206601858139\n",
      "Training loss: 1.4711554305894035   15  1.0981421172618866\n",
      "Training loss: 1.4687251363481795   16  1.100785106420517\n",
      "Training loss: 1.463564464024135   17  1.1031829416751862\n",
      "Training loss: 1.4678382532937186   18  1.1325395703315735\n",
      "Training loss: 1.4676969306809562   19  1.1208914518356323\n",
      "Training loss: 1.452872906412397   20  1.104636698961258\n",
      "Training loss: 1.4641581518309457   21  1.116553783416748\n",
      "Training loss: 1.4616820982524328   22  1.0819311141967773\n",
      "Training loss: 1.453917724745614   23  1.117232859134674\n",
      "Training loss: 1.4446828195026942   24  1.109390914440155\n",
      "Training loss: 1.4474982959883553   25  1.0959839820861816\n",
      "Training loss: 1.4433035935674394   26  1.0653127133846283\n",
      "Training loss: 1.4521655099732536   27  1.0726242661476135\n",
      "Training loss: 1.4459578565188818   28  1.121346890926361\n",
      "Training loss: 1.4375321098736353   29  1.0895900130271912\n",
      "Training loss: 1.4472206149782454   30  1.1058829128742218\n",
      "Training loss: 1.4399761131831579   31  1.1020421385765076\n",
      "Training loss: 1.4326895475387573   32  1.089337408542633\n",
      "Training loss: 1.4262666021074568   33  1.0538994371891022\n",
      "Training loss: 1.4258638705526079   34  1.0826737582683563\n",
      "Training loss: 1.4249378102166312   35  1.0957886576652527\n",
      "Training loss: 1.4240826538630895   36  1.085519939661026\n",
      "Training loss: 1.4252652525901794   37  1.1147003769874573\n",
      "Training loss: 1.4333569748061044   38  1.063887596130371\n",
      "Training loss: 1.4280397040503365   39  1.0824360847473145\n",
      "Training loss: 1.4251826660973685   40  1.0625179708003998\n",
      "Training loss: 1.416588544845581   41  1.042879581451416\n",
      "Training loss: 1.412207203251975   42  1.0893884301185608\n",
      "Training loss: 1.41651679788317   43  1.0584895014762878\n",
      "Training loss: 1.4190234456743513   44  1.0756683051586151\n",
      "Training loss: 1.415726593562535   45  1.0582005083560944\n",
      "Training loss: 1.404064689363752   46  1.0636287331581116\n",
      "Training loss: 1.4196108494486128   47  1.0824081897735596\n",
      "Training loss: 1.4097614458629064   48  1.100157916545868\n",
      "Training loss: 1.4149578043392725   49  1.071969360113144\n",
      "Training loss: 1.4148184401648385   50  1.0822373628616333\n",
      "Training loss: 1.4059545738356454   51  1.0810498297214508\n",
      "Training loss: 1.3960033059120178   52  1.054836630821228\n",
      "Training loss: 1.3820791414805822   53  1.1105780601501465\n",
      "Training loss: 1.3962206755365645   54  1.0580441653728485\n",
      "Training loss: 1.4090232934270586   55  1.0774661600589752\n",
      "Training loss: 1.3988423092024667   56  1.056726485490799\n",
      "Training loss: 1.3917794568198067   57  1.0536462664604187\n",
      "Training loss: 1.3907659820147924   58  1.0638565719127655\n",
      "Training loss: 1.3851623790604728   59  1.034548133611679\n",
      "Training loss: 1.3864320772034782   60  1.0535739362239838\n",
      "Training loss: 1.3893243840762548   61  1.1209520399570465\n",
      "Training loss: 1.412443254675184   62  1.0752185583114624\n",
      "Training loss: 1.3949541364397322   63  1.0800071060657501\n",
      "Training loss: 1.3828544616699219   64  1.047692060470581\n",
      "Training loss: 1.3861907294818334   65  1.059835821390152\n",
      "Training loss: 1.3873970678874425   66  1.0677164196968079\n",
      "Training loss: 1.383327100958143   67  1.0541915893554688\n",
      "Training loss: 1.3822595817702157   68  1.0359855890274048\n",
      "Training loss: 1.3752465077808924   69  1.0333589613437653\n",
      "Training loss: 1.3795509678976876   70  1.0772107243537903\n",
      "Training loss: 1.3915065101214819   71  1.036551982164383\n",
      "Training loss: 1.3931932704789298   72  1.0653439462184906\n",
      "Training loss: 1.3865941422326225   73  1.0942337214946747\n",
      "Training loss: 1.4126243080411638   74  1.061040461063385\n",
      "Training loss: 1.3932899917875017   75  1.0736771821975708\n",
      "Training loss: 1.390836843422481   76  1.040963888168335\n",
      "Training loss: 1.3784014071737016   77  1.0692626535892487\n",
      "Training loss: 1.3791948301451546   78  1.054140329360962\n",
      "Training loss: 1.3706648009163993   79  1.0605685114860535\n",
      "Training loss: 1.3673540183476038   80  1.02708101272583\n",
      "Training loss: 1.362417357308524   81  1.036647766828537\n",
      "Training loss: 1.3566508633749825   82  1.0259781181812286\n",
      "Training loss: 1.3579493165016174   83  1.0209177136421204\n",
      "Training loss: 1.3776628289903914   84  1.0296272933483124\n",
      "Training loss: 1.362359140600477   85  1.0535647571086884\n",
      "Training loss: 1.3798545343535287   86  1.0496177971363068\n",
      "Training loss: 1.380626950945173   87  1.0120855569839478\n",
      "Training loss: 1.3603262305259705   88  1.0288524329662323\n",
      "Training loss: 1.3681537423815047   89  1.047105848789215\n",
      "Training loss: 1.3822914106505257   90  1.059372216463089\n",
      "Training loss: 1.3695070573261805   91  1.0391759872436523\n",
      "Training loss: 1.356509166104453   92  1.0499582588672638\n",
      "Training loss: 1.3576752884047372   93  0.9983161985874176\n",
      "Training loss: 1.356043713433402   94  1.0202097594738007\n",
      "Training loss: 1.3547518508774894   95  0.9905676245689392\n",
      "Training loss: 1.3593174815177917   96  1.0146513879299164\n",
      "Training loss: 1.3546276433127267   97  1.0196263194084167\n",
      "Training loss: 1.3412000792367118   98  1.0276901125907898\n",
      "Training loss: 1.3419958863939558   99  1.000056654214859\n",
      "Training loss: 1.3518167989594596   100  1.032871037721634\n",
      "Training loss: 1.350087583065033   101  1.0046328604221344\n",
      "Training loss: 1.351795596735818   102  1.0725815296173096\n",
      "Training loss: 1.3535708359309606   103  1.0064436793327332\n",
      "Training loss: 1.3426242555890764   104  1.0164349675178528\n",
      "Training loss: 1.3371474317141943   105  1.014284998178482\n",
      "Training loss: 1.3268117819513594   106  1.0176523923873901\n",
      "Training loss: 1.33298659324646   107  1.0077168643474579\n",
      "Training loss: 1.3351215379578727   108  0.9900637865066528\n",
      "Training loss: 1.3327183382851737   109  0.9991412460803986\n",
      "Training loss: 1.320632483278002   110  0.9982971251010895\n",
      "Training loss: 1.3296397158077784   111  0.9878450334072113\n",
      "Training loss: 1.3539944291114807   112  1.0150447189807892\n",
      "Training loss: 1.3445642760821752   113  0.9950258433818817\n",
      "Training loss: 1.3317719868251257   114  1.0132452249526978\n",
      "Training loss: 1.3298190491540092   115  1.007568210363388\n",
      "Training loss: 1.3282835653850011   116  0.9915660917758942\n",
      "Training loss: 1.3253448094640459   117  0.9826448559761047\n",
      "Training loss: 1.320621235030038   118  0.9552572667598724\n",
      "Training loss: 1.31254723242351   119  1.0020556449890137\n",
      "Training loss: 1.3041164023535592   120  1.009970873594284\n",
      "Training loss: 1.3139150483267648   121  1.0141277015209198\n",
      "Training loss: 1.3337033987045288   122  0.9826842546463013\n",
      "Training loss: 1.3112799695559911   123  0.999809205532074\n",
      "Training loss: 1.3043936576162065   124  0.9761723279953003\n",
      "Training loss: 1.2999336974961417   125  0.9874810576438904\n",
      "Training loss: 1.3126938002450126   126  1.0037255585193634\n",
      "Training loss: 1.294297226837703   127  0.9980197250843048\n",
      "Training loss: 1.2976932951382227   128  0.9854776263237\n",
      "Training loss: 1.314893092427935   129  0.9962534308433533\n",
      "Training loss: 1.30369804586683   130  0.9753917753696442\n",
      "Training loss: 1.2934388518333435   131  0.986415296792984\n",
      "Training loss: 1.2995094401495797   132  0.9905451536178589\n",
      "Training loss: 1.2996473908424377   133  0.9745291471481323\n",
      "Training loss: 1.299965969153813   134  1.0027395188808441\n",
      "Training loss: 1.2950523240225655   135  0.9835822284221649\n",
      "Training loss: 1.305279016494751   136  0.9836868047714233\n",
      "Training loss: 1.3108800990240914   137  0.9714708924293518\n",
      "Training loss: 1.3032145415033614   138  1.0070778727531433\n",
      "Training loss: 1.2973573122705733   139  0.9554669559001923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.2870990633964539   140  0.9662400782108307\n",
      "Training loss: 1.2893993854522705   141  0.9868594706058502\n",
      "Training loss: 1.3071615866252355   142  0.9918246865272522\n",
      "Training loss: 1.2917183297021049   143  0.9600645303726196\n",
      "Training loss: 1.284931208406176   144  0.9840472936630249\n",
      "Training loss: 1.2795610938753401   145  0.9650731384754181\n",
      "Training loss: 1.277107570852552   146  0.9626758992671967\n",
      "Training loss: 1.277496269771031   147  0.9298088550567627\n",
      "Training loss: 1.2901583569390433   148  0.9746467173099518\n",
      "Training loss: 1.298892387322017   149  1.004775047302246\n",
      "Training loss: 1.2990456478936332   150  0.9556604623794556\n",
      "Training loss: 1.2898786408560616   151  1.0009907484054565\n",
      "Training loss: 1.285187040056501   152  1.0323231220245361\n",
      "Training loss: 1.2846333639962333   153  0.9664930999279022\n",
      "Training loss: 1.2985768488475256   154  0.9818023443222046\n",
      "Training loss: 1.2893991214888436   155  0.9601274728775024\n",
      "Training loss: 1.270624577999115   156  0.9886385202407837\n",
      "Training loss: 1.2775708266666956   157  0.9642762541770935\n",
      "Training loss: 1.273245973246438   158  0.9394284784793854\n",
      "Training loss: 1.2664708580289568   159  0.9694972038269043\n",
      "Training loss: 1.2831140841756548   160  0.996194988489151\n",
      "Training loss: 1.298325547150203   161  0.9929344654083252\n",
      "Training loss: 1.2879522102219718   162  0.9924029111862183\n",
      "Training loss: 1.2848964078085763   163  0.982485830783844\n",
      "Training loss: 1.2793882914951868   164  0.9496049284934998\n",
      "Training loss: 1.2761298758643014   165  0.9743365943431854\n",
      "Training loss: 1.2891614862850733   166  1.0222001671791077\n",
      "Training loss: 1.3134565693991525   167  0.9817649126052856\n",
      "Training loss: 1.2915096879005432   168  0.9727829396724701\n",
      "Training loss: 1.3003112844058446   169  0.9715075492858887\n",
      "Training loss: 1.2804656028747559   170  0.9530801773071289\n",
      "Training loss: 1.2820857422692435   171  0.967481404542923\n",
      "Training loss: 1.2746770637375968   172  0.9768098592758179\n",
      "Training loss: 1.2606511541775294   173  0.9565309584140778\n",
      "Training loss: 1.2694545643670219   174  0.988070011138916\n",
      "Training loss: 1.262799688747951   175  0.9592059254646301\n",
      "Training loss: 1.2693459221294947   176  0.9730297029018402\n",
      "Training loss: 1.2631527015141077   177  0.9780154824256897\n",
      "Training loss: 1.270050151007516   178  0.9553040564060211\n",
      "Training loss: 1.2690612375736237   179  0.9695847928524017\n",
      "Training loss: 1.2617029803139823   180  0.9545867741107941\n",
      "Training loss: 1.2572806307247706   181  0.9726504385471344\n",
      "Training loss: 1.2499021887779236   182  0.9620046615600586\n",
      "Training loss: 1.246172615459987   183  0.9543050527572632\n",
      "Training loss: 1.2570915222167969   184  0.9628388285636902\n",
      "Training loss: 1.2549447502408708   185  0.953520268201828\n",
      "Training loss: 1.2469428947993688   186  0.9417451024055481\n",
      "Training loss: 1.2558425068855286   187  0.9612742364406586\n",
      "Training loss: 1.2493812526975359   188  0.937626451253891\n",
      "Training loss: 1.2434736830847604   189  0.9573156237602234\n",
      "Training loss: 1.2469681331089564   190  0.9213886559009552\n",
      "Training loss: 1.2423052276883806   191  0.9393186867237091\n",
      "Training loss: 1.247920674937112   192  0.9486697018146515\n",
      "Training loss: 1.2454418965748377   193  0.9478185176849365\n",
      "Training loss: 1.2456865566117423   194  0.9624610543251038\n",
      "Training loss: 1.245513345514025   195  0.9693559408187866\n",
      "Training loss: 1.2415536556925093   196  0.9591678977012634\n",
      "Training loss: 1.2423585568155562   197  0.9473986923694611\n",
      "Training loss: 1.2407684070723397   198  0.9530142545700073\n",
      "Training loss: 1.2845957279205322   199  0.9418022632598877\n",
      "Training loss: 1.254758059978485   200  0.939686119556427\n",
      "Training loss: 1.2445075171334403   201  0.9148971438407898\n",
      "Training loss: 1.2417962040219988   202  0.9359950721263885\n",
      "Training loss: 1.2413141131401062   203  0.9617487192153931\n",
      "Training loss: 1.2357649888311113   204  0.9547373652458191\n",
      "Training loss: 1.231121003627777   205  0.9610508978366852\n",
      "Training loss: 1.2472795418330602   206  0.9336033463478088\n",
      "Training loss: 1.2432517153876168   207  0.9444238841533661\n",
      "Training loss: 1.230012059211731   208  0.9535424113273621\n",
      "Training loss: 1.2397327763693673   209  0.9365493059158325\n",
      "Training loss: 1.250499325139182   210  0.941285252571106\n",
      "Training loss: 1.2368058477129256   211  0.9509870409965515\n",
      "Training loss: 1.2350683297429765   212  0.958904504776001\n",
      "Training loss: 1.2362460153443473   213  0.9296734929084778\n",
      "Training loss: 1.2350863814353943   214  0.9417718350887299\n",
      "Training loss: 1.2273325409208025   215  0.9271039068698883\n",
      "Training loss: 1.2184511082512992   216  0.9618262648582458\n",
      "Training loss: 1.2213737964630127   217  0.9229069352149963\n",
      "Training loss: 1.2313813482012068   218  0.9466822743415833\n",
      "Training loss: 1.249773817402976   219  0.9357946217060089\n",
      "Training loss: 1.243520872933524   220  0.924347311258316\n",
      "Training loss: 1.2301388808659144   221  0.9423611164093018\n",
      "Training loss: 1.2193087594849723   222  0.954420268535614\n",
      "Training loss: 1.214082726410457   223  0.9252598285675049\n",
      "Training loss: 1.232226848602295   224  0.9504120647907257\n",
      "Training loss: 1.2332938398633684   225  0.9420817196369171\n",
      "Training loss: 1.2224923712866647   226  0.9618787467479706\n",
      "Training loss: 1.2238607491765703   227  0.9442316293716431\n",
      "Training loss: 1.2262235539300101   228  0.9078741073608398\n",
      "Training loss: 1.2238266808646066   229  0.9199595153331757\n",
      "Training loss: 1.2226733480181013   230  0.962194412946701\n",
      "Training loss: 1.2243351084845406   231  0.9486293494701385\n",
      "Training loss: 1.2189691151891435   232  0.9436679780483246\n",
      "Training loss: 1.224204650947026   233  0.9310198724269867\n",
      "Training loss: 1.2259161046573095   234  0.9243979454040527\n",
      "Training loss: 1.2254246473312378   235  0.9312514066696167\n",
      "Training loss: 1.2142254710197449   236  0.9190769493579865\n",
      "Training loss: 1.2174885358129228   237  0.944944441318512\n",
      "Training loss: 1.2188353793961662   238  0.9545754492282867\n",
      "Training loss: 1.2177147780145918   239  0.9352220594882965\n",
      "Training loss: 1.207576138632638   240  0.9435536563396454\n",
      "Training loss: 1.2161305121013097   241  0.9290313720703125\n",
      "Training loss: 1.2077994772366114   242  0.9152602851390839\n",
      "Training loss: 1.2095372336251395   243  0.9373926967382431\n",
      "Training loss: 1.2005500027111597   244  0.9419844448566437\n",
      "Training loss: 1.2051499145371574   245  0.9063216745853424\n",
      "Training loss: 1.2051756978034973   246  0.9051783978939056\n",
      "Training loss: 1.2133882471493311   247  0.925198882818222\n",
      "Training loss: 1.2038934315953935   248  0.9015924632549286\n",
      "Training loss: 1.1985947234289986   249  0.9369592368602753\n",
      "Training loss: 1.2028733321598597   250  0.9363444447517395\n",
      "Training loss: 1.1976910999843053   251  0.8994203507900238\n",
      "Training loss: 1.1934184772627694   252  0.9367712736129761\n",
      "Training loss: 1.206282309123448   253  0.9210046231746674\n",
      "Training loss: 1.1934095876557487   254  0.9153210520744324\n",
      "Training loss: 1.1942896417209081   255  0.9101460874080658\n",
      "Training loss: 1.197775661945343   256  0.8918547332286835\n",
      "Training loss: 1.1867783665657043   257  0.9101681113243103\n",
      "Training loss: 1.2185914857046944   258  0.8938145935535431\n",
      "Training loss: 1.2069970113890511   259  0.9298674464225769\n",
      "Training loss: 1.186608714716775   260  0.887903243303299\n",
      "Training loss: 1.1986327426774162   261  0.9342522621154785\n",
      "Training loss: 1.1955568875585283   262  0.9269315600395203\n",
      "Training loss: 1.1849907977240426   263  0.9011905193328857\n",
      "Training loss: 1.1864487613950456   264  0.933887392282486\n",
      "Training loss: 1.1870521817888533   265  0.9020684361457825\n",
      "Training loss: 1.1983241438865662   266  0.9173580706119537\n",
      "Training loss: 1.1885063222476415   267  0.8979596197605133\n",
      "Training loss: 1.1856083359037126   268  0.90143221616745\n",
      "Training loss: 1.174940185887473   269  0.9232472777366638\n",
      "Training loss: 1.1823717951774597   270  0.8849705457687378\n",
      "Training loss: 1.1751988955906458   271  0.9048833549022675\n",
      "Training loss: 1.1800427266529627   272  0.9132713079452515\n",
      "Training loss: 1.1729793122836523   273  0.8898248672485352\n",
      "Training loss: 1.2062125291143144   274  0.9071265459060669\n",
      "Training loss: 1.1964195626122611   275  0.9270834922790527\n",
      "Training loss: 1.1923223308154516   276  0.924959123134613\n",
      "Training loss: 1.1804300035749162   277  0.8891203105449677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.1806948610714503   278  0.8970811665058136\n",
      "Training loss: 1.1746576939310347   279  0.9160082340240479\n",
      "Training loss: 1.1696491922651018   280  0.8788106441497803\n",
      "Training loss: 1.1598889231681824   281  0.8882728815078735\n",
      "Training loss: 1.172902294567653   282  0.9207665026187897\n",
      "Training loss: 1.1689299941062927   283  0.8920082449913025\n",
      "Training loss: 1.1719615629741125   284  0.894107848405838\n",
      "Training loss: 1.1590734464781625   285  0.8869202733039856\n",
      "Training loss: 1.1574766295296806   286  0.9223597347736359\n",
      "Training loss: 1.1644460558891296   287  0.867669016122818\n",
      "Training loss: 1.1518190928867884   288  0.8878017663955688\n",
      "Training loss: 1.167718529701233   289  0.8916700780391693\n",
      "Training loss: 1.1635816097259521   290  0.8820352703332901\n",
      "Training loss: 1.1635838065828596   291  0.9186374843120575\n",
      "Training loss: 1.1641042998858862   292  0.8804431855678558\n",
      "Training loss: 1.1695894854409354   293  0.8704673647880554\n",
      "Training loss: 1.158463648387364   294  0.9068764746189117\n",
      "Training loss: 1.1494433283805847   295  0.9117222428321838\n",
      "Training loss: 1.151079637663705   296  0.8576344847679138\n",
      "Training loss: 1.1650138412203108   297  0.8960292637348175\n",
      "Training loss: 1.153848375592913   298  0.9135455191135406\n",
      "Training loss: 1.1492140463420324   299  0.8882895708084106\n",
      "Training loss: 1.1628214120864868   300  0.8683082163333893\n",
      "Training loss: 1.1428984233311243   301  0.8832645118236542\n",
      "Training loss: 1.1461202672549657   302  0.8920628130435944\n",
      "Training loss: 1.1390366469110762   303  0.8865678608417511\n",
      "Training loss: 1.145768599850791   304  0.8513673692941666\n",
      "Training loss: 1.152751190321786   305  0.8631418347358704\n",
      "Training loss: 1.1422796504838126   306  0.8685759007930756\n",
      "Training loss: 1.1400536043303353   307  0.8717361092567444\n",
      "Training loss: 1.1307435887200492   308  0.8662263751029968\n",
      "Training loss: 1.1439805456570216   309  0.8989304900169373\n",
      "Training loss: 1.1551395654678345   310  0.881380170583725\n",
      "Training loss: 1.1433035135269165   311  0.8925235271453857\n",
      "Training loss: 1.1486851232392448   312  0.9157680571079254\n",
      "Training loss: 1.1452592270714896   313  0.8911587297916412\n",
      "Training loss: 1.140379582132612   314  0.8548754751682281\n",
      "Training loss: 1.1434807777404785   315  0.8659818768501282\n",
      "Training loss: 1.1285476727145058   316  0.8487167656421661\n",
      "Training loss: 1.1295597979000636   317  0.8755165040493011\n",
      "Training loss: 1.1315387317112513   318  0.9078664481639862\n",
      "Training loss: 1.1414801478385925   319  0.876699835062027\n",
      "Training loss: 1.1273756282670158   320  0.8587056696414948\n",
      "Training loss: 1.1145836796079363   321  0.8658075332641602\n",
      "Training loss: 1.121344608919961   322  0.8572624325752258\n",
      "Training loss: 1.1229200618607658   323  0.8730981647968292\n",
      "Training loss: 1.118407232420785   324  0.8673442900180817\n",
      "Training loss: 1.1210049773965562   325  0.8756232261657715\n",
      "Training loss: 1.1196672405515398   326  0.873907059431076\n",
      "Training loss: 1.1172571097101485   327  0.8722372055053711\n",
      "Training loss: 1.1185424966471536   328  0.8364385068416595\n",
      "Training loss: 1.1195599436759949   329  0.8817455470561981\n",
      "Training loss: 1.1140858658722468   330  0.8484283983707428\n",
      "Training loss: 1.1086490324565343   331  0.8571003973484039\n",
      "Training loss: 1.1083502343722753   332  0.8356954455375671\n",
      "Training loss: 1.1096176164490836   333  0.8485323935747147\n",
      "Training loss: 1.1101048673902238   334  0.8674956560134888\n",
      "Training loss: 1.1132143565586634   335  0.8604206442832947\n",
      "Training loss: 1.1072240386690413   336  0.8697809278964996\n",
      "Training loss: 1.0991701696600233   337  0.8567562103271484\n",
      "Training loss: 1.104750394821167   338  0.8729318082332611\n",
      "Training loss: 1.1028732657432556   339  0.8593238294124603\n",
      "Training loss: 1.1235749210630144   340  0.8669883012771606\n",
      "Training loss: 1.1153482369014196   341  0.8716862797737122\n",
      "Training loss: 1.0996386664254325   342  0.852322906255722\n",
      "Training loss: 1.1066503524780273   343  0.8684571981430054\n",
      "Training loss: 1.0960251092910767   344  0.857787549495697\n",
      "Training loss: 1.0881738918168204   345  0.83542300760746\n",
      "Training loss: 1.0979672244616918   346  0.8481747508049011\n",
      "Training loss: 1.0909350173813956   347  0.8361930996179581\n",
      "Training loss: 1.0908527842589788   348  0.8676337599754333\n",
      "Training loss: 1.0896399872643607   349  0.865222692489624\n",
      "Training loss: 1.0880728236266546   350  0.8522960543632507\n",
      "Training loss: 1.0883907079696655   351  0.8413558006286621\n",
      "Training loss: 1.0757620249475752   352  0.8285319805145264\n",
      "Training loss: 1.068991277899061   353  0.8394275009632111\n",
      "Training loss: 1.0621636467320579   354  0.8407351672649384\n",
      "Training loss: 1.0741271546908788   355  0.8461225032806396\n",
      "Training loss: 1.1066008465630668   356  0.8546158373355865\n",
      "Training loss: 1.086589855807168   357  0.8339579105377197\n",
      "Training loss: 1.0772529329572404   358  0.8472668528556824\n",
      "Training loss: 1.0834290470395769   359  0.8691655099391937\n",
      "Training loss: 1.08315343941961   360  0.8237545490264893\n",
      "Training loss: 1.0748000528131212   361  0.8294839262962341\n",
      "Training loss: 1.0725059764725822   362  0.8228980600833893\n",
      "Training loss: 1.0661852955818176   363  0.8099449276924133\n",
      "Training loss: 1.054021954536438   364  0.8208546936511993\n",
      "Training loss: 1.051109790802002   365  0.796062096953392\n",
      "Training loss: 1.0521048264844077   366  0.8354413509368896\n",
      "Training loss: 1.0548070073127747   367  0.7989459335803986\n",
      "Training loss: 1.049651529107775   368  0.8169053494930267\n",
      "Training loss: 1.049071043729782   369  0.8215366750955582\n",
      "Training loss: 1.0478772180421012   370  0.8149411678314209\n",
      "Training loss: 1.0463998189994268   371  0.8017651736736298\n",
      "Training loss: 1.0423877239227295   372  0.8327739834785461\n",
      "Training loss: 1.0492068827152252   373  0.8117684423923492\n",
      "Training loss: 1.040047551904406   374  0.8184581696987152\n",
      "Training loss: 1.0489518003804343   375  0.8318216800689697\n",
      "Training loss: 1.0419845070157732   376  0.815040111541748\n",
      "Training loss: 1.0329669203077043   377  0.8155293762683868\n",
      "Training loss: 1.038068217890603   378  0.8101873993873596\n",
      "Training loss: 1.036661446094513   379  0.8127838969230652\n",
      "Training loss: 1.0287274164812905   380  0.8227038383483887\n",
      "Training loss: 1.0287554093769617   381  0.7991239130496979\n",
      "Training loss: 1.0248902865818568   382  0.8138748556375504\n",
      "Training loss: 1.0166087448596954   383  0.8002461791038513\n",
      "Training loss: 1.0157841656889235   384  0.8157742321491241\n",
      "Training loss: 1.0193845885140556   385  0.789725661277771\n",
      "Training loss: 1.0181869268417358   386  0.7898339480161667\n",
      "Training loss: 1.025556594133377   387  0.7898282110691071\n",
      "Training loss: 1.0145115341459001   388  0.7906940281391144\n",
      "Training loss: 1.0166726580687933   389  0.8086393773555756\n",
      "Training loss: 1.0102332958153315   390  0.7945187091827393\n",
      "Training loss: 1.0193151576178414   391  0.8069049119949341\n",
      "Training loss: 1.028281033039093   392  0.8166155219078064\n",
      "Training loss: 1.0146250554493494   393  0.770036369562149\n",
      "Training loss: 1.016891747713089   394  0.800649955868721\n",
      "Training loss: 1.005993366241455   395  0.7808316349983215\n",
      "Training loss: 0.9965968174593789   396  0.7712041735649109\n",
      "Training loss: 0.9974222906998226   397  0.8086546957492828\n",
      "Training loss: 1.0132046852793013   398  0.7955053746700287\n",
      "Training loss: 1.0171645709446497   399  0.7989203780889511\n",
      "Training loss: 1.0088113461221968   400  0.8181530833244324\n",
      "Training loss: 1.0061091184616089   401  0.8021637201309204\n",
      "Training loss: 1.0023074064935957   402  0.7939673066139221\n",
      "Training loss: 0.9917193268026624   403  0.8006632328033447\n",
      "Training loss: 0.998247253043311   404  0.7872517853975296\n",
      "Training loss: 0.9841713181563786   405  0.7698822915554047\n",
      "Training loss: 0.9788576194218227   406  0.7770079076290131\n",
      "Training loss: 0.9870246819087437   407  0.7782049924135208\n",
      "Training loss: 0.9928844656263079   408  0.7657271474599838\n",
      "Training loss: 0.9809311330318451   409  0.7699142545461655\n",
      "Training loss: 0.9736604733126504   410  0.7833632379770279\n",
      "Training loss: 0.9754029001508441   411  0.8192945122718811\n",
      "Training loss: 1.015514612197876   412  0.7981213331222534\n",
      "Training loss: 0.9991877930504935   413  0.7587106972932816\n",
      "Training loss: 0.9814107503209796   414  0.768673375248909\n",
      "Training loss: 0.9782746647085462   415  0.7666780352592468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.9848984437329429   416  0.7676781117916107\n",
      "Training loss: 0.9705189168453217   417  0.7557244449853897\n",
      "Training loss: 0.9740850499698094   418  0.7618400901556015\n",
      "Training loss: 0.972037558044706   419  0.7506909668445587\n",
      "Training loss: 0.9833469476018634   420  0.7810131162405014\n",
      "Training loss: 0.9607703174863543   421  0.738344594836235\n",
      "Training loss: 0.9632890863077981   422  0.7527330666780472\n",
      "Training loss: 0.952000196490969   423  0.7811207920312881\n",
      "Training loss: 0.9730360124792371   424  0.7653225660324097\n",
      "Training loss: 0.9598700489316668   425  0.7332167625427246\n",
      "Training loss: 0.9533925311905997   426  0.7589065283536911\n",
      "Training loss: 0.9459174360547747   427  0.7542515397071838\n",
      "Training loss: 0.9568435634885516   428  0.7446625083684921\n",
      "Training loss: 0.9510057142802647   429  0.7483092993497849\n",
      "Training loss: 0.9466025105544499   430  0.7543659955263138\n",
      "Training loss: 0.9397634182657514   431  0.7510873675346375\n",
      "Training loss: 0.9915561037404197   432  0.799673393368721\n",
      "Training loss: 0.9833419365542275   433  0.7657221853733063\n",
      "Training loss: 0.9529437933649335   434  0.7772494852542877\n",
      "Training loss: 0.9562988408974239   435  0.7463879436254501\n",
      "Training loss: 0.9623095691204071   436  0.7585500180721283\n",
      "Training loss: 0.9593101697308677   437  0.7424357831478119\n",
      "Training loss: 0.9481296624456134   438  0.7313953042030334\n",
      "Training loss: 0.9456352974687304   439  0.7606159001588821\n",
      "Training loss: 0.9416623328413282   440  0.7405901402235031\n",
      "Training loss: 0.9390390855925423   441  0.7544279396533966\n",
      "Training loss: 0.9380489076886859   442  0.7565799057483673\n",
      "Training loss: 0.93240168264934   443  0.7072081714868546\n",
      "Training loss: 0.9297053260462624   444  0.7359151691198349\n",
      "Training loss: 0.93483995114054   445  0.7307276278734207\n",
      "Training loss: 0.9282154270580837   446  0.7294613569974899\n",
      "Training loss: 0.9310982653072902   447  0.7472258806228638\n",
      "Training loss: 0.9346260172980172   448  0.7396817207336426\n",
      "Training loss: 0.9396020174026489   449  0.7337318062782288\n",
      "Training loss: 0.9295186868735722   450  0.7399615198373795\n",
      "Training loss: 0.9262909165450505   451  0.7452821880578995\n",
      "Training loss: 0.9345375980649676   452  0.75652015209198\n",
      "Training loss: 0.9226229361125401   453  0.7320142388343811\n",
      "Training loss: 0.9180152841976711   454  0.7179464399814606\n",
      "Training loss: 0.9189942819731576   455  0.7158937752246857\n",
      "Training loss: 0.9122113372598376   456  0.7410890311002731\n",
      "Training loss: 0.9126875996589661   457  0.7450448870658875\n",
      "Training loss: 0.9075136695589338   458  0.7184447050094604\n",
      "Training loss: 0.9145074657031468   459  0.7194391041994095\n",
      "Training loss: 0.9108107260295323   460  0.7375594973564148\n",
      "Training loss: 0.9217415281704494   461  0.7261418402194977\n",
      "Training loss: 0.9072444311210087   462  0.7249585539102554\n",
      "Training loss: 0.9114399169172559   463  0.7714915871620178\n",
      "Training loss: 0.9313188024929592   464  0.7412751317024231\n",
      "Training loss: 0.9123454391956329   465  0.7072549611330032\n",
      "Training loss: 0.9002415324960437   466  0.7257899045944214\n",
      "Training loss: 0.8976413990770068   467  0.7360241413116455\n",
      "Training loss: 0.8929268802915301   468  0.7010385543107986\n",
      "Training loss: 0.8969296004090991   469  0.702579528093338\n",
      "Training loss: 0.8871595561504364   470  0.7296486794948578\n",
      "Training loss: 0.896324349301202   471  0.7198899239301682\n",
      "Training loss: 0.9016051931040627   472  0.7137059569358826\n",
      "Training loss: 0.9203001814229148   473  0.7991017699241638\n",
      "Training loss: 0.9828617657933917   474  0.7334413826465607\n",
      "Training loss: 0.9464831309659141   475  0.7532825320959091\n",
      "Training loss: 0.9280323726790292   476  0.7389005571603775\n",
      "Training loss: 0.9207817145756313   477  0.7528602480888367\n",
      "Training loss: 0.9091256473745618   478  0.6972848623991013\n",
      "Training loss: 0.9069385485989707   479  0.7005573511123657\n",
      "Training loss: 0.9044009745121002   480  0.7056047767400742\n",
      "Training loss: 0.8913254184382302   481  0.6781122535467148\n",
      "Training loss: 0.8871810904570988   482  0.7041655629873276\n",
      "Training loss: 0.8934729525021144   483  0.7210901975631714\n",
      "Training loss: 0.8964383218969617   484  0.6954394280910492\n",
      "Training loss: 0.8932414523192814   485  0.6971931010484695\n",
      "Training loss: 0.8808290788105556   486  0.6918922960758209\n",
      "Training loss: 0.8841656531606402   487  0.7107297480106354\n",
      "Training loss: 0.8882705313818795   488  0.6787071079015732\n",
      "Training loss: 0.8888627886772156   489  0.6893812268972397\n",
      "Training loss: 0.8740697460515159   490  0.6787203103303909\n",
      "Training loss: 0.8719448745250702   491  0.7129084914922714\n",
      "Training loss: 0.8866912935461316   492  0.6937599033117294\n",
      "Training loss: 0.9056153169700077   493  0.7353711724281311\n",
      "Training loss: 0.8989479754652295   494  0.7150746434926987\n",
      "Training loss: 0.8770255105836051   495  0.705811083316803\n",
      "Training loss: 0.8729771545955113   496  0.7124953716993332\n",
      "Training loss: 0.862235644033977   497  0.6797888576984406\n",
      "Training loss: 0.8593275887625558   498  0.6821223944425583\n",
      "Training loss: 0.8559834914548057   499  0.6807597875595093\n",
      "Training loss: 0.8567784896918705   500  0.6765569448471069\n",
      "Training loss: 0.8562085245336805   501  0.6817580163478851\n",
      "Training loss: 0.8903605469635555   502  0.7111253440380096\n",
      "Training loss: 0.871187516621181   503  0.6932866871356964\n",
      "Training loss: 0.8636023998260498   504  0.6978416442871094\n",
      "Training loss: 0.8517476149967739   505  0.6843884736299515\n",
      "Training loss: 0.8484181037970951   506  0.6679472178220749\n",
      "Training loss: 0.8657131109918866   507  0.6828022301197052\n",
      "Training loss: 0.84626561829022   508  0.6759916543960571\n",
      "Training loss: 0.846956878900528   509  0.6767224371433258\n",
      "Training loss: 0.8480115064552852   510  0.6812928467988968\n",
      "Training loss: 0.8422388689858573   511  0.6541828960180283\n",
      "Training loss: 0.9749383202620915   512  0.7928958535194397\n",
      "Training loss: 0.988207744700568   513  0.7664786577224731\n",
      "Training loss: 0.9399153547627586   514  0.7202351540327072\n",
      "Training loss: 0.9128092314515796   515  0.7225026488304138\n",
      "Training loss: 0.8965504126889365   516  0.7176719307899475\n",
      "Training loss: 0.8920128175190517   517  0.6791753321886063\n",
      "Training loss: 0.8836635649204254   518  0.7041465193033218\n",
      "Training loss: 0.8816379989896502   519  0.7081557810306549\n",
      "Training loss: 0.8724270846162524   520  0.7160898596048355\n",
      "Training loss: 0.8635625158037458   521  0.6734865009784698\n",
      "Training loss: 0.8606350123882294   522  0.677661806344986\n",
      "Training loss: 0.8693293673651559   523  0.6960235685110092\n",
      "Training loss: 0.8591558762959072   524  0.6916000545024872\n",
      "Training loss: 0.8712881122316632   525  0.7113236337900162\n",
      "Training loss: 0.8627408615180424   526  0.6699712425470352\n",
      "Training loss: 0.8592883178165981   527  0.6683871150016785\n",
      "Training loss: 0.8488704221589225   528  0.682612270116806\n",
      "Training loss: 0.8459738450390952   529  0.6737634539604187\n",
      "Training loss: 0.8357748559543065   530  0.6474528908729553\n",
      "Training loss: 0.845096332686288   531  0.6671074330806732\n",
      "Training loss: 0.8477882955755506   532  0.666208952665329\n",
      "Training loss: 0.8417192016329084   533  0.6787493824958801\n",
      "Training loss: 0.8371329861027854   534  0.6721551567316055\n",
      "Training loss: 0.8297117820807866   535  0.6535439789295197\n",
      "Training loss: 0.8268088442938668   536  0.6686574071645737\n",
      "Training loss: 0.8294595352240971   537  0.6552309691905975\n",
      "Training loss: 0.8218303280217307   538  0.6442748308181763\n",
      "Training loss: 0.8240116579192025   539  0.651775598526001\n",
      "Training loss: 0.8207824868815286   540  0.6507735550403595\n",
      "Training loss: 0.8223071268626622   541  0.6712740212678909\n",
      "Training loss: 0.831667103937694   542  0.6758164167404175\n",
      "Training loss: 0.8445752475942884   543  0.6627568453550339\n",
      "Training loss: 0.8346229323319027   544  0.6479414403438568\n",
      "Training loss: 0.8290133561406817   545  0.6457821875810623\n",
      "Training loss: 0.8173014989921025   546  0.6585656702518463\n",
      "Training loss: 0.817023903131485   547  0.6496165692806244\n",
      "Training loss: 0.8258461441312518   548  0.6423472911119461\n",
      "Training loss: 0.8155637340886253   549  0.6575324535369873\n",
      "Training loss: 0.8201251200267247   550  0.6609349995851517\n",
      "Training loss: 0.8248855429036277   551  0.6564941853284836\n",
      "Training loss: 0.811743152993066   552  0.6470564752817154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8075224374021802   553  0.6574635058641434\n",
      "Training loss: 0.8067919399057116   554  0.6358129531145096\n",
      "Training loss: 0.8009189580168042   555  0.6524304747581482\n",
      "Training loss: 0.7974635532924107   556  0.6301226317882538\n",
      "Training loss: 0.7976865087236676   557  0.6393444240093231\n",
      "Training loss: 0.7955483453614371   558  0.6354949325323105\n",
      "Training loss: 0.7885213025978633   559  0.6496177762746811\n",
      "Training loss: 0.7959200825010028   560  0.6285674273967743\n",
      "Training loss: 0.7952002627508981   561  0.6251463145017624\n",
      "Training loss: 0.7959935878004346   562  0.627207413315773\n",
      "Training loss: 0.7956030155931201   563  0.6449263691902161\n",
      "Training loss: 0.813829847744533   564  0.6677771657705307\n",
      "Training loss: 0.8149447185652596   565  0.6516079157590866\n",
      "Training loss: 0.8000508802277702   566  0.6259519457817078\n",
      "Training loss: 0.7850808075496128   567  0.6417728811502457\n",
      "Training loss: 0.7804887890815735   568  0.6218893229961395\n",
      "Training loss: 0.789608610527856   569  0.605542927980423\n",
      "Training loss: 0.7856632002762386   570  0.6243248134851456\n",
      "Training loss: 0.7864521486418588   571  0.6303349286317825\n",
      "Training loss: 0.7986925499779838   572  0.6531915962696075\n",
      "Training loss: 0.8120998357023511   573  0.6529649347066879\n",
      "Training loss: 0.7931968654905047   574  0.6323097795248032\n",
      "Training loss: 0.7877308513436999   575  0.6289695799350739\n",
      "Training loss: 0.778456802879061   576  0.6000445485115051\n",
      "Training loss: 0.7817915039403098   577  0.6078354865312576\n",
      "Training loss: 0.7769854537078312   578  0.6124564111232758\n",
      "Training loss: 0.7793629467487335   579  0.639690101146698\n",
      "Training loss: 0.7791999365602221   580  0.6189742833375931\n",
      "Training loss: 0.7751962457384381   581  0.620672345161438\n",
      "Training loss: 0.7714846517358508   582  0.6141780465841293\n",
      "Training loss: 0.7705521242959159   583  0.6149169206619263\n",
      "Training loss: 0.7660248364721026   584  0.6197236627340317\n",
      "Training loss: 0.7729808262416294   585  0.6104946434497833\n",
      "Training loss: 0.7606717433248248   586  0.5922407656908035\n",
      "Training loss: 0.7623191859040942   587  0.6136712580919266\n",
      "Training loss: 0.761674416916711   588  0.5972262471914291\n",
      "Training loss: 0.7710092493465969   589  0.6052349954843521\n",
      "Training loss: 0.7731818514210838   590  0.6239283680915833\n",
      "Training loss: 0.7831436736243111   591  0.6522642374038696\n",
      "Training loss: 0.782779335975647   592  0.6211736500263214\n",
      "Training loss: 0.764378479548863   593  0.6128766536712646\n",
      "Training loss: 0.7577346350465503   594  0.6053141355514526\n",
      "Training loss: 0.766546117407935   595  0.6147074550390244\n",
      "Training loss: 0.7713258819920676   596  0.6243228167295456\n",
      "Training loss: 0.7604999371937343   597  0.6133294105529785\n",
      "Training loss: 0.769161513873509   598  0.6136382669210434\n",
      "Training loss: 0.764806547335216   599  0.592035174369812\n",
      "Training loss: 0.7559004553726741   600  0.6188852787017822\n",
      "Training loss: 0.7555580266884395   601  0.5901357978582382\n",
      "Training loss: 0.7549169191292354   602  0.6022324115037918\n",
      "Training loss: 0.7520649944032941   603  0.6037518382072449\n",
      "Training loss: 0.7532984188624791   604  0.6085177361965179\n",
      "Training loss: 0.7523274677140372   605  0.6012066304683685\n",
      "Training loss: 0.7546428399426597   606  0.6137130707502365\n",
      "Training loss: 0.7492416713918958   607  0.5805577039718628\n",
      "Training loss: 0.7420697552817208   608  0.59671251475811\n",
      "Training loss: 0.7493186167308262   609  0.6011453568935394\n",
      "Training loss: 0.7414399342877525   610  0.5988192707300186\n",
      "Training loss: 0.7502185106277466   611  0.6024048179388046\n",
      "Training loss: 0.752214504139764   612  0.6012446731328964\n",
      "Training loss: 0.7536242689405169   613  0.6064910143613815\n",
      "Training loss: 0.7435297199657985   614  0.5931498110294342\n",
      "Training loss: 0.741520379270826   615  0.6029683500528336\n",
      "Training loss: 0.7341654641287667   616  0.5980563461780548\n",
      "Training loss: 0.7353205297674451   617  0.5958360284566879\n",
      "Training loss: 0.7374573392527444   618  0.6011499166488647\n",
      "Training loss: 0.7383468278816768   619  0.6007090657949448\n",
      "Training loss: 0.7719746955803463   620  0.6319781392812729\n",
      "Training loss: 0.7996102401188442   621  0.640641063451767\n",
      "Training loss: 0.7908161665712085   622  0.623707726597786\n",
      "Training loss: 0.7740268324102674   623  0.611113429069519\n",
      "Training loss: 0.7554454505443573   624  0.6068769246339798\n",
      "Training loss: 0.7474700595651355   625  0.5710873305797577\n",
      "Training loss: 0.7427485840661185   626  0.5936223119497299\n",
      "Training loss: 0.7349925764969417   627  0.5928868651390076\n",
      "Training loss: 0.7328851308141436   628  0.5672229826450348\n",
      "Training loss: 0.7343094263757978   629  0.5689272284507751\n",
      "Training loss: 0.7316056021622249   630  0.5791611820459366\n",
      "Training loss: 0.7260512241295406   631  0.5821117907762527\n",
      "Training loss: 0.725737201316016   632  0.5718927085399628\n",
      "Training loss: 0.7243682614394596   633  0.5736070275306702\n",
      "Training loss: 0.73196998664311   634  0.6030834168195724\n",
      "Training loss: 0.743187108210155   635  0.598917543888092\n",
      "Training loss: 0.738370669739587   636  0.590082049369812\n",
      "Training loss: 0.7611992997782571   637  0.5932605713605881\n",
      "Training loss: 0.7474161897386823   638  0.5983885079622269\n",
      "Training loss: 0.7279553881713322   639  0.5975923091173172\n",
      "Training loss: 0.7257382571697235   640  0.5803950428962708\n",
      "Training loss: 0.7215608528682164   641  0.5854447484016418\n",
      "Training loss: 0.7289613655635289   642  0.5839715152978897\n",
      "Training loss: 0.7205600610801152   643  0.5810313522815704\n",
      "Training loss: 0.7154706248215267   644  0.5751434713602066\n",
      "Training loss: 0.7229089311191014   645  0.5907011479139328\n",
      "Training loss: 0.7250747680664062   646  0.5746183693408966\n",
      "Training loss: 0.7256250338894981   647  0.5676168352365494\n",
      "Training loss: 0.7200710305145809   648  0.5814113467931747\n",
      "Training loss: 0.71586931177548   649  0.5732245594263077\n",
      "Training loss: 0.716252190726144   650  0.5760454833507538\n",
      "Training loss: 0.7090839871338436   651  0.5655096769332886\n",
      "Training loss: 0.7133649928229195   652  0.5806826055049896\n",
      "Training loss: 0.7092092420373645   653  0.5424653440713882\n",
      "Training loss: 0.7124283952372414   654  0.5703565180301666\n",
      "Training loss: 0.7079464495182037   655  0.586102619767189\n",
      "Training loss: 0.7123951315879822   656  0.5909687727689743\n",
      "Training loss: 0.7363289339201791   657  0.6037069261074066\n",
      "Training loss: 0.7356371113232204   658  0.5685543864965439\n",
      "Training loss: 0.7223659328051976   659  0.5873953700065613\n",
      "Training loss: 0.7230193231787   660  0.5901040732860565\n",
      "Training loss: 0.7213620926652636   661  0.58128622174263\n",
      "Training loss: 0.825902304479054   662  0.6661182790994644\n",
      "Training loss: 0.8224321263177055   663  0.6338544338941574\n",
      "Training loss: 0.7781296500137874   664  0.602337047457695\n",
      "Training loss: 0.7497855595179966   665  0.6108129024505615\n",
      "Training loss: 0.7406486570835114   666  0.5849880427122116\n",
      "Training loss: 0.7363760684217725   667  0.6089277416467667\n",
      "Training loss: 0.7257933446339199   668  0.5868028849363327\n",
      "Training loss: 0.7276063476290021   669  0.6015813648700714\n",
      "Training loss: 0.7226145395210811   670  0.5700826793909073\n",
      "Training loss: 0.7173691945416587   671  0.5847537070512772\n",
      "Training loss: 0.7140092551708221   672  0.5723103284835815\n",
      "Training loss: 0.7112788898604256   673  0.5622999221086502\n",
      "Training loss: 0.7106667075838361   674  0.576079934835434\n",
      "Training loss: 0.7145726723330361   675  0.5776552706956863\n",
      "Training loss: 0.7108941887106214   676  0.5500273555517197\n",
      "Training loss: 0.705104968377522   677  0.5647142827510834\n",
      "Training loss: 0.6997362417834145   678  0.5566110014915466\n",
      "Training loss: 0.7056862584182194   679  0.5558275580406189\n",
      "Training loss: 0.70025087254388   680  0.5654887557029724\n",
      "Training loss: 0.7105069203036172   681  0.5704622864723206\n",
      "Training loss: 0.707650865827288   682  0.565111055970192\n",
      "Training loss: 0.7053645764078412   683  0.5687593817710876\n",
      "Training loss: 0.7009688402925219   684  0.5791566669940948\n",
      "Training loss: 0.7054648527077266   685  0.5794315934181213\n",
      "Training loss: 0.7124736436775753   686  0.5546073317527771\n",
      "Training loss: 0.7108354015009744   687  0.560933917760849\n",
      "Training loss: 0.7053621539047786   688  0.5843216627836227\n",
      "Training loss: 0.7078967988491058   689  0.561638280749321\n",
      "Training loss: 0.7147245194230761   690  0.5783976763486862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7327906319073269   691  0.5713682174682617\n",
      "Training loss: 0.7110129339354379   692  0.5748700797557831\n",
      "Training loss: 0.7073609360626766   693  0.5734978318214417\n",
      "Training loss: 0.6993221683161599   694  0.5631289929151535\n",
      "Training loss: 0.70525204709598   695  0.5611448138952255\n",
      "Training loss: 0.7053796180656978   696  0.5643362551927567\n",
      "Training loss: 0.6904613205364772   697  0.5435783416032791\n",
      "Training loss: 0.6907821893692017   698  0.5604518055915833\n",
      "Training loss: 0.6941729954310826   699  0.5503665506839752\n",
      "Training loss: 0.6907979377678463   700  0.5648224651813507\n",
      "Training loss: 0.6929506829806736   701  0.5533275306224823\n",
      "Training loss: 0.6884199380874634   702  0.550046518445015\n",
      "Training loss: 0.686895625931876   703  0.5666636973619461\n",
      "Training loss: 0.6839477845600673   704  0.5584440529346466\n",
      "Training loss: 0.6832717061042786   705  0.5624358952045441\n",
      "Training loss: 0.6906852509294238   706  0.555512860417366\n",
      "Training loss: 0.6862948451723371   707  0.558401420712471\n",
      "Training loss: 0.6830304222447532   708  0.5340120494365692\n",
      "Training loss: 0.6878605357238224   709  0.5585442036390305\n",
      "Training loss: 0.6851545316832406   710  0.5513418167829514\n",
      "Training loss: 0.680585537637983   711  0.5434957891702652\n",
      "Training loss: 0.6830298219408307   712  0.5477924942970276\n",
      "Training loss: 0.6840113954884666   713  0.5517624914646149\n",
      "Training loss: 0.679600362266813   714  0.5558658838272095\n",
      "Training loss: 0.6760597697326115   715  0.5384523719549179\n",
      "Training loss: 0.6726432144641876   716  0.539919376373291\n",
      "Training loss: 0.6752613271985736   717  0.5487215518951416\n",
      "Training loss: 0.6794527002743312   718  0.5437860935926437\n",
      "Training loss: 0.6810783701283591   719  0.5571113973855972\n",
      "Training loss: 0.6767392924853733   720  0.573191225528717\n",
      "Training loss: 0.6997986010142735   721  0.557547077536583\n",
      "Training loss: 0.6912031216280801   722  0.5590633451938629\n",
      "Training loss: 0.6799661602292743   723  0.5500536113977432\n",
      "Training loss: 0.6776500684874398   724  0.5428609102964401\n",
      "Training loss: 0.6735366795744214   725  0.5383197963237762\n",
      "Training loss: 0.677737125328609   726  0.5551473796367645\n",
      "Training loss: 0.6859692590577262   727  0.5484344959259033\n",
      "Training loss: 0.6796852605683463   728  0.5354998707771301\n",
      "Training loss: 0.6764551826885769   729  0.5650991648435593\n",
      "Training loss: 0.6867091613156455   730  0.5589889287948608\n",
      "Training loss: 0.6761979290417263   731  0.5355702191591263\n",
      "Training loss: 0.6713932880333492   732  0.5533579736948013\n",
      "Training loss: 0.6764268491949353   733  0.5556076467037201\n",
      "Training loss: 0.6703970985753196   734  0.5573393851518631\n",
      "Training loss: 0.6675389792237963   735  0.5251018851995468\n",
      "Training loss: 0.6689111973558154   736  0.5387103259563446\n",
      "Training loss: 0.6698744297027588   737  0.5385672897100449\n",
      "Training loss: 0.6689562329224178   738  0.550227627158165\n",
      "Training loss: 0.6697353890963963   739  0.5456429421901703\n",
      "Training loss: 0.6640634153570447   740  0.5362488478422165\n",
      "Training loss: 0.663783039365496   741  0.5241021811962128\n",
      "Training loss: 0.6609412474291665   742  0.5380188077688217\n",
      "Training loss: 0.6581943503447941   743  0.5451598465442657\n",
      "Training loss: 0.6623622902802059   744  0.5345344096422195\n",
      "Training loss: 0.6547823761190686   745  0.5223429501056671\n",
      "Training loss: 0.6508552559784481   746  0.5341223180294037\n",
      "Training loss: 0.6585067382880619   747  0.5240059643983841\n",
      "Training loss: 0.6606420065675463   748  0.5382407754659653\n",
      "Training loss: 0.6567922404834202   749  0.5303067266941071\n",
      "Training loss: 0.6540151579039437   750  0.5217159390449524\n",
      "Training loss: 0.6534831140722547   751  0.5263816118240356\n",
      "Training loss: 0.6586220860481262   752  0.5335841178894043\n",
      "Training loss: 0.6592074504920414   753  0.5394375324249268\n",
      "Training loss: 0.6596919511045728   754  0.541063666343689\n",
      "Training loss: 0.6657949984073639   755  0.5425670444965363\n",
      "Training loss: 0.6596710469041552   756  0.5389518886804581\n",
      "Training loss: 0.651567816734314   757  0.5449320524930954\n",
      "Training loss: 0.6554947282586779   758  0.5302454084157944\n",
      "Training loss: 0.6567105778626033   759  0.5436893552541733\n",
      "Training loss: 0.6542284446103233   760  0.5222359001636505\n",
      "Training loss: 0.655335226229259   761  0.5282581001520157\n",
      "Training loss: 0.6512385393892016   762  0.545736774802208\n",
      "Training loss: 0.6531620834554944   763  0.5347380191087723\n",
      "Training loss: 0.6531954279967717   764  0.5183218270540237\n",
      "Training loss: 0.6538407589708056   765  0.5357697904109955\n",
      "Training loss: 0.6444248003619057   766  0.5240059345960617\n",
      "Training loss: 0.6514634575162616   767  0.5326975286006927\n",
      "Training loss: 0.644893582378115   768  0.5427118837833405\n",
      "Training loss: 0.6530644595623016   769  0.5358311384916306\n",
      "Training loss: 0.6484581572668893   770  0.5311379581689835\n",
      "Training loss: 0.652346555675779   771  0.5405218452215195\n",
      "Training loss: 0.6455637812614441   772  0.5219927877187729\n",
      "Training loss: 0.6446697328771863   773  0.5296641290187836\n",
      "Training loss: 0.6450396605900356   774  0.5416191071271896\n",
      "Training loss: 0.6430401078292302   775  0.5311455428600311\n",
      "Training loss: 0.6478393205574581   776  0.5355664789676666\n",
      "Training loss: 0.6437068964753833   777  0.5446052998304367\n",
      "Training loss: 0.6587854453495571   778  0.5360236614942551\n",
      "Training loss: 0.6553843404565539   779  0.5306855887174606\n",
      "Training loss: 0.6503324849264962   780  0.5136828273534775\n",
      "Training loss: 0.6437480875423977   781  0.5263544619083405\n",
      "Training loss: 0.64228612610272   782  0.5234167873859406\n",
      "Training loss: 0.6419903252805982   783  0.5257744342088699\n",
      "Training loss: 0.6520724977765765   784  0.5510382652282715\n",
      "Training loss: 0.6457199113709586   785  0.5204268395900726\n",
      "Training loss: 0.6458148828574589   786  0.5196558833122253\n",
      "Training loss: 0.6410802475043705   787  0.5269440710544586\n",
      "Training loss: 0.6341425989355359   788  0.5270660668611526\n",
      "Training loss: 0.6370467501027244   789  0.5230451822280884\n",
      "Training loss: 0.6416446609156472   790  0.5267193913459778\n",
      "Training loss: 0.6399167052337101   791  0.5120607167482376\n",
      "Training loss: 0.6463108871664319   792  0.5189963132143021\n",
      "Training loss: 0.6428381502628326   793  0.5346911549568176\n",
      "Training loss: 0.6397042487348829   794  0.5070454776287079\n",
      "Training loss: 0.6335056637014661   795  0.5122814625501633\n",
      "Training loss: 0.6309772389275687   796  0.5058040469884872\n",
      "Training loss: 0.6323313329901014   797  0.5194010734558105\n",
      "Training loss: 0.6301795669964382   798  0.5066500306129456\n",
      "Training loss: 0.62973895243236   799  0.5313289910554886\n",
      "Training loss: 0.6296792796679905   800  0.5252069681882858\n",
      "Training loss: 0.6349237050328936   801  0.5234256386756897\n",
      "Training loss: 0.6325804122856685   802  0.5191318839788437\n",
      "Training loss: 0.6504829057625362   803  0.5324369221925735\n",
      "Training loss: 0.6398245351655143   804  0.5166231691837311\n",
      "Training loss: 0.6339748757226127   805  0.5162566602230072\n",
      "Training loss: 0.6361083175454821   806  0.5357266366481781\n",
      "Training loss: 0.6405364402702877   807  0.531097874045372\n",
      "Training loss: 0.6310302700315203   808  0.5135560631752014\n",
      "Training loss: 0.6308631386075702   809  0.516472339630127\n",
      "Training loss: 0.6321180633136204   810  0.5083288848400116\n",
      "Training loss: 0.6278253836291177   811  0.5094709098339081\n",
      "Training loss: 0.624460003205708   812  0.5165842324495316\n",
      "Training loss: 0.6396343410015106   813  0.5291313976049423\n",
      "Training loss: 0.6404066639287131   814  0.5171130299568176\n",
      "Training loss: 0.6337265712874276   815  0.5357773751020432\n",
      "Training loss: 0.6318298067365374   816  0.5175647139549255\n",
      "Training loss: 0.6245048769882747   817  0.5211943238973618\n",
      "Training loss: 0.6278656806264605   818  0.5140124708414078\n",
      "Training loss: 0.6272176802158356   819  0.5208353102207184\n",
      "Training loss: 0.6276270917483738   820  0.5082806497812271\n",
      "Training loss: 0.621746906212398   821  0.5216722041368484\n",
      "Training loss: 0.6197996011802128   822  0.5367993414402008\n",
      "Training loss: 0.6246555021830967   823  0.4978740066289902\n",
      "Training loss: 0.6275595809732165   824  0.51838119328022\n",
      "Training loss: 0.6218832731246948   825  0.5121137797832489\n",
      "Training loss: 0.6190652506692069   826  0.5222320705652237\n",
      "Training loss: 0.6205681690147945   827  0.5098732262849808\n",
      "Training loss: 0.6144792820726123   828  0.5367823392152786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6199512183666229   829  0.5113823860883713\n",
      "Training loss: 0.6175631497587476   830  0.5022210627794266\n",
      "Training loss: 0.6196096667221614   831  0.5152979791164398\n",
      "Training loss: 0.6187792292663029   832  0.5104581117630005\n",
      "Training loss: 0.6330355320658002   833  0.5673601180315018\n",
      "Training loss: 0.6725568899086544   834  0.5498537421226501\n",
      "Training loss: 0.6466471723147801   835  0.513456255197525\n",
      "Training loss: 0.6640673407486507   836  0.5595345348119736\n",
      "Training loss: 0.6596884684903281   837  0.5219593495130539\n",
      "Training loss: 0.6460983923503331   838  0.5271623283624649\n",
      "Training loss: 0.6312957789216723   839  0.5184099078178406\n",
      "Training loss: 0.6297698106084552   840  0.49591875076293945\n",
      "Training loss: 0.6314454419272286   841  0.5096138268709183\n",
      "Training loss: 0.6245046428271702   842  0.5269083082675934\n",
      "Training loss: 0.6259117722511292   843  0.5221398621797562\n",
      "Training loss: 0.6216715574264526   844  0.5028618276119232\n",
      "Training loss: 0.6259739739554269   845  0.5212056636810303\n",
      "Training loss: 0.6191113804067884   846  0.5006246864795685\n",
      "Training loss: 0.6137407081467765   847  0.49493569135665894\n",
      "Training loss: 0.6178155371120998   848  0.5107296705245972\n",
      "Training loss: 0.6174995005130768   849  0.5086300075054169\n",
      "Training loss: 0.6158984473773411   850  0.5109265446662903\n",
      "Training loss: 0.618751585483551   851  0.5036939382553101\n",
      "Training loss: 0.6221824671540942   852  0.5122993290424347\n",
      "Training loss: 0.6185135628495898   853  0.50648432970047\n",
      "Training loss: 0.6188726212297168   854  0.524171769618988\n",
      "Training loss: 0.6124936555113111   855  0.4928111582994461\n",
      "Training loss: 0.6101676481110709   856  0.49787865579128265\n",
      "Training loss: 0.6118980305535453   857  0.5027277618646622\n",
      "Training loss: 0.618923796074731   858  0.5134255737066269\n",
      "Training loss: 0.6160083327974591   859  0.5084412544965744\n",
      "Training loss: 0.6133769580296108   860  0.5084577798843384\n",
      "Training loss: 0.6106966052736554   861  0.5197286158800125\n",
      "Training loss: 0.60617938211986   862  0.5030035674571991\n",
      "Training loss: 0.6069944288049426   863  0.48606064915657043\n",
      "Training loss: 0.6120415031909943   864  0.4954538494348526\n",
      "Training loss: 0.6046719167913709   865  0.5153349488973618\n",
      "Training loss: 0.6085627206734249   866  0.4968031197786331\n",
      "Training loss: 0.6082663706370762   867  0.518929660320282\n",
      "Training loss: 0.6047528258391789   868  0.4983281344175339\n",
      "Training loss: 0.6090217743601117   869  0.5063712894916534\n",
      "Training loss: 0.6117761858872005   870  0.49955159425735474\n",
      "Training loss: 0.6144928761890956   871  0.5082130581140518\n",
      "Training loss: 0.6160371644156319   872  0.5108647346496582\n",
      "Training loss: 0.6227949772562299   873  0.5225005149841309\n",
      "Training loss: 0.6551085199628558   874  0.5266091823577881\n",
      "Training loss: 0.6317365339824131   875  0.5138660967350006\n",
      "Training loss: 0.6182111970015934   876  0.49662190675735474\n",
      "Training loss: 0.6190398463181087   877  0.5128324925899506\n",
      "Training loss: 0.6105288224560874   878  0.5118884295225143\n",
      "Training loss: 0.6115038394927979   879  0.48760728538036346\n",
      "Training loss: 0.6047139082636152   880  0.5024041533470154\n",
      "Training loss: 0.5982896983623505   881  0.491119384765625\n",
      "Training loss: 0.6025865886892591   882  0.5005416870117188\n",
      "Training loss: 0.5975821997438159   883  0.48580506443977356\n",
      "Training loss: 0.5998431188719613   884  0.48296697437763214\n",
      "Training loss: 0.5987956609044757   885  0.4885021150112152\n",
      "Training loss: 0.6060977152415684   886  0.4917679578065872\n",
      "Training loss: 0.6020915380546025   887  0.5000847578048706\n",
      "Training loss: 0.5972432110990796   888  0.49348728358745575\n",
      "Training loss: 0.5993152196918216   889  0.4925222396850586\n",
      "Training loss: 0.6048533703599658   890  0.49694758653640747\n",
      "Training loss: 0.60407383952822   891  0.49340441823005676\n",
      "Training loss: 0.5997839527470725   892  0.4860137552022934\n",
      "Training loss: 0.6074195631912777   893  0.5159382224082947\n",
      "Training loss: 0.5997719935008458   894  0.47839657962322235\n",
      "Training loss: 0.6014406893934522   895  0.48934848606586456\n",
      "Training loss: 0.5993437809603555   896  0.4949035495519638\n",
      "Training loss: 0.6004256222929273   897  0.5173453539609909\n",
      "Training loss: 0.5994328090122768   898  0.4975071847438812\n",
      "Training loss: 0.5940251733575549   899  0.49501679837703705\n",
      "Training loss: 0.5964184829166957   900  0.49750542640686035\n",
      "Training loss: 0.5969892910548619   901  0.504410520195961\n",
      "Training loss: 0.5990140565804073   902  0.4950695037841797\n",
      "Training loss: 0.5906391782420022   903  0.488338902592659\n",
      "Training loss: 0.5925456200327192   904  0.4948720484972\n",
      "Training loss: 0.589737091745649   905  0.5036327391862869\n",
      "Training loss: 0.5938713337693896   906  0.49943289160728455\n",
      "Training loss: 0.5958848467894963   907  0.49321000277996063\n",
      "Training loss: 0.5993313704218183   908  0.489308625459671\n",
      "Training loss: 0.5913057050534657   909  0.4964991360902786\n",
      "Training loss: 0.5927991569042206   910  0.5009017288684845\n",
      "Training loss: 0.5995018397058759   911  0.4826059937477112\n",
      "Training loss: 0.5946913276399884   912  0.5015207231044769\n",
      "Training loss: 0.5904006830283574   913  0.4920148402452469\n",
      "Training loss: 0.5912188078675952   914  0.4994131177663803\n",
      "Training loss: 0.5907533317804337   915  0.49614521861076355\n",
      "Training loss: 0.590739905834198   916  0.49686357378959656\n",
      "Training loss: 0.5938500549112048   917  0.48309724032878876\n",
      "Training loss: 0.5897047179085868   918  0.4924708604812622\n",
      "Training loss: 0.5881769401686532   919  0.484000563621521\n",
      "Training loss: 0.5830020776816777   920  0.4902219921350479\n",
      "Training loss: 0.5941008073943002   921  0.48583056032657623\n",
      "Training loss: 0.5904242822102138   922  0.49822036921977997\n",
      "Training loss: 0.5880607111113412   923  0.4819422513246536\n",
      "Training loss: 0.5803439276559013   924  0.48398298025131226\n",
      "Training loss: 0.5870922803878784   925  0.5048351436853409\n",
      "Training loss: 0.592553687947137   926  0.48483942449092865\n",
      "Training loss: 0.5944377183914185   927  0.49697548151016235\n",
      "Training loss: 0.5864552812916892   928  0.49687790870666504\n",
      "Training loss: 0.585083931684494   929  0.4770358055830002\n",
      "Training loss: 0.5829245575836727   930  0.48404307663440704\n",
      "Training loss: 0.5817688235214779   931  0.5041041970252991\n",
      "Training loss: 0.5841431404863086   932  0.48858867585659027\n",
      "Training loss: 0.5878272567476545   933  0.4829368591308594\n",
      "Training loss: 0.5809269164289746   934  0.4827727973461151\n",
      "Training loss: 0.5789567487580436   935  0.4541305601596832\n",
      "Training loss: 0.5801622654710498   936  0.4907356947660446\n",
      "Training loss: 0.5775414279529026   937  0.4881029725074768\n",
      "Training loss: 0.5775362082890102   938  0.4846072942018509\n",
      "Training loss: 0.5835736904825483   939  0.4984355717897415\n",
      "Training loss: 0.583758567060743   940  0.4815213084220886\n",
      "Training loss: 0.5892999001911708   941  0.5075327008962631\n",
      "Training loss: 0.5821027670587812   942  0.4647251069545746\n",
      "Training loss: 0.5820301473140717   943  0.4679232984781265\n",
      "Training loss: 0.5780175839151654   944  0.4858621507883072\n",
      "Training loss: 0.5810688436031342   945  0.470386803150177\n",
      "Training loss: 0.5778021344116756   946  0.4915241152048111\n",
      "Training loss: 0.573962641613824   947  0.4901925176382065\n",
      "Training loss: 0.5848010352679661   948  0.48540346324443817\n",
      "Training loss: 0.5823997003691537   949  0.4975031763315201\n",
      "Training loss: 0.5810415574482509   950  0.48016490042209625\n",
      "Training loss: 0.5782217000211988   951  0.48101767897605896\n",
      "Training loss: 0.5798084565571376   952  0.4853118658065796\n",
      "Training loss: 0.5719620670591082   953  0.4884287118911743\n",
      "Training loss: 0.574041085583823   954  0.47691304981708527\n",
      "Training loss: 0.5741898332323346   955  0.47034843266010284\n",
      "Training loss: 0.5739723443984985   956  0.47937391698360443\n",
      "Training loss: 0.5760075151920319   957  0.49143169820308685\n",
      "Training loss: 0.5767341469015393   958  0.4711327850818634\n",
      "Training loss: 0.578947548355375   959  0.48055461049079895\n",
      "Training loss: 0.5714098811149597   960  0.4835979491472244\n",
      "Training loss: 0.5727981541837964   961  0.492549866437912\n",
      "Training loss: 0.5765418325151715   962  0.48608751595020294\n",
      "Training loss: 0.5732851177453995   963  0.47366122901439667\n",
      "Training loss: 0.573719893183027   964  0.4873577356338501\n",
      "Training loss: 0.5745889885084969   965  0.4774394780397415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5710181721619197   966  0.48413753509521484\n",
      "Training loss: 0.5725545968328204   967  0.4678611308336258\n",
      "Training loss: 0.5713347579751696   968  0.4726055711507797\n",
      "Training loss: 0.5692391352994102   969  0.47276873886585236\n",
      "Training loss: 0.5695332969938006   970  0.4923999160528183\n",
      "Training loss: 0.5750727610928672   971  0.47156156599521637\n",
      "Training loss: 0.5745829897267478   972  0.5220858156681061\n",
      "Training loss: 0.6028435145105634   973  0.48622655868530273\n",
      "Training loss: 0.5866018320832934   974  0.47276605665683746\n",
      "Training loss: 0.5760755964687893   975  0.47998271882534027\n",
      "Training loss: 0.5717077936444964   976  0.4748797118663788\n",
      "Training loss: 0.571132732289178   977  0.4751490205526352\n",
      "Training loss: 0.569406053849629   978  0.47289493680000305\n",
      "Training loss: 0.5730758862836021   979  0.46166467666625977\n",
      "Training loss: 0.5691882542201451   980  0.47747087478637695\n",
      "Training loss: 0.5662833367075238   981  0.47949910163879395\n",
      "Training loss: 0.5707340368202755   982  0.4735957533121109\n",
      "Training loss: 0.5670980300222125   983  0.4857911616563797\n",
      "Training loss: 0.5665341871125358   984  0.47237564623355865\n",
      "Training loss: 0.5655787480728967   985  0.4623027592897415\n",
      "Training loss: 0.5681215439523969   986  0.4737183451652527\n",
      "Training loss: 0.5653599223920277   987  0.46951620280742645\n",
      "Training loss: 0.5627205414431435   988  0.472810834646225\n",
      "Training loss: 0.5683850986616952   989  0.47736112773418427\n",
      "Training loss: 0.5664348006248474   990  0.47705815732479095\n",
      "Training loss: 0.5647489173071725   991  0.47717592120170593\n",
      "Training loss: 0.565245794398444   992  0.4607715457677841\n",
      "Training loss: 0.5621247376714434   993  0.4687008857727051\n",
      "Training loss: 0.5598837520395007   994  0.47118043899536133\n",
      "Training loss: 0.5610982860837664   995  0.4581361562013626\n",
      "Training loss: 0.5605229543788093   996  0.4771953821182251\n",
      "Training loss: 0.563490514244352   997  0.4713483899831772\n",
      "Training loss: 0.5647984402520316   998  0.470899373292923\n",
      "Training loss: 0.5624805050236839   999  0.4721771329641342\n",
      "Training loss: 0.5644712746143341   1000  0.48326030373573303\n",
      "Training loss: 0.5630535653659275   1001  0.47357869148254395\n",
      "Training loss: 0.5687009053570884   1002  0.4741128236055374\n",
      "Training loss: 0.5654334745236805   1003  0.48657645285129547\n",
      "Training loss: 0.5641371011734009   1004  0.4875294417142868\n",
      "Training loss: 0.5587828755378723   1005  0.4760274142026901\n",
      "Training loss: 0.5578660411494119   1006  0.47821612656116486\n",
      "Training loss: 0.5586948905672345   1007  0.46809637546539307\n",
      "Training loss: 0.5517379620245525   1008  0.465138703584671\n",
      "Training loss: 0.5540217501776559   1009  0.4658559709787369\n",
      "Training loss: 0.5549202050481524   1010  0.46455085277557373\n",
      "Training loss: 0.5544000225407737   1011  0.47143055498600006\n",
      "Training loss: 0.5540271699428558   1012  0.4757715165615082\n",
      "Training loss: 0.5535479145390647   1013  0.45108312368392944\n",
      "Training loss: 0.5587015237127032   1014  0.4816044718027115\n",
      "Training loss: 0.5603940827505929   1015  0.47094421088695526\n",
      "Training loss: 0.5678112932613918   1016  0.4838775098323822\n",
      "Training loss: 0.5651125865323203   1017  0.4738018661737442\n",
      "Training loss: 0.5612185554844993   1018  0.4731825143098831\n",
      "Training loss: 0.5654789507389069   1019  0.4667760580778122\n",
      "Training loss: 0.5545509670461927   1020  0.4574939161539078\n",
      "Training loss: 0.5490272896630424   1021  0.4494882822036743\n",
      "Training loss: 0.554338350892067   1022  0.469928115606308\n",
      "Training loss: 0.5519150027206966   1023  0.47886717319488525\n",
      "Training loss: 0.5524460886205945   1024  0.4658135622739792\n",
      "Training loss: 0.5515057700020927   1025  0.4712733030319214\n",
      "Training loss: 0.5558716143880572   1026  0.46878983080387115\n",
      "Training loss: 0.5533260149615151   1027  0.483884260058403\n",
      "Training loss: 0.5646449370043618   1028  0.4767127186059952\n",
      "Training loss: 0.5606321500880378   1029  0.46523718535900116\n",
      "Training loss: 0.5593732595443726   1030  0.4664859175682068\n",
      "Training loss: 0.5611214254583631   1031  0.47626569867134094\n",
      "Training loss: 0.5753300956317356   1032  0.47658562660217285\n",
      "Training loss: 0.5619055884225028   1033  0.468296617269516\n",
      "Training loss: 0.5567941750798907   1034  0.46106186509132385\n",
      "Training loss: 0.5514836226190839   1035  0.4646032303571701\n",
      "Training loss: 0.5551591260092599   1036  0.4783543646335602\n",
      "Training loss: 0.5617491688047137   1037  0.46218056976795197\n",
      "Training loss: 0.5571292979376656   1038  0.46577273309230804\n",
      "Training loss: 0.5604733824729919   1039  0.48439255356788635\n",
      "Training loss: 0.5569424033164978   1040  0.47753797471523285\n",
      "Training loss: 0.559413880109787   1041  0.4712345004081726\n",
      "Training loss: 0.5529982617923191   1042  0.46571339666843414\n",
      "Training loss: 0.5558373161724636   1043  0.46635058522224426\n",
      "Training loss: 0.5528265748705182   1044  0.4521101713180542\n",
      "Training loss: 0.5486707602228437   1045  0.4663587212562561\n",
      "Training loss: 0.547363817691803   1046  0.46000605821609497\n",
      "Training loss: 0.5494426701750074   1047  0.47824084758758545\n",
      "Training loss: 0.5499420080866132   1048  0.4737922102212906\n",
      "Training loss: 0.5495844462088176   1049  0.4617452174425125\n",
      "Training loss: 0.5490398279258183   1050  0.4642368108034134\n",
      "Training loss: 0.547368232692991   1051  0.457424595952034\n",
      "Training loss: 0.5474114205156054   1052  0.4567353278398514\n",
      "Training loss: 0.5470204821654728   1053  0.45783376693725586\n",
      "Training loss: 0.5458889454603195   1054  0.45814505219459534\n",
      "Training loss: 0.5465904495545796   1055  0.4578379690647125\n",
      "Training loss: 0.5435378828219005   1056  0.45427948236465454\n",
      "Training loss: 0.5405570566654205   1057  0.4567050486803055\n",
      "Training loss: 0.5429412041391645   1058  0.46113163232803345\n",
      "Training loss: 0.5476876965590886   1059  0.45682986080646515\n",
      "Training loss: 0.5426512254135949   1060  0.45570723712444305\n",
      "Training loss: 0.5442096633570535   1061  0.46485579013824463\n",
      "Training loss: 0.5454558857849666   1062  0.46336260437965393\n",
      "Training loss: 0.5486240983009338   1063  0.46218259632587433\n",
      "Training loss: 0.5459011346101761   1064  0.4659692645072937\n",
      "Training loss: 0.5490335268633706   1065  0.4636335074901581\n",
      "Training loss: 0.5435625825609479   1066  0.46501176059246063\n",
      "Training loss: 0.5455945985657829   1067  0.4562664180994034\n",
      "Training loss: 0.5406330823898315   1068  0.46737685799598694\n",
      "Training loss: 0.5425806386130196   1069  0.45802105963230133\n",
      "Training loss: 0.5428101739713124   1070  0.4613841027021408\n",
      "Training loss: 0.5424533188343048   1071  0.4660883843898773\n",
      "Training loss: 0.5458826763289315   1072  0.4637601524591446\n",
      "Training loss: 0.5476698321955544   1073  0.45974425971508026\n",
      "Training loss: 0.5446697047778538   1074  0.45779937505722046\n",
      "Training loss: 0.5400063012327466   1075  0.4550836831331253\n",
      "Training loss: 0.5452210988317218   1076  0.4612233191728592\n",
      "Training loss: 0.5423887180430549   1077  0.47295480966567993\n",
      "Training loss: 0.5339248308113643   1078  0.4584297090768814\n",
      "Training loss: 0.5350127305303302   1079  0.4483835846185684\n",
      "Training loss: 0.5415068737098149   1080  0.4616662561893463\n",
      "Training loss: 0.5364495686122349   1081  0.4628836512565613\n",
      "Training loss: 0.540328826223101   1082  0.4718605577945709\n",
      "Training loss: 0.54899748308318   1083  0.45963284373283386\n",
      "Training loss: 0.5738544293812343   1084  0.49343203008174896\n",
      "Training loss: 0.5679142560277667   1085  0.4629320055246353\n",
      "Training loss: 0.55989522593362   1086  0.4722512662410736\n",
      "Training loss: 0.5495685083525521   1087  0.45683519542217255\n",
      "Training loss: 0.5435113481112889   1088  0.4600696563720703\n",
      "Training loss: 0.5387352151530129   1089  0.45519623160362244\n",
      "Training loss: 0.5382192432880402   1090  0.45632824301719666\n",
      "Training loss: 0.5378551312855312   1091  0.4513177275657654\n",
      "Training loss: 0.5346785656043461   1092  0.4419625848531723\n",
      "Training loss: 0.5342191202299935   1093  0.4558849483728409\n",
      "Training loss: 0.5316287790025983   1094  0.45182155072689056\n",
      "Training loss: 0.5296650699206761   1095  0.4508855640888214\n",
      "Training loss: 0.5305175185203552   1096  0.45145757496356964\n",
      "Training loss: 0.5317810986723218   1097  0.4560014456510544\n",
      "Training loss: 0.5365613102912903   1098  0.4487595409154892\n",
      "Training loss: 0.5398681653397424   1099  0.44650018215179443\n",
      "Training loss: 0.5383339183671134   1100  0.45257702469825745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5364735679967063   1101  0.4692421555519104\n",
      "Training loss: 0.5393264208521161   1102  0.4561818540096283\n",
      "Training loss: 0.5347529479435512   1103  0.45397840440273285\n",
      "Training loss: 0.5313228368759155   1104  0.4441836178302765\n",
      "Training loss: 0.5374107999461037   1105  0.45319071412086487\n",
      "Training loss: 0.5345901101827621   1106  0.45109136402606964\n",
      "Training loss: 0.5319638081959316   1107  0.44690561294555664\n",
      "Training loss: 0.5371699801513127   1108  0.4565250277519226\n",
      "Training loss: 0.5333423273903983   1109  0.4675247371196747\n",
      "Training loss: 0.535566800407001   1110  0.4569006711244583\n",
      "Training loss: 0.5325707984822137   1111  0.4528777450323105\n",
      "Training loss: 0.5312960211719785   1112  0.4491082727909088\n",
      "Training loss: 0.5306393461568015   1113  0.4563973993062973\n",
      "Training loss: 0.5310978037970406   1114  0.45377446711063385\n",
      "Training loss: 0.5336800004754748   1115  0.47326405346393585\n",
      "Training loss: 0.5290257973330361   1116  0.4421786069869995\n",
      "Training loss: 0.5333246609994343   1117  0.45770758390426636\n",
      "Training loss: 0.530434210385595   1118  0.448090061545372\n",
      "Training loss: 0.5260384636265891   1119  0.43243755400180817\n",
      "Training loss: 0.5292651951313019   1120  0.4383123815059662\n",
      "Training loss: 0.5282803241695676   1121  0.4549929350614548\n",
      "Training loss: 0.5318608837468284   1122  0.424788698554039\n",
      "Training loss: 0.5344140848943165   1123  0.46664726734161377\n",
      "Training loss: 0.5316120194537299   1124  0.45075051486492157\n",
      "Training loss: 0.5309066985334668   1125  0.4569939225912094\n",
      "Training loss: 0.5321447806698936   1126  0.46798694133758545\n",
      "Training loss: 0.536377272435597   1127  0.44622817635536194\n",
      "Training loss: 0.5332956697259631   1128  0.4357365369796753\n",
      "Training loss: 0.5313388747828347   1129  0.45838581025600433\n",
      "Training loss: 0.5306849479675293   1130  0.44902433454990387\n",
      "Training loss: 0.5271547479288918   1131  0.44795387983322144\n",
      "Training loss: 0.5317329253469195   1132  0.4626869261264801\n",
      "Training loss: 0.5308052620717457   1133  0.4355185329914093\n",
      "Training loss: 0.526365129011018   1134  0.4417964518070221\n",
      "Training loss: 0.5245260766574315   1135  0.466794952750206\n",
      "Training loss: 0.529670170375279   1136  0.45012450218200684\n",
      "Training loss: 0.5267186335154942   1137  0.4420272260904312\n",
      "Training loss: 0.5297287958008903   1138  0.46411341428756714\n",
      "Training loss: 0.5294107688324792   1139  0.448651060461998\n",
      "Training loss: 0.5245963931083679   1140  0.4493514597415924\n",
      "Training loss: 0.5255507571356637   1141  0.4455931931734085\n",
      "Training loss: 0.5268468175615583   1142  0.4517737329006195\n",
      "Training loss: 0.5256544905049461   1143  0.44823823869228363\n",
      "Training loss: 0.5217696726322174   1144  0.4451841711997986\n",
      "Training loss: 0.5244758895465306   1145  0.45579302310943604\n",
      "Training loss: 0.5265133380889893   1146  0.44382432103157043\n",
      "Training loss: 0.5253353459494454   1147  0.451259970664978\n",
      "Training loss: 0.5246514422552926   1148  0.44389283657073975\n",
      "Training loss: 0.5259396944727216   1149  0.45435217022895813\n",
      "Training loss: 0.5237790069409779   1150  0.44473256170749664\n",
      "Training loss: 0.5400494911840984   1151  0.47046321630477905\n",
      "Training loss: 0.5666844248771667   1152  0.46903182566165924\n",
      "Training loss: 0.5471789368561336   1153  0.45465537905693054\n",
      "Training loss: 0.5362502464226314   1154  0.4505181908607483\n",
      "Training loss: 0.5278669680867877   1155  0.44734621047973633\n",
      "Training loss: 0.525196516088077   1156  0.45298905670642853\n",
      "Training loss: 0.5271420436246055   1157  0.464539498090744\n",
      "Training loss: 0.5244718406881604   1158  0.4488791972398758\n",
      "Training loss: 0.5228907508509499   1159  0.44610779732465744\n",
      "Training loss: 0.5214959872620446   1160  0.45107918977737427\n",
      "Training loss: 0.5186586529016495   1161  0.4395229369401932\n",
      "Training loss: 0.5230511469500405   1162  0.4604182839393616\n",
      "Training loss: 0.5245396665164402   1163  0.45112501084804535\n",
      "Training loss: 0.5220943689346313   1164  0.44749946892261505\n",
      "Training loss: 0.5216947155339378   1165  0.45521824061870575\n",
      "Training loss: 0.5193577642951693   1166  0.4648648053407669\n",
      "Training loss: 0.519952512213162   1167  0.43917208909988403\n",
      "Training loss: 0.521192906158311   1168  0.4588582515716553\n",
      "Training loss: 0.5251226127147675   1169  0.4575034976005554\n",
      "Training loss: 0.5234960104737963   1170  0.4481612741947174\n",
      "Training loss: 0.5179554777485984   1171  0.4523133337497711\n",
      "Training loss: 0.5190713831356594   1172  0.43576107919216156\n",
      "Training loss: 0.5139657812459129   1173  0.4532236307859421\n",
      "Training loss: 0.5159992022173745   1174  0.4437463581562042\n",
      "Training loss: 0.5181151585919517   1175  0.4515276551246643\n",
      "Training loss: 0.5170968409095492   1176  0.45647118985652924\n",
      "Training loss: 0.5187284009797233   1177  0.4446535259485245\n",
      "Training loss: 0.5200354329177311   1178  0.4492364823818207\n",
      "Training loss: 0.5184840432235173   1179  0.45784659683704376\n",
      "Training loss: 0.5205377766064235   1180  0.44071967899799347\n",
      "Training loss: 0.5171688454491752   1181  0.44774308800697327\n",
      "Training loss: 0.5169910745961326   1182  0.4468580186367035\n",
      "Training loss: 0.5182669418198722   1183  0.44965821504592896\n",
      "Training loss: 0.5169955853904996   1184  0.45114995539188385\n",
      "Training loss: 0.5177790820598602   1185  0.45961780846118927\n",
      "Training loss: 0.5165037746940341   1186  0.4587352126836777\n",
      "Training loss: 0.5170568823814392   1187  0.4444065988063812\n",
      "Training loss: 0.5161747975008828   1188  0.4429123252630234\n",
      "Training loss: 0.5149351060390472   1189  0.46081309020519257\n",
      "Training loss: 0.5189651506287711   1190  0.44006477296352386\n",
      "Training loss: 0.5215419722454888   1191  0.44753775000572205\n",
      "Training loss: 0.5269824628319059   1192  0.4522651433944702\n",
      "Training loss: 0.5242331900766918   1193  0.44887563586235046\n",
      "Training loss: 0.522756542478289   1194  0.4466569572687149\n",
      "Training loss: 0.5263513880116599   1195  0.4435807019472122\n",
      "Training loss: 0.5159677075488227   1196  0.4474996030330658\n",
      "Training loss: 0.5199574892010007   1197  0.42861998081207275\n",
      "Training loss: 0.5180842003652028   1198  0.43989036977291107\n",
      "Training loss: 0.5141521138804299   1199  0.44378407299518585\n",
      "Training loss: 0.5165720858744213   1200  0.45240452885627747\n",
      "Training loss: 0.5124386101961136   1201  0.44855986535549164\n",
      "Training loss: 0.513065076300076   1202  0.4450097531080246\n",
      "Training loss: 0.5117617419787815   1203  0.4509921222925186\n",
      "Training loss: 0.516171646969659   1204  0.44896137714385986\n",
      "Training loss: 0.5138392341988427   1205  0.43768104910850525\n",
      "Training loss: 0.5127890982798168   1206  0.44669942557811737\n",
      "Training loss: 0.5119794734886715   1207  0.4406637102365494\n",
      "Training loss: 0.5114448964595795   1208  0.43817321956157684\n",
      "Training loss: 0.5159402468374797   1209  0.4512732923030853\n",
      "Training loss: 0.5140361445290702   1210  0.45304788649082184\n",
      "Training loss: 0.5196107200213841   1211  0.45401421189308167\n",
      "Training loss: 0.5138067092214312   1212  0.4380073845386505\n",
      "Training loss: 0.5135122835636139   1213  0.43993717432022095\n",
      "Training loss: 0.5135830364056996   1214  0.4499547779560089\n",
      "Training loss: 0.5083172747067043   1215  0.4445711374282837\n",
      "Training loss: 0.5111745532069888   1216  0.45493723452091217\n",
      "Training loss: 0.5095520700727191   1217  0.4396274983882904\n",
      "Training loss: 0.5102708573852267   1218  0.4483543038368225\n",
      "Training loss: 0.5117200102124896   1219  0.4364466816186905\n",
      "Training loss: 0.5121208471911294   1220  0.45097815990448\n",
      "Training loss: 0.5103985816240311   1221  0.43287795782089233\n",
      "Training loss: 0.509704549397741   1222  0.447491779923439\n",
      "Training loss: 0.5122085234948567   1223  0.45380084216594696\n",
      "Training loss: 0.5095861405134201   1224  0.4363517314195633\n",
      "Training loss: 0.5111746702875409   1225  0.45377184450626373\n",
      "Training loss: 0.5130299542631421   1226  0.44788603484630585\n",
      "Training loss: 0.5052461666720254   1227  0.4489402174949646\n",
      "Training loss: 0.5101612231561116   1228  0.4498633146286011\n",
      "Training loss: 0.5136117679732186   1229  0.4410843700170517\n",
      "Training loss: 0.5108655137675149   1230  0.44431157410144806\n",
      "Training loss: 0.5136255153587886   1231  0.4367757588624954\n",
      "Training loss: 0.5108672678470612   1232  0.45488396286964417\n",
      "Training loss: 0.5117992439440319   1233  0.4287126585841179\n",
      "Training loss: 0.5093747824430466   1234  0.43840840458869934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5071674393756049   1235  0.44595324993133545\n",
      "Training loss: 0.5080091612679618   1236  0.4392599016427994\n",
      "Training loss: 0.5061424842902592   1237  0.4476613700389862\n",
      "Training loss: 0.5083162401403699   1238  0.442431703209877\n",
      "Training loss: 0.505082585981914   1239  0.4558262228965759\n",
      "Training loss: 0.508097693324089   1240  0.44858093559741974\n",
      "Training loss: 0.5050846487283707   1241  0.4541259855031967\n",
      "Training loss: 0.5085123287779945   1242  0.43310244381427765\n",
      "Training loss: 0.5074691218989236   1243  0.4265556037425995\n",
      "Training loss: 0.5066816125597272   1244  0.44584769010543823\n",
      "Training loss: 0.5058867782354355   1245  0.41920357942581177\n",
      "Training loss: 0.5052247089999062   1246  0.43980397284030914\n",
      "Training loss: 0.509086719581059   1247  0.4449339509010315\n",
      "Training loss: 0.5049890599080494   1248  0.4395916908979416\n",
      "Training loss: 0.5015129404408591   1249  0.4420631378889084\n",
      "Training loss: 0.5049457017864499   1250  0.43365684151649475\n",
      "Training loss: 0.504605444414275   1251  0.4333001524209976\n",
      "Training loss: 0.5064416485173362   1252  0.43532727658748627\n",
      "Training loss: 0.5089041526828494   1253  0.4438585042953491\n",
      "Training loss: 0.508427945630891   1254  0.44043803215026855\n",
      "Training loss: 0.5056175163814   1255  0.446006178855896\n",
      "Training loss: 0.5084181725978851   1256  0.44668012857437134\n",
      "Training loss: 0.5052648889166969   1257  0.45013371109962463\n",
      "Training loss: 0.5075334651129586   1258  0.4534847140312195\n",
      "Training loss: 0.5096793706927981   1259  0.4459039270877838\n",
      "Training loss: 0.5065476787941796   1260  0.4374134689569473\n",
      "Training loss: 0.5073745527437755   1261  0.4443359225988388\n",
      "Training loss: 0.5062333451850074   1262  0.44704411923885345\n",
      "Training loss: 0.5046851634979248   1263  0.42609329521656036\n",
      "Training loss: 0.5083864118371691   1264  0.4419831186532974\n",
      "Training loss: 0.5102083065680095   1265  0.4422721862792969\n",
      "Training loss: 0.5064260831901005   1266  0.431414932012558\n",
      "Training loss: 0.5036960542201996   1267  0.44264788925647736\n",
      "Training loss: 0.5034680707114083   1268  0.4441075474023819\n",
      "Training loss: 0.501506724527904   1269  0.44428230822086334\n",
      "Training loss: 0.5067061313561031   1270  0.44324999302625656\n",
      "Training loss: 0.5015855261257717   1271  0.43648774921894073\n",
      "Training loss: 0.5035865093980517   1272  0.4372805953025818\n",
      "Training loss: 0.5017076134681702   1273  0.43805550038814545\n",
      "Training loss: 0.5040326352630343   1274  0.4413665235042572\n",
      "Training loss: 0.5010970618043628   1275  0.4471372961997986\n",
      "Training loss: 0.5050695389509201   1276  0.43752332031726837\n",
      "Training loss: 0.5032980101449149   1277  0.4366517961025238\n",
      "Training loss: 0.4993804395198822   1278  0.44124366343021393\n",
      "Training loss: 0.5030437346015658   1279  0.43984776735305786\n",
      "Training loss: 0.500892984015601   1280  0.42600905895233154\n",
      "Training loss: 0.5009820908308029   1281  0.4476199150085449\n",
      "Training loss: 0.5083207658358982   1282  0.4574618488550186\n",
      "Training loss: 0.5315911982740674   1283  0.45843228697776794\n",
      "Training loss: 0.5302239486149379   1284  0.4604235291481018\n",
      "Training loss: 0.5213559695652553   1285  0.44380927085876465\n",
      "Training loss: 0.5147537099463599   1286  0.4592622220516205\n",
      "Training loss: 0.5277659722736904   1287  0.4397241324186325\n",
      "Training loss: 0.5153231131178992   1288  0.4526319205760956\n",
      "Training loss: 0.5124898233583995   1289  0.4465426057577133\n",
      "Training loss: 0.5120136567524501   1290  0.44283953309059143\n",
      "Training loss: 0.5141308350222451   1291  0.4600983113050461\n",
      "Training loss: 0.5094238221645355   1292  0.44489723443984985\n",
      "Training loss: 0.5009746828249523   1293  0.43534213304519653\n",
      "Training loss: 0.5092581595693316   1294  0.445202499628067\n",
      "Training loss: 0.5031466015747615   1295  0.44159793853759766\n",
      "Training loss: 0.5038605800696782   1296  0.4416939467191696\n",
      "Training loss: 0.502599509699004   1297  0.435160756111145\n",
      "Training loss: 0.5023845327751977   1298  0.43129298090934753\n",
      "Training loss: 0.5016522769417081   1299  0.42869265377521515\n",
      "Training loss: 0.4944213905504772   1300  0.4497693330049515\n",
      "Training loss: 0.4961135557719639   1301  0.4360983371734619\n",
      "Training loss: 0.5051871900047574   1302  0.45030859112739563\n",
      "Training loss: 0.5008012524672917   1303  0.4363492876291275\n",
      "Training loss: 0.4986657734428133   1304  0.4310351610183716\n",
      "Training loss: 0.49670422290052685   1305  0.4376746565103531\n",
      "Training loss: 0.4976593852043152   1306  0.43628518283367157\n",
      "Training loss: 0.4971399818147932   1307  0.4364803582429886\n",
      "Training loss: 0.49662426326956066   1308  0.4295858070254326\n",
      "Training loss: 0.4965273993355887   1309  0.4475101828575134\n",
      "Training loss: 0.4961546063423157   1310  0.4458203762769699\n",
      "Training loss: 0.499083080462047   1311  0.43265318870544434\n",
      "Training loss: 0.49473169658865246   1312  0.4448999762535095\n",
      "Training loss: 0.49593388608523775   1313  0.43431609869003296\n",
      "Training loss: 0.497217344386237   1314  0.4233191981911659\n",
      "Training loss: 0.4965180243764605   1315  0.43405720591545105\n",
      "Training loss: 0.49610348471573423   1316  0.4362045079469681\n",
      "Training loss: 0.49857393332890104   1317  0.4369804263114929\n",
      "Training loss: 0.49168606102466583   1318  0.4280547499656677\n",
      "Training loss: 0.49818397419793264   1319  0.43364621698856354\n",
      "Training loss: 0.4995684304407665   1320  0.45472192764282227\n",
      "Training loss: 0.4997418054512569   1321  0.43000833690166473\n",
      "Training loss: 0.5006518321377891   1322  0.42967747151851654\n",
      "Training loss: 0.49782833244119373   1323  0.4341527447104454\n",
      "Training loss: 0.49875746241637636   1324  0.4470324367284775\n",
      "Training loss: 0.49568301865032743   1325  0.44110268354415894\n",
      "Training loss: 0.4927024671009609   1326  0.43717603385448456\n",
      "Training loss: 0.4941276141575405   1327  0.43590760231018066\n",
      "Training loss: 0.49065263143607546   1328  0.43878813087940216\n",
      "Training loss: 0.49169754556247164   1329  0.4401753544807434\n",
      "Training loss: 0.4889193241085325   1330  0.4366246461868286\n",
      "Training loss: 0.48975958142961773   1331  0.44580745697021484\n",
      "Training loss: 0.4982657900878361   1332  0.4328492805361748\n",
      "Training loss: 0.49469616796289173   1333  0.43431761860847473\n",
      "Training loss: 0.49612155130931307   1334  0.43859991431236267\n",
      "Training loss: 0.49501589579241617   1335  0.43244409561157227\n",
      "Training loss: 0.49057107738086153   1336  0.43819381296634674\n",
      "Training loss: 0.4892179859536035   1337  0.44069230556488037\n",
      "Training loss: 0.49115263138498577   1338  0.43741175532341003\n",
      "Training loss: 0.4932632510151182   1339  0.44127124547958374\n",
      "Training loss: 0.49637686567647116   1340  0.44985322654247284\n",
      "Training loss: 0.4997198773281915   1341  0.4537077695131302\n",
      "Training loss: 0.49745024953569683   1342  0.43184101581573486\n",
      "Training loss: 0.49226461350917816   1343  0.43864019215106964\n",
      "Training loss: 0.4907851091453007   1344  0.44034402072429657\n",
      "Training loss: 0.4912882021495274   1345  0.42889294028282166\n",
      "Training loss: 0.4927344492503575   1346  0.44015534967184067\n",
      "Training loss: 0.493040959749903   1347  0.43632882833480835\n",
      "Training loss: 0.49029635744435446   1348  0.4396015852689743\n",
      "Training loss: 0.4950349820511682   1349  0.42503808438777924\n",
      "Training loss: 0.49109775040830883   1350  0.44564811885356903\n",
      "Training loss: 0.4915943273476192   1351  0.44590313732624054\n",
      "Training loss: 0.4895748921803066   1352  0.44216200709342957\n",
      "Training loss: 0.4870127333062036   1353  0.4256300926208496\n",
      "Training loss: 0.49809018416064127   1354  0.442341685295105\n",
      "Training loss: 0.4990748528923307   1355  0.4467353820800781\n",
      "Training loss: 0.49451763715062824   1356  0.44224366545677185\n",
      "Training loss: 0.4915524444409779   1357  0.4381011426448822\n",
      "Training loss: 0.4899835011788777   1358  0.4440455734729767\n",
      "Training loss: 0.4919039330312184   1359  0.4290691763162613\n",
      "Training loss: 0.489785373210907   1360  0.43575238436460495\n",
      "Training loss: 0.4860229343175888   1361  0.43755652010440826\n",
      "Training loss: 0.4878528139420918   1362  0.44114965200424194\n",
      "Training loss: 0.4924867366041456   1363  0.4509752094745636\n",
      "Training loss: 0.4950663170644215   1364  0.4320646971464157\n",
      "Training loss: 0.4929421714373997   1365  0.4295620173215866\n",
      "Training loss: 0.48788632239614216   1366  0.43146680295467377\n",
      "Training loss: 0.48638014069625307   1367  0.42959778010845184\n",
      "Training loss: 0.4892927961690085   1368  0.4324188828468323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4950626088040216   1369  0.4367448091506958\n",
      "Training loss: 0.49173347226211   1370  0.44321632385253906\n",
      "Training loss: 0.48752109493528095   1371  0.44348129630088806\n",
      "Training loss: 0.49046324619225096   1372  0.42031829059123993\n",
      "Training loss: 0.4879577266318457   1373  0.4386895000934601\n",
      "Training loss: 0.4863051963703973   1374  0.4436190575361252\n",
      "Training loss: 0.4869066093649183   1375  0.4386262148618698\n",
      "Training loss: 0.48785457227911266   1376  0.4358702749013901\n",
      "Training loss: 0.49062260559626986   1377  0.4368131011724472\n",
      "Training loss: 0.48628982050078257   1378  0.4262281209230423\n",
      "Training loss: 0.488622037427766   1379  0.4308330863714218\n",
      "Training loss: 0.4866294392517635   1380  0.4319232255220413\n",
      "Training loss: 0.4883585295506886   1381  0.4251580536365509\n",
      "Training loss: 0.4897377980606897   1382  0.44186678528785706\n",
      "Training loss: 0.4895821639469692   1383  0.43440042436122894\n",
      "Training loss: 0.4888113694531577   1384  0.43576765060424805\n",
      "Training loss: 0.483831341777529   1385  0.4231428951025009\n",
      "Training loss: 0.4898787353719984   1386  0.42510513961315155\n",
      "Training loss: 0.48281651522432056   1387  0.428022563457489\n",
      "Training loss: 0.4833231270313263   1388  0.43273983895778656\n",
      "Training loss: 0.48653723299503326   1389  0.43348585069179535\n",
      "Training loss: 0.4842654764652252   1390  0.4297456443309784\n",
      "Training loss: 0.48270543984004427   1391  0.41991402208805084\n",
      "Training loss: 0.48468995094299316   1392  0.42438162863254547\n",
      "Training loss: 0.48478417311395916   1393  0.4352298825979233\n",
      "Training loss: 0.48608250064509256   1394  0.42679332941770554\n",
      "Training loss: 0.48283594633851734   1395  0.43595409393310547\n",
      "Training loss: 0.4835165504898344   1396  0.44139887392520905\n",
      "Training loss: 0.48264061553137644   1397  0.4384938031435013\n",
      "Training loss: 0.47984655840056284   1398  0.4300558716058731\n",
      "Training loss: 0.4843263179063797   1399  0.433661624789238\n",
      "Training loss: 0.4835922249725887   1400  0.42134805023670197\n",
      "Training loss: 0.4837153958422797   1401  0.4302322119474411\n",
      "Training loss: 0.4843724412577493   1402  0.43144193291664124\n",
      "Training loss: 0.4837703491960253   1403  0.42893803119659424\n",
      "Training loss: 0.48861425689288546   1404  0.4316985160112381\n",
      "Training loss: 0.4949184762580054   1405  0.4241750240325928\n",
      "Training loss: 0.4962966867855617   1406  0.4376223385334015\n",
      "Training loss: 0.499395136322294   1407  0.4560387432575226\n",
      "Training loss: 0.5103325013603482   1408  0.43844039738178253\n",
      "Training loss: 0.5037926818643298   1409  0.43775177001953125\n",
      "Training loss: 0.49498380507741657   1410  0.44435013830661774\n",
      "Training loss: 0.48786633142403196   1411  0.42868034541606903\n",
      "Training loss: 0.48588936243738445   1412  0.42941904067993164\n",
      "Training loss: 0.48876901609557016   1413  0.42904186248779297\n",
      "Training loss: 0.48746841507298605   1414  0.44640353322029114\n",
      "Training loss: 0.4828795960971287   1415  0.4255483001470566\n",
      "Training loss: 0.482723314847265   1416  0.43932434916496277\n",
      "Training loss: 0.48079611148153034   1417  0.4317922592163086\n",
      "Training loss: 0.48435387866837637   1418  0.4344129115343094\n",
      "Training loss: 0.48510134858744486   1419  0.4264051616191864\n",
      "Training loss: 0.4827985635825566   1420  0.43822823464870453\n",
      "Training loss: 0.48510691097804476   1421  0.41737009584903717\n",
      "Training loss: 0.4807670904057367   1422  0.42455244809389114\n",
      "Training loss: 0.482875611100878   1423  0.4391327053308487\n",
      "Training loss: 0.4819999805518559   1424  0.42715825140476227\n",
      "Training loss: 0.48137055550302776   1425  0.4293753057718277\n",
      "Training loss: 0.48283062875270844   1426  0.43359924852848053\n",
      "Training loss: 0.4812784194946289   1427  0.43046070635318756\n",
      "Training loss: 0.48154327486242565   1428  0.4264148324728012\n",
      "Training loss: 0.47991806481565746   1429  0.4303819537162781\n",
      "Training loss: 0.48262766003608704   1430  0.4171982407569885\n",
      "Training loss: 0.4804623488868986   1431  0.43433305621147156\n",
      "Training loss: 0.4803351696048464   1432  0.4301821142435074\n",
      "Training loss: 0.47849690701280323   1433  0.42966528236866\n",
      "Training loss: 0.48066486205373493   1434  0.437364399433136\n",
      "Training loss: 0.4812825045415333   1435  0.4301783889532089\n",
      "Training loss: 0.47813819561685833   1436  0.4376460909843445\n",
      "Training loss: 0.482626206108502   1437  0.41738075017929077\n",
      "Training loss: 0.476854036961283   1438  0.4448254853487015\n",
      "Training loss: 0.4778312529836382   1439  0.4465746134519577\n",
      "Training loss: 0.47991760075092316   1440  0.429378867149353\n",
      "Training loss: 0.48266579636505674   1441  0.4469408541917801\n",
      "Training loss: 0.4835619798728398   1442  0.4394804388284683\n",
      "Training loss: 0.4793233019965036   1443  0.43438470363616943\n",
      "Training loss: 0.48031028679439   1444  0.42910246551036835\n",
      "Training loss: 0.47626177966594696   1445  0.43513184785842896\n",
      "Training loss: 0.47891115290778025   1446  0.4306400716304779\n",
      "Training loss: 0.47624031560761587   1447  0.438988521695137\n",
      "Training loss: 0.4776682662112372   1448  0.41988734900951385\n",
      "Training loss: 0.476528525352478   1449  0.43721647560596466\n",
      "Training loss: 0.4780626467296055   1450  0.43222056329250336\n",
      "Training loss: 0.47548640625817434   1451  0.42810387909412384\n",
      "Training loss: 0.4790795317717961   1452  0.43478450179100037\n",
      "Training loss: 0.4770334469420569   1453  0.4355984479188919\n",
      "Training loss: 0.47577010095119476   1454  0.4397021383047104\n",
      "Training loss: 0.4763867940221514   1455  0.4327535182237625\n",
      "Training loss: 0.47707133846623556   1456  0.43750639259815216\n",
      "Training loss: 0.47577717687402454   1457  0.4300539642572403\n",
      "Training loss: 0.4769690441233771   1458  0.4335661232471466\n",
      "Training loss: 0.4784814247063228   1459  0.42248770594596863\n",
      "Training loss: 0.472957900592259   1460  0.4321892410516739\n",
      "Training loss: 0.47484864720276426   1461  0.4361436069011688\n",
      "Training loss: 0.4757689727204187   1462  0.43271590769290924\n",
      "Training loss: 0.475270471402577   1463  0.42814160138368607\n",
      "Training loss: 0.47379358751433237   1464  0.42671504616737366\n",
      "Training loss: 0.4770989694765636   1465  0.42560461163520813\n",
      "Training loss: 0.48065412257398876   1466  0.4266267418861389\n",
      "Training loss: 0.4843293045248304   1467  0.4407598078250885\n",
      "Training loss: 0.4834904117243631   1468  0.43880584836006165\n",
      "Training loss: 0.4774570273501532   1469  0.4200032502412796\n",
      "Training loss: 0.47814268512385233   1470  0.4257429242134094\n",
      "Training loss: 0.47623438920293537   1471  0.43889322131872177\n",
      "Training loss: 0.477468694959368   1472  0.4265168458223343\n",
      "Training loss: 0.4744098207780293   1473  0.42788930237293243\n",
      "Training loss: 0.4741106650659016   1474  0.4268440753221512\n",
      "Training loss: 0.4738085461514337   1475  0.42057861387729645\n",
      "Training loss: 0.474354978118624   1476  0.4263458251953125\n",
      "Training loss: 0.47523865529469084   1477  0.42628949880599976\n",
      "Training loss: 0.4775449356862477   1478  0.43673689663410187\n",
      "Training loss: 0.47343979137284414   1479  0.43002407252788544\n",
      "Training loss: 0.47328099395547596   1480  0.438113272190094\n",
      "Training loss: 0.47422900795936584   1481  0.42448320984840393\n",
      "Training loss: 0.4719299461160387   1482  0.4364525079727173\n",
      "Training loss: 0.4731432689087732   1483  0.4324265122413635\n",
      "Training loss: 0.4748584542955671   1484  0.4204818606376648\n",
      "Training loss: 0.473689398595265   1485  0.40666109323501587\n",
      "Training loss: 0.4728831137929644   1486  0.43040377646684647\n",
      "Training loss: 0.47417600240026203   1487  0.4285825788974762\n",
      "Training loss: 0.4730408958026341   1488  0.43999195098876953\n",
      "Training loss: 0.4734195726258414   1489  0.43888305127620697\n",
      "Training loss: 0.47707986405917574   1490  0.42804886400699615\n",
      "Training loss: 0.473406793815749   1491  0.43347932398319244\n",
      "Training loss: 0.47311974636146   1492  0.4320238530635834\n",
      "Training loss: 0.4712230201278414   1493  0.43040984869003296\n",
      "Training loss: 0.4729427972010204   1494  0.42543770372867584\n",
      "Training loss: 0.47385800736291067   1495  0.4316715747117996\n",
      "Training loss: 0.4749071236167635   1496  0.41750967502593994\n",
      "Training loss: 0.47506491201264517   1497  0.43282417953014374\n",
      "Training loss: 0.4751629467521395   1498  0.4340365529060364\n",
      "Training loss: 0.4753905428307397   1499  0.4244830757379532\n",
      "Training loss: 0.47111955497946056   1500  0.4270079433917999\n",
      "Training loss: 0.4690039519752775   1501  0.4303409904241562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.47045973581927164   1502  0.43869350850582123\n",
      "Training loss: 0.47347962643418995   1503  0.4330657720565796\n",
      "Training loss: 0.4721415362187794   1504  0.4428427517414093\n",
      "Training loss: 0.47319528673376354   1505  0.43893542885780334\n",
      "Training loss: 0.47268074325152803   1506  0.4381299614906311\n",
      "Training loss: 0.4730234124830791   1507  0.4370219111442566\n",
      "Training loss: 0.4747551360300609   1508  0.42809122800827026\n",
      "Training loss: 0.477971436721938   1509  0.43251171708106995\n",
      "Training loss: 0.47800698237759726   1510  0.4249340742826462\n",
      "Training loss: 0.4763660281896591   1511  0.43115513026714325\n",
      "Training loss: 0.47476031524794443   1512  0.42455002665519714\n",
      "Training loss: 0.4736347219773701   1513  0.43740007281303406\n",
      "Training loss: 0.4743309574467795   1514  0.4280000776052475\n",
      "Training loss: 0.46804557953562054   1515  0.422966867685318\n",
      "Training loss: 0.47073034729276386   1516  0.42808519303798676\n",
      "Training loss: 0.4711825932775225   1517  0.42839576303958893\n",
      "Training loss: 0.47004277578422   1518  0.43789973855018616\n",
      "Training loss: 0.4726787486246654   1519  0.43457046151161194\n",
      "Training loss: 0.4743440555674689   1520  0.4345456063747406\n",
      "Training loss: 0.4686879387923649   1521  0.4339311867952347\n",
      "Training loss: 0.47250655719212126   1522  0.4297362267971039\n",
      "Training loss: 0.4704677952187402   1523  0.42243045568466187\n",
      "Training loss: 0.4737943261861801   1524  0.4405064731836319\n",
      "Training loss: 0.4713926634618214   1525  0.43120673298835754\n",
      "Training loss: 0.47072737983294893   1526  0.4307401031255722\n",
      "Training loss: 0.46953293468270985   1527  0.43211929500102997\n",
      "Training loss: 0.4693689388888223   1528  0.4232141226530075\n",
      "Training loss: 0.4705795965024403   1529  0.42868541181087494\n",
      "Training loss: 0.470106195126261   1530  0.43814945220947266\n",
      "Training loss: 0.4715117173535483   1531  0.4300415515899658\n",
      "Training loss: 0.46993587272507803   1532  0.4231928139925003\n",
      "Training loss: 0.47195609552519663   1533  0.4297432005405426\n",
      "Training loss: 0.4731819523232324   1534  0.4313944876194\n",
      "Training loss: 0.4688058240073068   1535  0.4232194572687149\n",
      "Training loss: 0.474310074533735   1536  0.4345618039369583\n",
      "Training loss: 0.4703282415866852   1537  0.43281060457229614\n",
      "Training loss: 0.4703035226890019   1538  0.42900313436985016\n",
      "Training loss: 0.46836056240967344   1539  0.41519594192504883\n",
      "Training loss: 0.46966089734009336   1540  0.43434929847717285\n",
      "Training loss: 0.46392001637390684   1541  0.43755774199962616\n",
      "Training loss: 0.46484597878796713   1542  0.43474744260311127\n",
      "Training loss: 0.4663297746862684   1543  0.42797473073005676\n",
      "Training loss: 0.47093256030763897   1544  0.41294461488723755\n",
      "Training loss: 0.46834454579012735   1545  0.4239141047000885\n",
      "Training loss: 0.46637878460543497   1546  0.4322562515735626\n",
      "Training loss: 0.46823730213301523   1547  0.42532822489738464\n",
      "Training loss: 0.4665980849947248   1548  0.4361514300107956\n",
      "Training loss: 0.4650802420718329   1549  0.42337051033973694\n",
      "Training loss: 0.46322170751435415   1550  0.4203682541847229\n",
      "Training loss: 0.4696073404380253   1551  0.42608632147312164\n",
      "Training loss: 0.47124654267515453   1552  0.42640502750873566\n",
      "Training loss: 0.4712589191538947   1553  0.43366754055023193\n",
      "Training loss: 0.47014313084738596   1554  0.4351028800010681\n",
      "Training loss: 0.4667371426309858   1555  0.4292632043361664\n",
      "Training loss: 0.46976653167179655   1556  0.43175792694091797\n",
      "Training loss: 0.46751100037779125   1557  0.42676571011543274\n",
      "Training loss: 0.4673097048486982   1558  0.43906208872795105\n",
      "Training loss: 0.4668543317488262   1559  0.42331360280513763\n",
      "Training loss: 0.4701149825538908   1560  0.4255567342042923\n",
      "Training loss: 0.47029372836862293   1561  0.4189644753932953\n",
      "Training loss: 0.4668696629149573   1562  0.42660097032785416\n",
      "Training loss: 0.46447865664958954   1563  0.409831240773201\n",
      "Training loss: 0.4683369909014021   1564  0.43284745514392853\n",
      "Training loss: 0.4675427462373461   1565  0.42094871401786804\n",
      "Training loss: 0.46514064712183817   1566  0.41308994591236115\n",
      "Training loss: 0.4955992954117911   1567  0.44701430201530457\n",
      "Training loss: 0.48304675093718935   1568  0.4263962209224701\n",
      "Training loss: 0.4749648996761867   1569  0.4179258942604065\n",
      "Training loss: 0.4714426738875253   1570  0.4381980299949646\n",
      "Training loss: 0.46722777826445444   1571  0.43546779453754425\n",
      "Training loss: 0.46570074558258057   1572  0.4361904710531235\n",
      "Training loss: 0.45981661336762564   1573  0.42123426496982574\n",
      "Training loss: 0.46237816342285704   1574  0.42306673526763916\n",
      "Training loss: 0.4643330510173525   1575  0.420218750834465\n",
      "Training loss: 0.46177455357142855   1576  0.41742701828479767\n",
      "Training loss: 0.46341489042554584   1577  0.4181375056505203\n",
      "Training loss: 0.4651034431798117   1578  0.4334418773651123\n",
      "Training loss: 0.4657836662871497   1579  0.4290098696947098\n",
      "Training loss: 0.46369321644306183   1580  0.42578941583633423\n",
      "Training loss: 0.4627566124711718   1581  0.4357585459947586\n",
      "Training loss: 0.4646350273064205   1582  0.4257750064134598\n",
      "Training loss: 0.462923132947513   1583  0.4175749123096466\n",
      "Training loss: 0.45986035253320423   1584  0.42318132519721985\n",
      "Training loss: 0.4621181743485587   1585  0.4322415441274643\n",
      "Training loss: 0.46094160207680296   1586  0.42319805920124054\n",
      "Training loss: 0.4608674134526934   1587  0.4317525029182434\n",
      "Training loss: 0.4658395286117281   1588  0.42978155612945557\n",
      "Training loss: 0.462407471878188   1589  0.4253837913274765\n",
      "Training loss: 0.46805238510881153   1590  0.4324988126754761\n",
      "Training loss: 0.46419637330940794   1591  0.4223053678870201\n",
      "Training loss: 0.4657090540443148   1592  0.43190309405326843\n",
      "Training loss: 0.46829796901771   1593  0.4332810640335083\n",
      "Training loss: 0.46771166580063955   1594  0.4305412918329239\n",
      "Training loss: 0.4668700248003006   1595  0.4317454546689987\n",
      "Training loss: 0.4680097656590598   1596  0.4307774305343628\n",
      "Training loss: 0.4650777706078121   1597  0.4212557151913643\n",
      "Training loss: 0.46546014717647005   1598  0.4258206784725189\n",
      "Training loss: 0.46759816791330067   1599  0.42799076437950134\n",
      "Training loss: 0.46827259021145956   1600  0.4277859628200531\n",
      "Training loss: 0.464076640350478   1601  0.4276910722255707\n",
      "Training loss: 0.46219080473695484   1602  0.4253808706998825\n",
      "Training loss: 0.46075624440397533   1603  0.4159892052412033\n",
      "Training loss: 0.46517420027937206   1604  0.42787210643291473\n",
      "Training loss: 0.46301562445504324   1605  0.4201391041278839\n",
      "Training loss: 0.45491986189569744   1606  0.4340532273054123\n",
      "Training loss: 0.46088030508586336   1607  0.42791325598955154\n",
      "Training loss: 0.46360673010349274   1608  0.4366982579231262\n",
      "Training loss: 0.4617710773433958   1609  0.4206780195236206\n",
      "Training loss: 0.4600571266242436   1610  0.42134836316108704\n",
      "Training loss: 0.4594473455633436   1611  0.43292754888534546\n",
      "Training loss: 0.46359390233244213   1612  0.4282432943582535\n",
      "Training loss: 0.46045963040419985   1613  0.4182143956422806\n",
      "Training loss: 0.4581967123917171   1614  0.4188089370727539\n",
      "Training loss: 0.4633070464645113   1615  0.4285569190979004\n",
      "Training loss: 0.45919635040419443   1616  0.41699160635471344\n",
      "Training loss: 0.45907459940229145   1617  0.41574330627918243\n",
      "Training loss: 0.4607195087841579   1618  0.41951435804367065\n",
      "Training loss: 0.462173906820161   1619  0.42099231481552124\n",
      "Training loss: 0.4582159434046064   1620  0.4236282855272293\n",
      "Training loss: 0.4574273313794817   1621  0.42347879707813263\n",
      "Training loss: 0.4596733067716871   1622  0.42855772376060486\n",
      "Training loss: 0.4602307677268982   1623  0.41796933114528656\n",
      "Training loss: 0.45859490760735105   1624  0.42836400866508484\n",
      "Training loss: 0.4597529811518533   1625  0.433273583650589\n",
      "Training loss: 0.4620444412742342   1626  0.4239142835140228\n",
      "Training loss: 0.4572344550064632   1627  0.4212391674518585\n",
      "Training loss: 0.4593163899012974   1628  0.4124165177345276\n",
      "Training loss: 0.46105463802814484   1629  0.4246457815170288\n",
      "Training loss: 0.45605689712933134   1630  0.42448848485946655\n",
      "Training loss: 0.45915602147579193   1631  0.4232019931077957\n",
      "Training loss: 0.4618064250264849   1632  0.4249195158481598\n",
      "Training loss: 0.45728158950805664   1633  0.4281967431306839\n",
      "Training loss: 0.45620144265038626   1634  0.4209861755371094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.45984145998954773   1635  0.43100446462631226\n",
      "Training loss: 0.46109019219875336   1636  0.4244239553809166\n",
      "Training loss: 0.45803259313106537   1637  0.43877631425857544\n",
      "Training loss: 0.4615952180964606   1638  0.4318993389606476\n",
      "Training loss: 0.4528081459658487   1639  0.42368318140506744\n",
      "Training loss: 0.45892870851925444   1640  0.4128943085670471\n",
      "Training loss: 0.45692204790455954   1641  0.426034078001976\n",
      "Training loss: 0.4626591759068625   1642  0.4298192858695984\n",
      "Training loss: 0.46124155819416046   1643  0.42550331354141235\n",
      "Training loss: 0.46236033950533184   1644  0.4342655688524246\n",
      "Training loss: 0.4615675296102251   1645  0.4280371516942978\n",
      "Training loss: 0.4639065074069159   1646  0.4354322552680969\n",
      "Training loss: 0.457621374300548   1647  0.4287768080830574\n",
      "Training loss: 0.46080351514475687   1648  0.43403784930706024\n",
      "Training loss: 0.4619360587426594   1649  0.4356554299592972\n",
      "Training loss: 0.4625746394906725   1650  0.429902046918869\n",
      "Training loss: 0.46081965948854176   1651  0.43055112659931183\n",
      "Training loss: 0.460726586835725   1652  0.4208306223154068\n",
      "Training loss: 0.45773689235959736   1653  0.42984244227409363\n",
      "Training loss: 0.46196572695459637   1654  0.44143059849739075\n",
      "Training loss: 0.455175587109157   1655  0.4357898086309433\n",
      "Training loss: 0.4590794337647302   1656  0.425031378865242\n",
      "Training loss: 0.45888696185180117   1657  0.42434684932231903\n",
      "Training loss: 0.45661236132894245   1658  0.4285276234149933\n",
      "Training loss: 0.45743095661912647   1659  0.4335269182920456\n",
      "Training loss: 0.457843365413802   1660  0.43747447431087494\n",
      "Training loss: 0.4571869117873056   1661  0.4324474036693573\n",
      "Training loss: 0.4568300949675696   1662  0.4269278943538666\n",
      "Training loss: 0.4572681358882359   1663  0.43296390771865845\n",
      "Training loss: 0.4576374739408493   1664  0.4249654710292816\n",
      "Training loss: 0.4562616582427706   1665  0.4230690151453018\n",
      "Training loss: 0.45597364008426666   1666  0.4394320994615555\n",
      "Training loss: 0.456016338297299   1667  0.41120298206806183\n",
      "Training loss: 0.4561841445309775   1668  0.42000003159046173\n",
      "Training loss: 0.45795824272292   1669  0.43160317838191986\n",
      "Training loss: 0.45905408688953947   1670  0.4210154861211777\n",
      "Training loss: 0.4563358724117279   1671  0.4305306375026703\n",
      "Training loss: 0.45424294258866993   1672  0.4212726056575775\n",
      "Training loss: 0.45549776937280384   1673  0.41768383979797363\n",
      "Training loss: 0.4575912633112499   1674  0.4132557436823845\n",
      "Training loss: 0.45420038274356295   1675  0.42894700169563293\n",
      "Training loss: 0.45145547815731596   1676  0.42372505366802216\n",
      "Training loss: 0.455244066459792   1677  0.42194077372550964\n",
      "Training loss: 0.45512858246053967   1678  0.42516254633665085\n",
      "Training loss: 0.45450527540275026   1679  0.42876726388931274\n",
      "Training loss: 0.45946353886808666   1680  0.43004104495048523\n",
      "Training loss: 0.4553072282246181   1681  0.4230293780565262\n",
      "Training loss: 0.45736446550914217   1682  0.41417933255434036\n",
      "Training loss: 0.4570031336375645   1683  0.41883744299411774\n",
      "Training loss: 0.4553057551383972   1684  0.4373663514852524\n",
      "Training loss: 0.45647515782288145   1685  0.41243238747119904\n",
      "Training loss: 0.454105692250388   1686  0.4290802553296089\n",
      "Training loss: 0.45377807532038006   1687  0.4206819459795952\n",
      "Training loss: 0.45483226009777616   1688  0.4283599853515625\n",
      "Training loss: 0.4573201047522681   1689  0.424581415951252\n",
      "Training loss: 0.45466538625104086   1690  0.4207548201084137\n",
      "Training loss: 0.4551438348633902   1691  0.42911097407341003\n",
      "Training loss: 0.45176113503319876   1692  0.42588356137275696\n",
      "Training loss: 0.4512660311801093   1693  0.4294227361679077\n",
      "Training loss: 0.45463534763881136   1694  0.43348656594753265\n",
      "Training loss: 0.46122549048491884   1695  0.42954084277153015\n",
      "Training loss: 0.4579425943749292   1696  0.42753270268440247\n",
      "Training loss: 0.4522717467376164   1697  0.4229298233985901\n",
      "Training loss: 0.4530837429421289   1698  0.43854740262031555\n",
      "Training loss: 0.4568472185305187   1699  0.43222469091415405\n",
      "Training loss: 0.45877225058419363   1700  0.4240041971206665\n",
      "Training loss: 0.4556503636496408   1701  0.42511552572250366\n",
      "Training loss: 0.45273364228861673   1702  0.4183416962623596\n",
      "Training loss: 0.45475244309220997   1703  0.4318052679300308\n",
      "Training loss: 0.45623746301446644   1704  0.4262172430753708\n",
      "Training loss: 0.4590373912027904   1705  0.42827263474464417\n",
      "Training loss: 0.45483091686453136   1706  0.417830690741539\n",
      "Training loss: 0.4541785312550409   1707  0.4339580237865448\n",
      "Training loss: 0.45303982283387867   1708  0.4102809578180313\n",
      "Training loss: 0.45619423048836844   1709  0.42264431715011597\n",
      "Training loss: 0.4534549670560019   1710  0.4235352575778961\n",
      "Training loss: 0.4525382454906191   1711  0.4295586049556732\n",
      "Training loss: 0.45545609082494465   1712  0.42782342433929443\n",
      "Training loss: 0.45729940065315794   1713  0.42615897953510284\n",
      "Training loss: 0.45159054015363964   1714  0.4252081587910652\n",
      "Training loss: 0.4536035337618419   1715  0.4319629520177841\n",
      "Training loss: 0.4537944346666336   1716  0.4171054810285568\n",
      "Training loss: 0.4533299058675766   1717  0.4233454018831253\n",
      "Training loss: 0.4524825108902795   1718  0.42795291543006897\n",
      "Training loss: 0.450534958924566   1719  0.41772007942199707\n",
      "Training loss: 0.4510037920304707   1720  0.4113073945045471\n",
      "Training loss: 0.4527275115251541   1721  0.4266325682401657\n",
      "Training loss: 0.4561012514999935   1722  0.417879581451416\n",
      "Training loss: 0.45311277891908375   1723  0.4262278974056244\n",
      "Training loss: 0.45577043294906616   1724  0.41476868093013763\n",
      "Training loss: 0.4512603921549661   1725  0.4357845038175583\n",
      "Training loss: 0.45112291829926626   1726  0.4256482720375061\n",
      "Training loss: 0.45092265733650755   1727  0.4364246353507042\n",
      "Training loss: 0.4529289411646979   1728  0.4245433956384659\n",
      "Training loss: 0.45220773773533957   1729  0.4288095384836197\n",
      "Training loss: 0.4517396773610796   1730  0.42508527636528015\n",
      "Training loss: 0.45109793543815613   1731  0.41541797667741776\n",
      "Training loss: 0.4543594717979431   1732  0.43055059015750885\n",
      "Training loss: 0.45630791783332825   1733  0.43058711290359497\n",
      "Training loss: 0.44905200600624084   1734  0.4362981915473938\n",
      "Training loss: 0.45343409904411863   1735  0.4207017421722412\n",
      "Training loss: 0.4541722131626947   1736  0.43212635815143585\n",
      "Training loss: 0.4666030577250889   1737  0.4436861723661423\n",
      "Training loss: 0.47421563948903767   1738  0.42666780948638916\n",
      "Training loss: 0.4614217111042568   1739  0.4212917983531952\n",
      "Training loss: 0.4566853514739445   1740  0.43468454480171204\n",
      "Training loss: 0.4545656570366451   1741  0.41836947202682495\n",
      "Training loss: 0.45709991242204395   1742  0.4224441796541214\n",
      "Training loss: 0.4540377642427172   1743  0.4257388859987259\n",
      "Training loss: 0.4535334621156965   1744  0.44366325438022614\n",
      "Training loss: 0.44969749237809864   1745  0.4193119704723358\n",
      "Training loss: 0.44665998007569996   1746  0.4274527579545975\n",
      "Training loss: 0.44954319511141094   1747  0.426406130194664\n",
      "Training loss: 0.44825194776058197   1748  0.418008491396904\n",
      "Training loss: 0.44822976206030163   1749  0.42216019332408905\n",
      "Training loss: 0.44897099903651644   1750  0.4185051843523979\n",
      "Training loss: 0.4508056917360851   1751  0.43093669414520264\n",
      "Training loss: 0.4468012239251818   1752  0.4216208904981613\n",
      "Training loss: 0.4460562765598297   1753  0.4271984249353409\n",
      "Training loss: 0.4435464165040425   1754  0.42301616072654724\n",
      "Training loss: 0.4509505863700594   1755  0.4149084687232971\n",
      "Training loss: 0.45000698949609486   1756  0.44443434476852417\n",
      "Training loss: 0.4520714559725353   1757  0.42648860812187195\n",
      "Training loss: 0.4499029070138931   1758  0.4174813777208328\n",
      "Training loss: 0.44900488214833395   1759  0.42641057074069977\n",
      "Training loss: 0.4533690150294985   1760  0.4393286108970642\n",
      "Training loss: 0.45123961355004993   1761  0.41377994418144226\n",
      "Training loss: 0.44911040578569683   1762  0.42065951228141785\n",
      "Training loss: 0.4525007073368345   1763  0.43115760385990143\n",
      "Training loss: 0.45077534445694517   1764  0.4166538268327713\n",
      "Training loss: 0.44688485988548826   1765  0.4261760711669922\n",
      "Training loss: 0.45169006288051605   1766  0.42147886008024216\n",
      "Training loss: 0.45095643401145935   1767  0.42389513552188873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4515384201492582   1768  0.41188783198595047\n",
      "Training loss: 0.45249429983752115   1769  0.42003870010375977\n",
      "Training loss: 0.4505291389567511   1770  0.42635834217071533\n",
      "Training loss: 0.4522106498479843   1771  0.4365610033273697\n",
      "Training loss: 0.4462636262178421   1772  0.4214615076780319\n",
      "Training loss: 0.4492395945957729   1773  0.424550399184227\n",
      "Training loss: 0.44579602990831646   1774  0.4250079095363617\n",
      "Training loss: 0.4461849608591625   1775  0.4258144348859787\n",
      "Training loss: 0.44881595245429445   1776  0.41806499660015106\n",
      "Training loss: 0.44519131098474773   1777  0.4112323597073555\n",
      "Training loss: 0.44463758170604706   1778  0.4266585111618042\n",
      "Training loss: 0.4507383406162262   1779  0.4142521843314171\n",
      "Training loss: 0.4486188271215984   1780  0.41985300183296204\n",
      "Training loss: 0.44549245493752615   1781  0.42493753135204315\n",
      "Training loss: 0.4485915260655539   1782  0.4264573007822037\n",
      "Training loss: 0.4483054833752768   1783  0.4201434850692749\n",
      "Training loss: 0.44870809997831074   1784  0.4227665662765503\n",
      "Training loss: 0.44846324409757343   1785  0.425519123673439\n",
      "Training loss: 0.4491694952760424   1786  0.4128186106681824\n",
      "Training loss: 0.4484067325081144   1787  0.429980993270874\n",
      "Training loss: 0.44777626650674   1788  0.43122580647468567\n",
      "Training loss: 0.44811779260635376   1789  0.4158130884170532\n",
      "Training loss: 0.44730570699487415   1790  0.4285685569047928\n",
      "Training loss: 0.4461151084729603   1791  0.4098576009273529\n",
      "Training loss: 0.4458160123654774   1792  0.41882631182670593\n",
      "Training loss: 0.4471448596034731   1793  0.42097286880016327\n",
      "Training loss: 0.4517683599676405   1794  0.4223795235157013\n",
      "Training loss: 0.44943967461586   1795  0.4257332533597946\n",
      "Training loss: 0.4506398183958871   1796  0.4327506050467491\n",
      "Training loss: 0.4488028564623424   1797  0.42530855536460876\n",
      "Training loss: 0.445131527526038   1798  0.413182333111763\n",
      "Training loss: 0.4439192882605961   1799  0.4328882396221161\n",
      "Training loss: 0.44377633716378895   1800  0.4206199645996094\n",
      "Training loss: 0.44517724428858074   1801  0.41809414327144623\n",
      "Training loss: 0.4509570598602295   1802  0.43667151033878326\n",
      "Training loss: 0.4663087214742388   1803  0.4303223490715027\n",
      "Training loss: 0.45910566406590597   1804  0.4259181022644043\n",
      "Training loss: 0.4590311199426651   1805  0.42326144874095917\n",
      "Training loss: 0.451322357569422   1806  0.4304351210594177\n",
      "Training loss: 0.4498488690171923   1807  0.4207918792963028\n",
      "Training loss: 0.44656771208558765   1808  0.4224092960357666\n",
      "Training loss: 0.4429900475910732   1809  0.42146407067775726\n",
      "Training loss: 0.44559477056775776   1810  0.43092532455921173\n",
      "Training loss: 0.4465323473726   1811  0.4259812980890274\n",
      "Training loss: 0.444893125976835   1812  0.4120628833770752\n",
      "Training loss: 0.44552913521017345   1813  0.4170060455799103\n",
      "Training loss: 0.44759723969868254   1814  0.41520851850509644\n",
      "Training loss: 0.4458872079849243   1815  0.4301523119211197\n",
      "Training loss: 0.44661347355161396   1816  0.4333638846874237\n",
      "Training loss: 0.44811972975730896   1817  0.4188203364610672\n",
      "Training loss: 0.44623298730169025   1818  0.4475112110376358\n",
      "Training loss: 0.44633673982960836   1819  0.42603468894958496\n",
      "Training loss: 0.4470275512763432   1820  0.42024029791355133\n",
      "Training loss: 0.44780389751706806   1821  0.42403537034988403\n",
      "Training loss: 0.44488163931029184   1822  0.4133565127849579\n",
      "Training loss: 0.4422529957124165   1823  0.43627753108739853\n",
      "Training loss: 0.4439221088375364   1824  0.4189281314611435\n",
      "Training loss: 0.4440633910042899   1825  0.4244607537984848\n",
      "Training loss: 0.4399461959089552   1826  0.42164061963558197\n",
      "Training loss: 0.44164521992206573   1827  0.425639346241951\n",
      "Training loss: 0.4414546276841845   1828  0.4313662499189377\n",
      "Training loss: 0.4416131206921169   1829  0.42205579578876495\n",
      "Training loss: 0.44182289285319193   1830  0.4280885308980942\n",
      "Training loss: 0.4461103826761246   1831  0.42228391766548157\n",
      "Training loss: 0.44749010247843607   1832  0.426600843667984\n",
      "Training loss: 0.44585637535367695   1833  0.43123531341552734\n",
      "Training loss: 0.4469158095972879   1834  0.4163509011268616\n",
      "Training loss: 0.4473089022295816   1835  0.42754456400871277\n",
      "Training loss: 0.4468237651245935   1836  0.41020163893699646\n",
      "Training loss: 0.4448676896946771   1837  0.4301033318042755\n",
      "Training loss: 0.4442544175045831   1838  0.4237615019083023\n",
      "Training loss: 0.445767200418881   1839  0.42171449959278107\n",
      "Training loss: 0.4426305911370686   1840  0.4202788174152374\n",
      "Training loss: 0.4434761766876493   1841  0.41637006402015686\n",
      "Training loss: 0.4453860116856439   1842  0.41839396953582764\n",
      "Training loss: 0.44359308906963896   1843  0.42849086225032806\n",
      "Training loss: 0.4478131596531187   1844  0.42519335448741913\n",
      "Training loss: 0.4520376537527357   1845  0.42783182859420776\n",
      "Training loss: 0.45097643775599344   1846  0.4204687029123306\n",
      "Training loss: 0.44599173963069916   1847  0.426429346203804\n",
      "Training loss: 0.4449444349323   1848  0.4279700815677643\n",
      "Training loss: 0.443955197930336   1849  0.4231536388397217\n",
      "Training loss: 0.44242844198431286   1850  0.4277315139770508\n",
      "Training loss: 0.4445552719490869   1851  0.4227144569158554\n",
      "Training loss: 0.44740489976746695   1852  0.43317608535289764\n",
      "Training loss: 0.4417313975947244   1853  0.4160654544830322\n",
      "Training loss: 0.44212465839726584   1854  0.4201090782880783\n",
      "Training loss: 0.44501415746552603   1855  0.4183911308646202\n",
      "Training loss: 0.44316559178488596   1856  0.43253903090953827\n",
      "Training loss: 0.4443283442940031   1857  0.41251029074192047\n",
      "Training loss: 0.44334376071180615   1858  0.41193045675754547\n",
      "Training loss: 0.44066489594323294   1859  0.43008284270763397\n",
      "Training loss: 0.44522340169974733   1860  0.41998396813869476\n",
      "Training loss: 0.4438679303441729   1861  0.4155801385641098\n",
      "Training loss: 0.44345391009535107   1862  0.4240932911634445\n",
      "Training loss: 0.442898622580937   1863  0.432186558842659\n",
      "Training loss: 0.44006633332797457   1864  0.42237570881843567\n",
      "Training loss: 0.4410672996725355   1865  0.4254268556833267\n",
      "Training loss: 0.4410642087459564   1866  0.41471822559833527\n",
      "Training loss: 0.4403974711894989   1867  0.42850837111473083\n",
      "Training loss: 0.4428996720484325   1868  0.42795930802822113\n",
      "Training loss: 0.43928163392203196   1869  0.4217468202114105\n",
      "Training loss: 0.44111948353903635   1870  0.4215968996286392\n",
      "Training loss: 0.4446797030312674   1871  0.43003447353839874\n",
      "Training loss: 0.44245265211377827   1872  0.4231179803609848\n",
      "Training loss: 0.44418804560388836   1873  0.42441363632678986\n",
      "Training loss: 0.4439133746283395   1874  0.4261394143104553\n",
      "Training loss: 0.44199817734105246   1875  0.42614728957414627\n",
      "Training loss: 0.4430414778845651   1876  0.43732617050409317\n",
      "Training loss: 0.441864081791469   1877  0.4166423976421356\n",
      "Training loss: 0.4428057074546814   1878  0.4248892813920975\n",
      "Training loss: 0.4464306150163923   1879  0.4332340136170387\n",
      "Training loss: 0.44840255805424284   1880  0.42529504001140594\n",
      "Training loss: 0.4448524160044534   1881  0.4377993047237396\n",
      "Training loss: 0.4455268659761974   1882  0.43057429790496826\n",
      "Training loss: 0.4392009292330061   1883  0.4178085923194885\n",
      "Training loss: 0.44107839465141296   1884  0.41808992624282837\n",
      "Training loss: 0.4405946135520935   1885  0.4324916750192642\n",
      "Training loss: 0.44280510927949635   1886  0.437715545296669\n",
      "Training loss: 0.44441537984779905   1887  0.4249618649482727\n",
      "Training loss: 0.44278621886457714   1888  0.42591701447963715\n",
      "Training loss: 0.44627555353300913   1889  0.43381933867931366\n",
      "Training loss: 0.44279044227940695   1890  0.4192994683980942\n",
      "Training loss: 0.43743004756314413   1891  0.4196145534515381\n",
      "Training loss: 0.43788881599903107   1892  0.4177899956703186\n",
      "Training loss: 0.440329338823046   1893  0.4195856600999832\n",
      "Training loss: 0.4419803555522646   1894  0.4253814369440079\n",
      "Training loss: 0.4400273731776646   1895  0.4289265424013138\n",
      "Training loss: 0.4387351168053491   1896  0.4234688729047775\n",
      "Training loss: 0.4374918213912419   1897  0.4239721745252609\n",
      "Training loss: 0.4401205714259829   1898  0.42646901309490204\n",
      "Training loss: 0.4376765766314098   1899  0.41884005069732666\n",
      "Training loss: 0.4381601150546755   1900  0.4142034202814102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.43742023408412933   1901  0.4210031181573868\n",
      "Training loss: 0.4419423575912203   1902  0.41663211584091187\n",
      "Training loss: 0.4401667756693704   1903  0.41640280187129974\n",
      "Training loss: 0.4400188901594707   1904  0.42913688719272614\n",
      "Training loss: 0.441036839570318   1905  0.4150788486003876\n",
      "Training loss: 0.4399653949907848   1906  0.4242722988128662\n",
      "Training loss: 0.44242620042392183   1907  0.42333002388477325\n",
      "Training loss: 0.4391521449599947   1908  0.4293709546327591\n",
      "Training loss: 0.44202831174646107   1909  0.4269978106021881\n",
      "Training loss: 0.4363934887307031   1910  0.4115978628396988\n",
      "Training loss: 0.43851955235004425   1911  0.43218448758125305\n",
      "Training loss: 0.4359066230910165   1912  0.42651695013046265\n",
      "Training loss: 0.43900237764631   1913  0.42199644446372986\n",
      "Training loss: 0.4404508577925818   1914  0.43074795603752136\n",
      "Training loss: 0.43886728159018923   1915  0.42755311727523804\n",
      "Training loss: 0.43920678751809256   1916  0.42234010994434357\n",
      "Training loss: 0.43583843963486807   1917  0.43170343339443207\n",
      "Training loss: 0.4341701354299273   1918  0.4295908063650131\n",
      "Training loss: 0.44116257556847166   1919  0.4260779917240143\n",
      "Training loss: 0.43791452050209045   1920  0.4288754314184189\n",
      "Training loss: 0.43849726234163555   1921  0.4146432429552078\n",
      "Training loss: 0.43931340319769724   1922  0.41977275907993317\n",
      "Training loss: 0.4392137825489044   1923  0.4235583692789078\n",
      "Training loss: 0.43557009100914   1924  0.4153628796339035\n",
      "Training loss: 0.4399488738604954   1925  0.4275191128253937\n",
      "Training loss: 0.4357184022665024   1926  0.41573627293109894\n",
      "Training loss: 0.4378771505185536   1927  0.41482020914554596\n",
      "Training loss: 0.4379404698099409   1928  0.4200664609670639\n",
      "Training loss: 0.4360527183328356   1929  0.4201391637325287\n",
      "Training loss: 0.44017005605357035   1930  0.4209045469760895\n",
      "Training loss: 0.4390225389174053   1931  0.40702828019857407\n",
      "Training loss: 0.43865448236465454   1932  0.42422372102737427\n",
      "Training loss: 0.43882521986961365   1933  0.4227682948112488\n",
      "Training loss: 0.4384947270154953   1934  0.4243972599506378\n",
      "Training loss: 0.438179686665535   1935  0.4131588786840439\n",
      "Training loss: 0.438080621617181   1936  0.43024320900440216\n",
      "Training loss: 0.4374033489397594   1937  0.42733408510684967\n",
      "Training loss: 0.4370647647551128   1938  0.4160163700580597\n",
      "Training loss: 0.4373068021876471   1939  0.4163242131471634\n",
      "Training loss: 0.43778049520083834   1940  0.41811173409223557\n",
      "Training loss: 0.43775039485522677   1941  0.40945932269096375\n",
      "Training loss: 0.43849828839302063   1942  0.42676642537117004\n",
      "Training loss: 0.43722568452358246   1943  0.42991217970848083\n",
      "Training loss: 0.4393084730420794   1944  0.4183626174926758\n",
      "Training loss: 0.43935630151203703   1945  0.4196094125509262\n",
      "Training loss: 0.4376142237867628   1946  0.44061416387557983\n",
      "Training loss: 0.43739298837525503   1947  0.41748350858688354\n",
      "Training loss: 0.43474613981587545   1948  0.40852656960487366\n",
      "Training loss: 0.4359776037079947   1949  0.4244108647108078\n",
      "Training loss: 0.43598134389945437   1950  0.4227074384689331\n",
      "Training loss: 0.4371615797281265   1951  0.4164956659078598\n",
      "Training loss: 0.43499869108200073   1952  0.42567406594753265\n",
      "Training loss: 0.4331393263169697   1953  0.42849622666835785\n",
      "Training loss: 0.4362861854689462   1954  0.427667498588562\n",
      "Training loss: 0.43384713785988943   1955  0.41416873037815094\n",
      "Training loss: 0.43632301475320545   1956  0.42745405435562134\n",
      "Training loss: 0.43377645100866047   1957  0.4218307435512543\n",
      "Training loss: 0.43937238837991444   1958  0.42270076274871826\n",
      "Training loss: 0.43520652822085787   1959  0.42355601489543915\n",
      "Training loss: 0.4346709166254316   1960  0.4304172992706299\n",
      "Training loss: 0.43893489028726307   1961  0.4213327020406723\n",
      "Training loss: 0.4390325163091932   1962  0.4381329417228699\n",
      "Training loss: 0.4369501918554306   1963  0.4274470657110214\n",
      "Training loss: 0.4359185163463865   1964  0.4159824550151825\n",
      "Training loss: 0.4331536569765636   1965  0.42449522018432617\n",
      "Training loss: 0.45258558860846926   1966  0.45013009011745453\n",
      "Training loss: 0.4851357213088444   1967  0.44330208003520966\n",
      "Training loss: 0.48422633750098093   1968  0.4337559938430786\n",
      "Training loss: 0.4674365073442459   1969  0.43061695992946625\n",
      "Training loss: 0.45142023265361786   1970  0.42209118604660034\n",
      "Training loss: 0.44576183387211393   1971  0.41745100915431976\n",
      "Training loss: 0.44390031695365906   1972  0.4242156445980072\n",
      "Training loss: 0.4417942294052669   1973  0.4224957674741745\n",
      "Training loss: 0.44115430755274637   1974  0.4209700673818588\n",
      "Training loss: 0.4409683964082173   1975  0.431434229016304\n",
      "Training loss: 0.44113847826208386   1976  0.41727156937122345\n",
      "Training loss: 0.43617638094084604   1977  0.4288017153739929\n",
      "Training loss: 0.43452838914734976   1978  0.4107137471437454\n",
      "Training loss: 0.43653406628540586   1979  0.42899148166179657\n",
      "Training loss: 0.4357207013028009   1980  0.41848108172416687\n",
      "Training loss: 0.43722081397260937   1981  0.4180699735879898\n",
      "Training loss: 0.43541804594652994   1982  0.41308562457561493\n",
      "Training loss: 0.43673701158591677   1983  0.42197485268116\n",
      "Training loss: 0.43455095163413454   1984  0.4261336475610733\n",
      "Training loss: 0.4375790442739214   1985  0.41320670396089554\n",
      "Training loss: 0.43652202614716124   1986  0.42554430663585663\n",
      "Training loss: 0.43685054779052734   1987  0.4325641393661499\n",
      "Training loss: 0.4343571066856384   1988  0.4305294454097748\n",
      "Training loss: 0.4340688330786569   1989  0.41421307623386383\n",
      "Training loss: 0.4336354647363935   1990  0.4155547171831131\n",
      "Training loss: 0.44200093831334797   1991  0.41868822276592255\n",
      "Training loss: 0.43968414621693747   1992  0.41411788761615753\n",
      "Training loss: 0.43278970037187847   1993  0.42869508266448975\n",
      "Training loss: 0.43469469036374775   1994  0.4267626702785492\n",
      "Training loss: 0.43664627628667013   1995  0.4134063944220543\n",
      "Training loss: 0.4361830587897982   1996  0.4272115081548691\n",
      "Training loss: 0.4338099786213466   1997  0.4085261821746826\n",
      "Training loss: 0.433750861457416   1998  0.4263905882835388\n",
      "Training loss: 0.4353958602462496   1999  0.4159590005874634\n",
      "Training loss: 0.43830415180751253   2000  0.41838397830724716\n",
      "Training loss: 0.43634573902402607   2001  0.4188980311155319\n",
      "Training loss: 0.43972185254096985   2002  0.4198458343744278\n",
      "Training loss: 0.4364709662539618   2003  0.4211546927690506\n",
      "Training loss: 0.4322180428675243   2004  0.4137529879808426\n",
      "Training loss: 0.4315842943532126   2005  0.4151482880115509\n",
      "Training loss: 0.43556795588561464   2006  0.4311913251876831\n",
      "Training loss: 0.4338086524180004   2007  0.41324934363365173\n",
      "Training loss: 0.43131289524691446   2008  0.4221295863389969\n",
      "Training loss: 0.43644638785294126   2009  0.42124588787555695\n",
      "Training loss: 0.43405840013708386   2010  0.4186854511499405\n",
      "Training loss: 0.4346950032881328   2011  0.4291697144508362\n",
      "Training loss: 0.4329927521092551   2012  0.42469654977321625\n",
      "Training loss: 0.43152814890657154   2013  0.41651229560375214\n",
      "Training loss: 0.43189518579414915   2014  0.4154161214828491\n",
      "Training loss: 0.4317662630762373   2015  0.4275984764099121\n",
      "Training loss: 0.4320006029946463   2016  0.41705172508955\n",
      "Training loss: 0.4320786850793021   2017  0.424685001373291\n",
      "Training loss: 0.43159961487565723   2018  0.4206506311893463\n",
      "Training loss: 0.43296970639910015   2019  0.4190489500761032\n",
      "Training loss: 0.4320309864623206   2020  0.428949698805809\n",
      "Training loss: 0.4323194601706096   2021  0.42219991981983185\n",
      "Training loss: 0.42992883707795826   2022  0.4208268076181412\n",
      "Training loss: 0.43326205015182495   2023  0.4120461791753769\n",
      "Training loss: 0.4359325404678072   2024  0.41169673204421997\n",
      "Training loss: 0.4332762658596039   2025  0.426127165555954\n",
      "Training loss: 0.4326815477439335   2026  0.43015073239803314\n",
      "Training loss: 0.4347219190427235   2027  0.429034948348999\n",
      "Training loss: 0.4311303177050182   2028  0.4208277612924576\n",
      "Training loss: 0.42866550598825726   2029  0.4077889770269394\n",
      "Training loss: 0.43134668682302746   2030  0.41154129803180695\n",
      "Training loss: 0.4308917692729405   2031  0.4144778698682785\n",
      "Training loss: 0.4330166684729712   2032  0.4269399493932724\n",
      "Training loss: 0.43058418376105173   2033  0.4223521202802658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.43065534106322695   2034  0.4161585569381714\n",
      "Training loss: 0.43130276671477724   2035  0.4237271249294281\n",
      "Training loss: 0.4338745581252234   2036  0.42665134370326996\n",
      "Training loss: 0.43055179502282825   2037  0.41877666115760803\n",
      "Training loss: 0.42915694415569305   2038  0.41643479466438293\n",
      "Training loss: 0.43396329454013277   2039  0.4253055453300476\n",
      "Training loss: 0.4328002333641052   2040  0.41313162446022034\n",
      "Training loss: 0.4301786742040089   2041  0.42425039410591125\n",
      "Training loss: 0.4303323988403593   2042  0.41885410249233246\n",
      "Training loss: 0.4309165094579969   2043  0.43486931920051575\n",
      "Training loss: 0.431599172098296   2044  0.4078539311885834\n",
      "Training loss: 0.4322362393140793   2045  0.4185013920068741\n",
      "Training loss: 0.4296954231602805   2046  0.41445397585630417\n",
      "Training loss: 0.4328245882477079   2047  0.42154310643672943\n",
      "Training loss: 0.43056649821145193   2048  0.4281664341688156\n",
      "Training loss: 0.43444552591868807   2049  0.40912748873233795\n",
      "Training loss: 0.43082934617996216   2050  0.41182152926921844\n",
      "Training loss: 0.43444162820066723   2051  0.4211871027946472\n",
      "Training loss: 0.43054743324007305   2052  0.41570572555065155\n",
      "Training loss: 0.4326791273696082   2053  0.4121001660823822\n",
      "Training loss: 0.43067591743809835   2054  0.418209433555603\n",
      "Training loss: 0.4310490531580789   2055  0.4157741516828537\n",
      "Training loss: 0.4288456716707775   2056  0.42499133944511414\n",
      "Training loss: 0.4307840125901358   2057  0.4199690520763397\n",
      "Training loss: 0.4298105665615627   2058  0.4222339540719986\n",
      "Training loss: 0.4313770575182779   2059  0.4214761257171631\n",
      "Training loss: 0.43144391477108   2060  0.41984839737415314\n",
      "Training loss: 0.43000742154461996   2061  0.41203727573156357\n",
      "Training loss: 0.43107482365199495   2062  0.41181957721710205\n",
      "Training loss: 0.42919835661138805   2063  0.4239141047000885\n",
      "Training loss: 0.4319969117641449   2064  0.40998218953609467\n",
      "Training loss: 0.4323219039610454   2065  0.4262252300977707\n",
      "Training loss: 0.4323827922344208   2066  0.41234534978866577\n",
      "Training loss: 0.4323870156492506   2067  0.42704060673713684\n",
      "Training loss: 0.43138276040554047   2068  0.4151909574866295\n",
      "Training loss: 0.4325904824904033   2069  0.41618217527866364\n",
      "Training loss: 0.4316682517528534   2070  0.4333876967430115\n",
      "Training loss: 0.4329858102968761   2071  0.42010071873664856\n",
      "Training loss: 0.4315521589347294   2072  0.4150235801935196\n",
      "Training loss: 0.4326939114502498   2073  0.42905503511428833\n",
      "Training loss: 0.43365263513156344   2074  0.42648176848888397\n",
      "Training loss: 0.431882381439209   2075  0.41467177867889404\n",
      "Training loss: 0.430189596755164   2076  0.42446285486221313\n",
      "Training loss: 0.42974344321659635   2077  0.4233137369155884\n",
      "Training loss: 0.431804501584598   2078  0.4316822737455368\n",
      "Training loss: 0.4422339754445212   2079  0.4474591314792633\n",
      "Training loss: 0.4488950265305383   2080  0.4170594960451126\n",
      "Training loss: 0.4435537861926215   2081  0.42663417011499405\n",
      "Training loss: 0.4414175408227103   2082  0.4195728003978729\n",
      "Training loss: 0.4359603545495442   2083  0.42314663529396057\n",
      "Training loss: 0.43183920851775576   2084  0.42317645251750946\n",
      "Training loss: 0.4293935362781797   2085  0.4203280881047249\n",
      "Training loss: 0.42969956355435507   2086  0.4196648895740509\n",
      "Training loss: 0.4285385012626648   2087  0.42039864510297775\n",
      "Training loss: 0.4286243234361921   2088  0.41525810956954956\n",
      "Training loss: 0.427226185798645   2089  0.41373519599437714\n",
      "Training loss: 0.42982745809214457   2090  0.41652458906173706\n",
      "Training loss: 0.4301734077078955   2091  0.4116107225418091\n",
      "Training loss: 0.4322806830917086   2092  0.41016822308301926\n",
      "Training loss: 0.42752874961921145   2093  0.43453988432884216\n",
      "Training loss: 0.4290634606565748   2094  0.4207235127687454\n",
      "Training loss: 0.4282250127622059   2095  0.4249170124530792\n",
      "Training loss: 0.4267926514148712   2096  0.4309140741825104\n",
      "Training loss: 0.4260421097278595   2097  0.42140716314315796\n",
      "Training loss: 0.4288809022733143   2098  0.42026756703853607\n",
      "Training loss: 0.4274446836539677   2099  0.41819553077220917\n",
      "Training loss: 0.42469718626567293   2100  0.42185989022254944\n",
      "Training loss: 0.4315909445285797   2101  0.42219097912311554\n",
      "Training loss: 0.42931048359189716   2102  0.418595090508461\n",
      "Training loss: 0.43435883096286226   2103  0.4203203618526459\n",
      "Training loss: 0.4305323192051479   2104  0.4277511239051819\n",
      "Training loss: 0.432753677879061   2105  0.41924554109573364\n",
      "Training loss: 0.4306188630206244   2106  0.4287945181131363\n",
      "Training loss: 0.4291461727448872   2107  0.4290797561407089\n",
      "Training loss: 0.4282443587269102   2108  0.4230143129825592\n",
      "Training loss: 0.42865006838526043   2109  0.4273250699043274\n",
      "Training loss: 0.4234173148870468   2110  0.42780372500419617\n",
      "Training loss: 0.4269320262329919   2111  0.42058104276657104\n",
      "Training loss: 0.4297559474195753   2112  0.4202948808670044\n",
      "Training loss: 0.42989650794437956   2113  0.4154518097639084\n",
      "Training loss: 0.4287405184337071   2114  0.41228893399238586\n",
      "Training loss: 0.4243135154247284   2115  0.42246510833501816\n",
      "Training loss: 0.4273093832390649   2116  0.4339589476585388\n",
      "Training loss: 0.42816629792962757   2117  0.41882985830307007\n",
      "Training loss: 0.42802566502775463   2118  0.42193514108657837\n",
      "Training loss: 0.4261618150132043   2119  0.41203509271144867\n",
      "Training loss: 0.426063739827701   2120  0.4091055691242218\n",
      "Training loss: 0.4260400916848864   2121  0.42308664321899414\n",
      "Training loss: 0.4292472026177815   2122  0.41672326624393463\n",
      "Training loss: 0.42918492427894045   2123  0.42673133313655853\n",
      "Training loss: 0.4292561113834381   2124  0.43349744379520416\n",
      "Training loss: 0.43112265850816456   2125  0.4174989014863968\n",
      "Training loss: 0.43273158584322247   2126  0.4185945987701416\n",
      "Training loss: 0.4312217043978827   2127  0.40227389335632324\n",
      "Training loss: 0.434975619826998   2128  0.4253449887037277\n",
      "Training loss: 0.43102677166461945   2129  0.41932813823223114\n",
      "Training loss: 0.4316119892256601   2130  0.42892321944236755\n",
      "Training loss: 0.42716851830482483   2131  0.41128501296043396\n",
      "Training loss: 0.4244753973824637   2132  0.4232281595468521\n",
      "Training loss: 0.42867471277713776   2133  0.42073817551136017\n",
      "Training loss: 0.425470198903765   2134  0.4137341380119324\n",
      "Training loss: 0.4243429069008146   2135  0.4170145243406296\n",
      "Training loss: 0.42545599596840994   2136  0.4233930706977844\n",
      "Training loss: 0.4272351073367255   2137  0.41611427068710327\n",
      "Training loss: 0.42313183205468313   2138  0.4266443997621536\n",
      "Training loss: 0.42500834805624826   2139  0.4152904152870178\n",
      "Training loss: 0.42550483133111683   2140  0.42709141969680786\n",
      "Training loss: 0.42589646364961353   2141  0.42353595793247223\n",
      "Training loss: 0.4277807580573218   2142  0.4065505564212799\n",
      "Training loss: 0.42854686081409454   2143  0.42103715240955353\n",
      "Training loss: 0.4297809047358377   2144  0.4209764152765274\n",
      "Training loss: 0.4255351892539433   2145  0.4170479327440262\n",
      "Training loss: 0.4266535980360849   2146  0.42031343281269073\n",
      "Training loss: 0.42832587020737783   2147  0.4156978577375412\n",
      "Training loss: 0.4265703686646053   2148  0.41960904002189636\n",
      "Training loss: 0.4267645244087492   2149  0.4197313189506531\n",
      "Training loss: 0.4265254884958267   2150  0.42049677670001984\n",
      "Training loss: 0.4261170732123511   2151  0.42683184146881104\n",
      "Training loss: 0.4261277658598764   2152  0.42397984862327576\n",
      "Training loss: 0.4267830635820116   2153  0.4246521294116974\n",
      "Training loss: 0.42392305604049135   2154  0.4210032969713211\n",
      "Training loss: 0.4270715245178768   2155  0.4308973699808121\n",
      "Training loss: 0.42785697323935373   2156  0.4230894297361374\n",
      "Training loss: 0.42864011228084564   2157  0.42670439183712006\n",
      "Training loss: 0.428860034261431   2158  0.40441102534532547\n",
      "Training loss: 0.42517114749976564   2159  0.4247475191950798\n",
      "Training loss: 0.4270153833287103   2160  0.41107262670993805\n",
      "Training loss: 0.4299292266368866   2161  0.42708225548267365\n",
      "Training loss: 0.4276519758360727   2162  0.4246726930141449\n",
      "Training loss: 0.42741471954754423   2163  0.411093607544899\n",
      "Training loss: 0.425059688942773   2164  0.41704580187797546\n",
      "Training loss: 0.425495337162699   2165  0.42346595227718353\n",
      "Training loss: 0.42550428424562725   2166  0.4212529808282852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4257000450577055   2167  0.4269804507493973\n",
      "Training loss: 0.42835501049246105   2168  0.4291510283946991\n",
      "Training loss: 0.42705913526671274   2169  0.4090203195810318\n",
      "Training loss: 0.4238929876259395   2170  0.4185274988412857\n",
      "Training loss: 0.4267979498420443   2171  0.42255255579948425\n",
      "Training loss: 0.42552558864865986   2172  0.41717927157878876\n",
      "Training loss: 0.42487660476139616   2173  0.433111771941185\n",
      "Training loss: 0.4278194542442049   2174  0.42694883793592453\n",
      "Training loss: 0.42119783588818144   2175  0.4110095649957657\n",
      "Training loss: 0.4245309169803347   2176  0.42535941302776337\n",
      "Training loss: 0.4266851863690785   2177  0.4314318895339966\n",
      "Training loss: 0.42683098358767374   2178  0.42204780876636505\n",
      "Training loss: 0.425224495785577   2179  0.43248747289180756\n",
      "Training loss: 0.4229059134210859   2180  0.42348430305719376\n",
      "Training loss: 0.4218550516026361   2181  0.4253982752561569\n",
      "Training loss: 0.4263057985476085   2182  0.4158557280898094\n",
      "Training loss: 0.42100783969674793   2183  0.40984974801540375\n",
      "Training loss: 0.4252058970076697   2184  0.4305811822414398\n",
      "Training loss: 0.4236618919031961   2185  0.4251125827431679\n",
      "Training loss: 0.421592258981296   2186  0.4172853156924248\n",
      "Training loss: 0.42447551233427866   2187  0.4198226109147072\n",
      "Training loss: 0.4247039726802281   2188  0.4087789058685303\n",
      "Training loss: 0.42155123182705473   2189  0.43457643687725067\n",
      "Training loss: 0.42687239178589415   2190  0.41676852107048035\n",
      "Training loss: 0.42365165267671856   2191  0.4153181314468384\n",
      "Training loss: 0.4250142531735556   2192  0.4215754270553589\n",
      "Training loss: 0.4239443710872105   2193  0.4193519651889801\n",
      "Training loss: 0.423223648752485   2194  0.42419348657131195\n",
      "Training loss: 0.4253692924976349   2195  0.42291565239429474\n",
      "Training loss: 0.4216872751712799   2196  0.4180820360779762\n",
      "Training loss: 0.42168016731739044   2197  0.41219374537467957\n",
      "Training loss: 0.4235101618937084   2198  0.41272158920764923\n",
      "Training loss: 0.4249886614935739   2199  0.424153208732605\n",
      "Training loss: 0.424075003181185   2200  0.4150099828839302\n",
      "Training loss: 0.42609395938260214   2201  0.4238293766975403\n",
      "Training loss: 0.42614148557186127   2202  0.4134628027677536\n",
      "Training loss: 0.425896765930312   2203  0.4148091822862625\n",
      "Training loss: 0.4250043375151498   2204  0.42616936564445496\n",
      "Training loss: 0.4247024144445147   2205  0.4137631952762604\n",
      "Training loss: 0.42413791588374544   2206  0.4220569282770157\n",
      "Training loss: 0.42160752202783314   2207  0.41718679666519165\n",
      "Training loss: 0.42273164859839846   2208  0.42228731513023376\n",
      "Training loss: 0.42368983158043455   2209  0.4214239567518234\n",
      "Training loss: 0.42717012763023376   2210  0.41551437973976135\n",
      "Training loss: 0.42688286304473877   2211  0.42883794009685516\n",
      "Training loss: 0.42270186969212126   2212  0.4268132895231247\n",
      "Training loss: 0.42401026827948435   2213  0.42288602888584137\n",
      "Training loss: 0.42204031348228455   2214  0.4235769361257553\n",
      "Training loss: 0.4243096836975643   2215  0.42092955112457275\n",
      "Training loss: 0.4251575086797987   2216  0.42026765644550323\n",
      "Training loss: 0.4275962454932077   2217  0.4222624897956848\n",
      "Training loss: 0.42422239908150267   2218  0.42237675189971924\n",
      "Training loss: 0.4227673326219831   2219  0.40916525572538376\n",
      "Training loss: 0.42636549047061373   2220  0.41571299731731415\n",
      "Training loss: 0.42259461113384794   2221  0.4228412061929703\n",
      "Training loss: 0.4234211253268378   2222  0.4132719784975052\n",
      "Training loss: 0.42360298548425945   2223  0.4298855662345886\n",
      "Training loss: 0.4218660145998001   2224  0.43235283344984055\n",
      "Training loss: 0.4231853974717004   2225  0.4142870008945465\n",
      "Training loss: 0.42334477816309246   2226  0.4159421846270561\n",
      "Training loss: 0.42210331346307484   2227  0.41993848979473114\n",
      "Training loss: 0.42013869115284513   2228  0.4304538816213608\n",
      "Training loss: 0.42158426131520954   2229  0.4125289022922516\n",
      "Training loss: 0.42495458253792356   2230  0.41910332441329956\n",
      "Training loss: 0.4242806775229318   2231  0.4324028640985489\n",
      "Training loss: 0.42281097386564526   2232  0.41786111891269684\n",
      "Training loss: 0.4228046493870871   2233  0.42916011810302734\n",
      "Training loss: 0.42401958789144245   2234  0.41504522413015366\n",
      "Training loss: 0.4232115958418165   2235  0.4279758334159851\n",
      "Training loss: 0.42409745497362955   2236  0.4295117259025574\n",
      "Training loss: 0.4216485321521759   2237  0.4210323244333267\n",
      "Training loss: 0.4215023475033896   2238  0.4319746047258377\n",
      "Training loss: 0.4236982762813568   2239  0.4270552098751068\n",
      "Training loss: 0.4222971946001053   2240  0.4209072142839432\n",
      "Training loss: 0.4226249541555132   2241  0.4200865924358368\n",
      "Training loss: 0.420997428042548   2242  0.42425277829170227\n",
      "Training loss: 0.4207861146756581   2243  0.4077526405453682\n",
      "Training loss: 0.4242579575095858   2244  0.4095398932695389\n",
      "Training loss: 0.4195786075932639   2245  0.4159995764493942\n",
      "Training loss: 0.42067266362054007   2246  0.42093582451343536\n",
      "Training loss: 0.4221500222172056   2247  0.41544661670923233\n",
      "Training loss: 0.420871907046863   2248  0.4198436439037323\n",
      "Training loss: 0.42566912301949095   2249  0.4238673225045204\n",
      "Training loss: 0.4222672241074698   2250  0.41618333756923676\n",
      "Training loss: 0.42868146300315857   2251  0.4301248639822006\n",
      "Training loss: 0.4290814293282373   2252  0.4229639023542404\n",
      "Training loss: 0.4288582865680967   2253  0.4250868856906891\n",
      "Training loss: 0.4266291026558195   2254  0.4178183078765869\n",
      "Training loss: 0.4240872881242207   2255  0.4081931486725807\n",
      "Training loss: 0.42536360025405884   2256  0.42459240555763245\n",
      "Training loss: 0.42048237579209463   2257  0.41299615800380707\n",
      "Training loss: 0.42484928028924124   2258  0.429964080452919\n",
      "Training loss: 0.42099920979567934   2259  0.4254116415977478\n",
      "Training loss: 0.4234317626271929   2260  0.41061295568943024\n",
      "Training loss: 0.4243062117270061   2261  0.421670064330101\n",
      "Training loss: 0.424721930708204   2262  0.4114658087491989\n",
      "Training loss: 0.42276247910090853   2263  0.4197389483451843\n",
      "Training loss: 0.4267592281103134   2264  0.43369758129119873\n",
      "Training loss: 0.42468983573572977   2265  0.4242208003997803\n",
      "Training loss: 0.4262489527463913   2266  0.4172207787632942\n",
      "Training loss: 0.4222406595945358   2267  0.4147140085697174\n",
      "Training loss: 0.4228027995143618   2268  0.41944456100463867\n",
      "Training loss: 0.4191373416355678   2269  0.42507050931453705\n",
      "Training loss: 0.42315480751650675   2270  0.41528718173503876\n",
      "Training loss: 0.4235139638185501   2271  0.41519895195961\n",
      "Training loss: 0.42128530357565197   2272  0.42156174778938293\n",
      "Training loss: 0.41972064333302633   2273  0.4209514260292053\n",
      "Training loss: 0.4194146692752838   2274  0.406307652592659\n",
      "Training loss: 0.4194900883095605   2275  0.41539014875888824\n",
      "Training loss: 0.42305542315755573   2276  0.42301368713378906\n",
      "Training loss: 0.42353014647960663   2277  0.42863576114177704\n",
      "Training loss: 0.4209557814257486   2278  0.4153812676668167\n",
      "Training loss: 0.4213321783712932   2279  0.41739441454410553\n",
      "Training loss: 0.4205100174461092   2280  0.4266916513442993\n",
      "Training loss: 0.4190932427133833   2281  0.42071790993213654\n",
      "Training loss: 0.42073229593890055   2282  0.4225916117429733\n",
      "Training loss: 0.4218511027949197   2283  0.41822783648967743\n",
      "Training loss: 0.41905148114476887   2284  0.4275575280189514\n",
      "Training loss: 0.4215280818087714   2285  0.4188031107187271\n",
      "Training loss: 0.42127904295921326   2286  0.4134087637066841\n",
      "Training loss: 0.4201153303895678   2287  0.4239824190735817\n",
      "Training loss: 0.4205450883933476   2288  0.42494121193885803\n",
      "Training loss: 0.42036670233522144   2289  0.4202590733766556\n",
      "Training loss: 0.4152881588254656   2290  0.4264025241136551\n",
      "Training loss: 0.4171943409102304   2291  0.4131636321544647\n",
      "Training loss: 0.4198782060827528   2292  0.41789761930704117\n",
      "Training loss: 0.4195404989378793   2293  0.4131876677274704\n",
      "Training loss: 0.42113778633730753   2294  0.42010338604450226\n",
      "Training loss: 0.4187139059816088   2295  0.4149811863899231\n",
      "Training loss: 0.4219426342419216   2296  0.4287683963775635\n",
      "Training loss: 0.42243896637644085   2297  0.4246273636817932\n",
      "Training loss: 0.4201915306704385   2298  0.42687442898750305\n",
      "Training loss: 0.42279376089572906   2299  0.426162987947464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.42500826929296764   2300  0.41203773021698\n",
      "Training loss: 0.424361099089895   2301  0.4226606786251068\n",
      "Training loss: 0.42348247127873556   2302  0.4140232503414154\n",
      "Training loss: 0.4208999808345522   2303  0.41925445944070816\n",
      "Training loss: 0.4225529560020992   2304  0.41785678267478943\n",
      "Training loss: 0.42361422308853697   2305  0.4270898550748825\n",
      "Training loss: 0.41993012385708944   2306  0.4114725589752197\n",
      "Training loss: 0.4220701498644693   2307  0.410601407289505\n",
      "Training loss: 0.4203590282372066   2308  0.4186387211084366\n",
      "Training loss: 0.42254844307899475   2309  0.41022513061761856\n",
      "Training loss: 0.4239001146384648   2310  0.4124274104833603\n",
      "Training loss: 0.4212442487478256   2311  0.42254889011383057\n",
      "Training loss: 0.4211281750883375   2312  0.4250161796808243\n",
      "Training loss: 0.4198496341705322   2313  0.42183706164360046\n",
      "Training loss: 0.421266394002097   2314  0.41915762424468994\n",
      "Training loss: 0.4205546293939863   2315  0.4039642736315727\n",
      "Training loss: 0.4193953722715378   2316  0.41565099358558655\n",
      "Training loss: 0.4204859882593155   2317  0.4318370372056961\n",
      "Training loss: 0.4211460586105074   2318  0.42113102972507477\n",
      "Training loss: 0.4193171624626432   2319  0.4087468683719635\n",
      "Training loss: 0.41865512302943636   2320  0.4239592105150223\n",
      "Training loss: 0.42215282789298464   2321  0.4061809331178665\n",
      "Training loss: 0.4205451394830431   2322  0.42125876247882843\n",
      "Training loss: 0.4187784961291722   2323  0.4305798411369324\n",
      "Training loss: 0.41965366687093464   2324  0.42608876526355743\n",
      "Training loss: 0.42105308175086975   2325  0.4268152117729187\n",
      "Training loss: 0.4186378014939172   2326  0.41710057854652405\n",
      "Training loss: 0.41492772315229687   2327  0.42406246066093445\n",
      "Training loss: 0.419459479195731   2328  0.4268140643835068\n",
      "Training loss: 0.416329881974629   2329  0.42352473735809326\n",
      "Training loss: 0.4156200481312616   2330  0.41388969123363495\n",
      "Training loss: 0.4186793438025883   2331  0.4165138527750969\n",
      "Training loss: 0.41918814395155224   2332  0.4297649413347244\n",
      "Training loss: 0.42002454825810026   2333  0.42422740161418915\n",
      "Training loss: 0.42160652365003315   2334  0.42767244577407837\n",
      "Training loss: 0.4274765444653375   2335  0.41162822395563126\n",
      "Training loss: 0.42549277416297365   2336  0.4322667568922043\n",
      "Training loss: 0.42341548204421997   2337  0.4270995855331421\n",
      "Training loss: 0.4208456746169499   2338  0.4279042184352875\n",
      "Training loss: 0.4188512606280191   2339  0.4129651114344597\n",
      "Training loss: 0.42010881858212606   2340  0.415420264005661\n",
      "Training loss: 0.41697457432746887   2341  0.4049464166164398\n",
      "Training loss: 0.41679797427994864   2342  0.4368133991956711\n",
      "Training loss: 0.41765568937574116   2343  0.4043434262275696\n",
      "Training loss: 0.4183255795921598   2344  0.43044087290763855\n",
      "Training loss: 0.42166503412382944   2345  0.429400309920311\n",
      "Training loss: 0.41838550780500683   2346  0.43097858130931854\n",
      "Training loss: 0.4206589034625462   2347  0.41335807740688324\n",
      "Training loss: 0.41814084351062775   2348  0.42308729887008667\n",
      "Training loss: 0.4185071012803486   2349  0.4286934733390808\n",
      "Training loss: 0.41782606712409426   2350  0.4155813828110695\n",
      "Training loss: 0.4181816577911377   2351  0.4189201220870018\n",
      "Training loss: 0.4162200519016811   2352  0.4215790629386902\n",
      "Training loss: 0.41774059193474905   2353  0.4189048111438751\n",
      "Training loss: 0.4189698589699609   2354  0.4158863425254822\n",
      "Training loss: 0.42139468448502676   2355  0.4201411157846451\n",
      "Training loss: 0.4198586770466396   2356  0.4272262752056122\n",
      "Training loss: 0.42039762650217327   2357  0.42265279591083527\n",
      "Training loss: 0.41786379899297443   2358  0.4162127524614334\n",
      "Training loss: 0.42030795344284605   2359  0.41842590272426605\n",
      "Training loss: 0.41749667695590426   2360  0.41017474234104156\n",
      "Training loss: 0.4122173488140106   2361  0.4217085614800453\n",
      "Training loss: 0.4135837916816984   2362  0.42159876227378845\n",
      "Training loss: 0.4178970732859203   2363  0.42474365234375\n",
      "Training loss: 0.4177993727581842   2364  0.4187737703323364\n",
      "Training loss: 0.41894595324993134   2365  0.4154553711414337\n",
      "Training loss: 0.4193768139396395   2366  0.4218989163637161\n",
      "Training loss: 0.4166701393468039   2367  0.42048902809619904\n",
      "Training loss: 0.4189359205109732   2368  0.4251526743173599\n",
      "Training loss: 0.41717981014932903   2369  0.4213913083076477\n",
      "Training loss: 0.42055559371198925   2370  0.41205257177352905\n",
      "Training loss: 0.4190247654914856   2371  0.41786694526672363\n",
      "Training loss: 0.4176101876156671   2372  0.4317656010389328\n",
      "Training loss: 0.4170224113123758   2373  0.42204699665308\n",
      "Training loss: 0.4180644473859242   2374  0.4145636111497879\n",
      "Training loss: 0.414892383984157   2375  0.4116026759147644\n",
      "Training loss: 0.4149877833468573   2376  0.4290698319673538\n",
      "Training loss: 0.4162211460726602   2377  0.43368081748485565\n",
      "Training loss: 0.4167678313595908   2378  0.4091235473752022\n",
      "Training loss: 0.41533507832459043   2379  0.4229675680398941\n",
      "Training loss: 0.4133791433913367   2380  0.4127573221921921\n",
      "Training loss: 0.41539138768400463   2381  0.4220517724752426\n",
      "Training loss: 0.41757125726767946   2382  0.42583969235420227\n",
      "Training loss: 0.4206016553299768   2383  0.4251258447766304\n",
      "Training loss: 0.4160432836839131   2384  0.42755255103111267\n",
      "Training loss: 0.4136194203581129   2385  0.4151645302772522\n",
      "Training loss: 0.41430165086473736   2386  0.4270319640636444\n",
      "Training loss: 0.4162275918892452   2387  0.41634076833724976\n",
      "Training loss: 0.4189000278711319   2388  0.4186411648988724\n",
      "Training loss: 0.4162570408412388   2389  0.4138099402189255\n",
      "Training loss: 0.4169264073882784   2390  0.42495281994342804\n",
      "Training loss: 0.41770883330277037   2391  0.4101745933294296\n",
      "Training loss: 0.4157184383698872   2392  0.4152999818325043\n",
      "Training loss: 0.4155336192675999   2393  0.41365429759025574\n",
      "Training loss: 0.417968498808997   2394  0.42397192120552063\n",
      "Training loss: 0.4160680983747755   2395  0.4198852628469467\n",
      "Training loss: 0.41801185267312185   2396  0.41187822818756104\n",
      "Training loss: 0.4189081575189318   2397  0.42087796330451965\n",
      "Training loss: 0.4164108399833952   2398  0.41292934119701385\n",
      "Training loss: 0.4180164805480412   2399  0.4276074767112732\n",
      "Training loss: 0.4204156675509044   2400  0.4101217985153198\n",
      "Training loss: 0.4169937414782388   2401  0.4140271991491318\n",
      "Training loss: 0.41504686006477903   2402  0.4239560663700104\n",
      "Training loss: 0.4145918978112085   2403  0.4208553284406662\n",
      "Training loss: 0.41665448461260113   2404  0.4191761165857315\n",
      "Training loss: 0.41755384845393045   2405  0.4266355633735657\n",
      "Training loss: 0.416705133659499   2406  0.420450396835804\n",
      "Training loss: 0.4176606003727232   2407  0.41520318388938904\n",
      "Training loss: 0.41487620770931244   2408  0.4175332635641098\n",
      "Training loss: 0.41618350999695913   2409  0.423552542924881\n",
      "Training loss: 0.417088793856757   2410  0.4234703481197357\n",
      "Training loss: 0.41531646038804737   2411  0.42799557745456696\n",
      "Training loss: 0.41577947778361185   2412  0.42408840358257294\n",
      "Training loss: 0.415100912962641   2413  0.42706695199012756\n",
      "Training loss: 0.4160811049597604   2414  0.42289814352989197\n",
      "Training loss: 0.4167399172272001   2415  0.4278026223182678\n",
      "Training loss: 0.4142907623733793   2416  0.41409732401371\n",
      "Training loss: 0.4142775280135019   2417  0.4259217828512192\n",
      "Training loss: 0.4123363494873047   2418  0.41564206779003143\n",
      "Training loss: 0.41826890621866497   2419  0.4313589185476303\n",
      "Training loss: 0.4147160287414278   2420  0.4154008626937866\n",
      "Training loss: 0.4175179111106055   2421  0.4171413406729698\n",
      "Training loss: 0.4164724669286183   2422  0.4239487200975418\n",
      "Training loss: 0.4165299598659788   2423  0.4091821014881134\n",
      "Training loss: 0.41532585663454874   2424  0.43004316091537476\n",
      "Training loss: 0.4155093069587435   2425  0.41200457513332367\n",
      "Training loss: 0.41552071060453144   2426  0.4207255691289902\n",
      "Training loss: 0.416225220475878   2427  0.4156164526939392\n",
      "Training loss: 0.41366896459034513   2428  0.42580127716064453\n",
      "Training loss: 0.4131076399769102   2429  0.4210381358861923\n",
      "Training loss: 0.4153465117727007   2430  0.4095649719238281\n",
      "Training loss: 0.4129026085138321   2431  0.42458105087280273\n",
      "Training loss: 0.41634799540042877   2432  0.42035604268312454\n",
      "Training loss: 0.41293742401259287   2433  0.41083014011383057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.41367827568735394   2434  0.4188717305660248\n",
      "Training loss: 0.4165309944323131   2435  0.41737137734889984\n",
      "Training loss: 0.4145585468837193   2436  0.4245406985282898\n",
      "Training loss: 0.4146320479256766   2437  0.42779289186000824\n",
      "Training loss: 0.41247299313545227   2438  0.41617952287197113\n",
      "Training loss: 0.4188688887017114   2439  0.42271582782268524\n",
      "Training loss: 0.41473083198070526   2440  0.4188612699508667\n",
      "Training loss: 0.4145306199789047   2441  0.4204680547118187\n",
      "Training loss: 0.4143312041248594   2442  0.43220576643943787\n",
      "Training loss: 0.4157466058220182   2443  0.41228269040584564\n",
      "Training loss: 0.4143893186535154   2444  0.4114326760172844\n",
      "Training loss: 0.4149799112762724   2445  0.41191090643405914\n",
      "Training loss: 0.41540459649903433   2446  0.42388777434825897\n",
      "Training loss: 0.4148274617535727   2447  0.41566507518291473\n",
      "Training loss: 0.41397154544081005   2448  0.4223436415195465\n",
      "Training loss: 0.4149103973593031   2449  0.42077596485614777\n",
      "Training loss: 0.41817142920834677   2450  0.4221702516078949\n",
      "Training loss: 0.41465660291058676   2451  0.41937968134880066\n",
      "Training loss: 0.4158232020480292   2452  0.4150080382823944\n",
      "Training loss: 0.4145552281822477   2453  0.4267577528953552\n",
      "Training loss: 0.4118800610303879   2454  0.40874363481998444\n",
      "Training loss: 0.4153009695666177   2455  0.4155654311180115\n",
      "Training loss: 0.4150095454284123   2456  0.41904817521572113\n",
      "Training loss: 0.41497388056346346   2457  0.4212220758199692\n",
      "Training loss: 0.41690442178930553   2458  0.4107736498117447\n",
      "Training loss: 0.4162071368524006   2459  0.4228741377592087\n",
      "Training loss: 0.41392350835459574   2460  0.4256584867835045\n",
      "Training loss: 0.4165073058434895   2461  0.42007093131542206\n",
      "Training loss: 0.41555831262043547   2462  0.418949156999588\n",
      "Training loss: 0.41537833852427347   2463  0.4114343374967575\n",
      "Training loss: 0.414580905011722   2464  0.40624338388442993\n",
      "Training loss: 0.4139617553779057   2465  0.4243798702955246\n",
      "Training loss: 0.4136267048971994   2466  0.4214417636394501\n",
      "Training loss: 0.41461907965796335   2467  0.42154406011104584\n",
      "Training loss: 0.41875132066862925   2468  0.41917136311531067\n",
      "Training loss: 0.41639514906065805   2469  0.4289117306470871\n",
      "Training loss: 0.4134629475218909   2470  0.43046754598617554\n",
      "Training loss: 0.4130134646381651   2471  0.4276687949895859\n",
      "Training loss: 0.4148854911327362   2472  0.4281015545129776\n",
      "Training loss: 0.4132187919957297   2473  0.4231439083814621\n",
      "Training loss: 0.41232202095644815   2474  0.41264957189559937\n",
      "Training loss: 0.4115538341658456   2475  0.42994828522205353\n",
      "Training loss: 0.41423342909131733   2476  0.4174354374408722\n",
      "Training loss: 0.4140124831880842   2477  0.4224613457918167\n",
      "Training loss: 0.41306260228157043   2478  0.41268782317638397\n",
      "Training loss: 0.4146821562732969   2479  0.41900961846113205\n",
      "Training loss: 0.41239900461265017   2480  0.40883006155490875\n",
      "Training loss: 0.4158097931316921   2481  0.4225579649209976\n",
      "Training loss: 0.41317503792898996   2482  0.42323029041290283\n",
      "Training loss: 0.41359264722892214   2483  0.42845916748046875\n",
      "Training loss: 0.41107718646526337   2484  0.41224224865436554\n",
      "Training loss: 0.4152516701391765   2485  0.4229082316160202\n",
      "Training loss: 0.4179846835987909   2486  0.42451052367687225\n",
      "Training loss: 0.4115614891052246   2487  0.42657817900180817\n",
      "Training loss: 0.41226870247295927   2488  0.41814377903938293\n",
      "Training loss: 0.41088623234203886   2489  0.42659810185432434\n",
      "Training loss: 0.41453597588198526   2490  0.42199744284152985\n",
      "Training loss: 0.4133224891764777   2491  0.420356884598732\n",
      "Training loss: 0.41534705460071564   2492  0.42373277246952057\n",
      "Training loss: 0.4145803132227489   2493  0.4276704788208008\n",
      "Training loss: 0.4151067648615156   2494  0.4245366007089615\n",
      "Training loss: 0.4133399043764387   2495  0.4188220798969269\n",
      "Training loss: 0.413657118167196   2496  0.4181419014930725\n",
      "Training loss: 0.41329677615846905   2497  0.4214293658733368\n",
      "Training loss: 0.41223988362721037   2498  0.42007534205913544\n",
      "Training loss: 0.4108147088970457   2499  0.4235781580209732\n"
     ]
    }
   ],
   "source": [
    "epochs = 2500\n",
    "for i in range(epochs):\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    originalHidden=0\n",
    "    hidden=0\n",
    "    for data, labels in my_dataloader:\n",
    "        #print(data.shape)\n",
    "        if data.shape[0] != BatchSize:\n",
    "            #pass\n",
    "            continue;\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()    \n",
    "        logits,hidden,originalHidden = model(data,data.shape[0])\n",
    "        #print(logits.shape)\n",
    "        loss =criterion(logits,labels) #.permute(1,0,2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    model.eval();\n",
    "    #print(originalHidden[0,0,0],hidden[0][0,0,0])\n",
    "    evalLossNum=0\n",
    "    #generateAudio('4Beats2Mel','./generatedData/'+str(Counter))\n",
    "    for data, labels in my_validationloader:\n",
    "        if data.shape[0] != BatchSize:\n",
    "            #pass\n",
    "            continue;\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        pred,hidden,_ = model(data,data.shape[0])  \n",
    "        loss = criterion(pred,labels)#.permute(1,0,2)\n",
    "        evalLossNum += loss.item()\n",
    "    LossOverEpoch.append(running_loss/len(my_dataloader))\n",
    "    EvalLoss.append(evalLossNum/len(my_validationloader))\n",
    "    print(str(f\"Training loss: {running_loss/len(my_dataloader)}\" +\"   \"+ str(Counter))+\"  \"+str(evalLossNum/len(my_validationloader)))\n",
    "    Counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(LossOverEpoch)\n",
    "plt.plot(EvalLoss)\n",
    "plt.show()\n",
    "torch.save(model.state_dict(), './w50-128*2-256-10000-2050-50Epochs.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "outputArray=[]\n",
    "h0 = torch.zeros(1, 1, 10000).cuda()\n",
    "c0 = torch.zeros(1, 1, 10000).cuda()\n",
    "for data, labels in my_testloader:\n",
    "    with torch.no_grad():\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        print(data.shape)\n",
    "        output,_ = model.forward(data,h0,c0)\n",
    "        for i in range(SequenceLength):\n",
    "            outputArray.append(output[i,0,:].cpu().numpy())\n",
    "outputArray=np.array(outputArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputArray.shape)\n",
    "plt.imshow(outputArray.T)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedArray=[]\n",
    "for elem in outputArray:\n",
    "    a=np.array(np.int((n_fft/2+1)) *[1+1j])\n",
    "    a.real=elem[:np.int(n_fft/2+1)]\n",
    "    a.imag=elem[np.int(n_fft/2+1):]\n",
    "    transformedArray.append(a)\n",
    "transformedArray=np.array(transformedArray).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print((transformedArray).shape)\n",
    "Y_infered = librosa.istft(transformedArray,hop_length=hopLength)\n",
    "ipd.Audio(Y_infered,rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(y,rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "librosa.display.specshow(librosa.amplitude_to_db(transformedArray.real,\n",
    "                                                  ref=np.max),\n",
    "                          y_axis='log', x_axis='time')\n",
    "plt.title('Power spectrogram')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "librosa.display.specshow(librosa.amplitude_to_db(tensor_y.numpy()[:,:1025].T,ref=np.max),y_axis='log', x_axis='time')\n",
    "plt.title('Power spectrogram')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output=np.array(0);\n",
    "with torch.no_grad():\n",
    "        test=torch.zeros([1, 128*windowSize], dtype=torch.float32)\n",
    "       # test[0][46]=0.0;\n",
    "        output = model.forward(test)\n",
    "      #  test2=torch.ones([1, 1025], dtype=torch.float32)\n",
    "      #  test2[0][46]=0.0;\n",
    "        loss = criterion(output,torch.zeros([1, 1025], dtype=torch.float32))\n",
    "        print(loss.item())\n",
    "print(output.numpy()[0].shape,D_data[250].shape)\n",
    "plt.plot(output.numpy()[0],'r')\n",
    "#plt.plot(D_data[250],'g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(outPutMidiArray[60].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "midiTestArray=[]\n",
    "HopSize=0.023\n",
    "timeTracker=0.0\n",
    "totalTimeTracker=0.0\n",
    "MemoryArray=np.zeros(128)\n",
    "lasttime=0;\n",
    "bpm=60\n",
    "for msg in mido.MidiFile('4Beats2Mel.mid'):\n",
    "    if not msg.is_meta:\n",
    "        totalTimeTracker+=msg.time\n",
    "        nextEventTime=lasttime+msg.time\n",
    "        while nextEventTime> timeTracker + HopSize:\n",
    "            MemoryArray[127]=((timeTracker%4) * (bpm/60.0))/4.0\n",
    "            midiTestArray.append(MemoryArray.copy())\n",
    "            timeTracker += HopSize;\n",
    "        if msg.type=='note_on':\n",
    "            MemoryArray[msg.note]+=1.0;\n",
    "        elif msg.type=='note_off':\n",
    "            MemoryArray[msg.note]-=1.0;\n",
    "        if(timeTracker+msg.time >= timeTracker + HopSize):\n",
    "            MemoryArray[127]=((timeTracker%4) * (bpm/60.0))/4.0\n",
    "            midiTestArray.append(MemoryArray.copy())\n",
    "            timeTracker += HopSize\n",
    "        lasttime += msg.time\n",
    "    else:\n",
    "        if(msg.type == 'set_tempo'):\n",
    "            bpm=60000000/msg.tempo\n",
    "midiTestArray = np.float32(np.array(midiTestArray))\n",
    "print(midiTestArray.shape)\n",
    "MainMidiTest=[]\n",
    "for i in range(midiTestArray.shape[0]):\n",
    "    temp=[]\n",
    "    for j in range(windowSize):\n",
    "        val = int(j-(np.floor(windowSize/2)))\n",
    "        val = i+val\n",
    "        if(val<0 or val+1>midiTestArray.shape[0]):\n",
    "             temp.append(np.zeros(midiTestArray.shape[1]))\n",
    "        else:\n",
    "            temp.append(midiTestArray[val])\n",
    "    MainMidiTest.append(temp)\n",
    "MainMidiTest=np.array(MainMidiTest)\n",
    "\n",
    "\n",
    "\n",
    "tensor_x_test = torch.stack([torch.Tensor(i) for i in np.abs(MainMidiTest)])\n",
    "tensor_x_test = tensor_x_test.view(tensor_x_test.shape[0],-1)\n",
    "\n",
    "testDataSet = utils.TensorDataset(tensor_x_test,torch.zeros(tensor_x_test.shape)) # create your datset\n",
    "TestLoader = utils.DataLoader(testDataSet) # create your dataloader\n",
    "\n",
    "\n",
    "model.eval()\n",
    "outputArray=[]\n",
    "for data, labels in TestLoader:\n",
    "    with torch.no_grad():\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        output = model.forward(data)\n",
    "        outputArray.append(output[0].cpu().numpy())\n",
    "outputArray=np.array(outputArray).T\n",
    "print((outputArray).shape)\n",
    "Y_infered2 = librosa.istft(outputArray,hop_length=hopLength) \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "librosa.display.specshow(librosa.amplitude_to_db(outputArray,\n",
    "                                                  ref=np.max),\n",
    "                          y_axis='log', x_axis='time')\n",
    "plt.title('Power spectrogram')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(midiTestArray.T,interpolation='nearest', aspect='auto')\n",
    "plt.title('Power spectrogram')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show();\n",
    "\n",
    "\n",
    "ipd.Audio(Y_infered2,rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "librosa.display.waveplot(y[:500], sr=sr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating audio and Spectrum image\n",
      "torch.Size([9, 25, 128])\n",
      "torch.Size([128, 25, 128])\n",
      "(3200, 2050)\n"
     ]
    }
   ],
   "source": [
    "generateAudio('4Beats2Mel','./generatedData/'+str(Counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
